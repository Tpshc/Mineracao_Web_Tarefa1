1
Algorithmically probable mutations reproduce
aspects of evolution such as convergence rate,
genetic memory, modularity, diversity
explosions, and mass extinction
Santiago Herna?ndez-Orozco, Hector Zenil and Narsis A. Kiani
Abstract—Natural selection explains how life has evolved over millions of years from more primitive forms. The speed at which this
happens, however, has sometimes defied explanations based on (uniformly distributed) random mutations. Here we show that if
evolution is algorithmic in any form and can thus be considered a program in software space, the emergence of a natural algorithmic
probability distribution has the potential to become an accelerating mechanism. When the strategy produces unfit organisms—because
mutations may lead to, for example, syntactically useless evolutionary programs—massive extinctions occur and modularity provides
an evolutionary advantage evolving a genetic memory. We simulate the application of these mutations (no recombination) based on
numerical approximations to algorithmic probability, in what constitutes the first experiments in artificial life based on aspects of
Algorithmic Information Theory. We find that recurring structures can rapidly become pervasive, potentially explaining some aspects of
convergent evolution, and that the emergence of information modules by local evolution is unavoidable, requiring memory support
reminiscent of functional structures such as genes and biological information carriers such as DNA. We demonstrate that such regular
structures are preserved and carried on when they first occur and can also lead to an accelerated production of diversity and extinction,
possibly explaining natural phenomena such as periods of accelerated growth of the number of species (e.g. the Cambrian explosion)
and the occurrence of massive extinctions (e.g. the End Triassic) whose causes are a matter of considerable debate. The approach
introduced here appears to be a better approximation to actual biological evolution than models based upon the application of mutation
from uniform probability distributions, and because evolution by algorithmic probability converges faster to regular structures (both
artificial and natural, as tested on a small biological network), it also approaches a formal version of open-ended evolution based on
previous results. The results validate the motivations and results of Chaitin’s Metabiology programme. We also show that the procedure
has the potential to significantly accelerate solving optimization problems in the context of artificial evolutionary algorithms.
Index Terms—ALife, Artificial Evolution, Algorithmic Complexity, Algoritmic Probability, Metabiology, Universal Distribution,
Randomness, Diversity, Extinction, Evolvability, Biological Memory, Adaptability, Extinction
F
1 ALGORITHMIC EVOLUTION
C Entral to modern synthesis and general evolutionarytheory is the understanding that evolution is gradual
and is explained by small genetic changes in populations
over time. Genetic variation in populations arises by chance
through mutation, with these small changes leading to
major evolutionary changes over time.
• S. Herna?ndez-Orozco is a PhD student at Posgrado en Ciencia e
Ingenier??a de la Computacio?n, Universidad Nacional Auto?noma de
Me?xico (UNAM), Mexico.
• H. Zenil and N. Kiani are with the Algorithmic Dynamics Lab, Unit
of Computational Medicine, SciLifeLab, Department of Medicine Solna
Centre for Molecular Medicine, Stockholm, Sweden.
• H. Zenil and N. Kiani are with the Algorithmic Nature Group,
LABORES, Paris, France.
• To whom correspondence should be addressed: hec-
tor.zenil@algorithmicnaturelab.org Contributions: HZ and SHO
conceived the project. HZ and NAK provided guidance, data and proposed
new experiments. SHO, HZ and NAK analyzed the data. SHO and
HZ wrote code. SHO and HZ wrote the paper. The results can be
reproduced using the Online Algorithmic Complexity Calculator at
http://www.complexitycalculator.com/
Manuscript received *, 2017; revised *.
Of interest in connection to the possible links between
the theory of biological evolution and the theory of infor-
mation is the place and role of randomness in the process
that provides the variety necessary to allow organisms to
change and adapt over time.
It has been suggested [1], [2], [3], [4] that the deeply
informational and computational nature of biological or-
ganisms makes them amenable to being studied or consid-
ered as computer programs following (algorithmic) random
walks in software space, that is, the space of all possible—
and valid—computer programs. Here we numerically test
this hypothesis and explore the possible consequences of
its validation vis-a-vis our understanding of the biological
aspects of life and natural evolution by natural selection, as
well as for applications to optimization problems in areas
such as evolutionary programming.
1.1 Classical v. Algorithmic Probability
The classical probability of producing the first n digits of
a mathematical constant like ? in binary by chance, e.g.,
by typing on a typewriter, is exponentially unlikely as a
function of the number of digits. However, because ? is
ar
X
iv
:1
70
9.
00
26
8v
3 
 [
cs
.N
E
] 
 6
 S
ep
 2
01
7
2
not random, in the sense that it has a short description
that can generate an arbitrary number of digits of ? with
the same short formula, the algorithmic probability of ? is
much higher than its classical probability. This is because the
classical probability of producing a short computer program
encoding a short mathematical formula by chance is more
likely than typing the digits of ? themselves one by one. A
?-generating formula can be written as a computer program
in no more than N bits, Thus it has the classical probability
of occurring by chance for the first n digits of ?, where
log(n) only accounts for the size of the number of digits n
to be introduced into the computer program before it halts,
compared to 1/2n for the classical probability of producing
n digits of ? directly. Clearly, 1/2N + log(n) and 1/2n are
divergent and for n > N +log(n) on 1/2N +log(n) < 1/2n.
1.2 Chaitin’s Evolutionary Model
In the context of his work on Metabiology, Chaitin developed
a theoretical computational model that evolves relative to
its environment significantly faster than classical random
mutation [1], [5], [6]. Starting with an empty binary string,
the system approximates Chaitin’s constant ?, defined as
? =
?
p?HP 2
?|p| where HP is the set of all halting programs,
in an expected time of O(t2(log t)(1+O(1))), which is signif-
icantly faster than the exponential time that the process is
expected to take when randomly choosing mutations from
a uniform distribution. This speed-up is obtained by drawing
mutations according to the Universal Distribution, which is
explained in the following chapter.
In a previous result [7], it was shown that the previous
system exhibits open-ended evolution (OEE, [8]) according
to a strict definition of the concept, and that no decidable
system with computable dynamics can achieve OEE. Here
we will present a system that, by following the Universal
Distribution, approaches this behaviour.
2 METHODOLOGY
2.1 Algorithmic Probability and the Universal Distribu-
tion
At the core of our approach is the concept of Algorithmic
Probability introduced by Solomonoff [9], Levin [10] and
Chaitin [11]. Denoted by P (s), the algorithmic probability
of a binary string s is formally defined as:
P (s) =
?
p:U(p)=s
1/2|p| (1)
where p is a random computer program in binary (whose
bits were chosen at random) running on a so-called prefix-
free (in order to constrain the number of valid programs
as it would happen in physical systems) universal Turing
machine U that outputs s and halts.
Algorithmic probability connects the algorithmic likeli-
hood of s to the intrinsic algorithmic s. The less algorith-
mically complex s (like ?), the more frequently it will be
produced on U by running a random computer program
p. If K(s) is the descriptive algorithmic complexity of s
(also known as Kolmogorov-Chaitin complexity [12], [13]),
we have it that P (s) = 1
2K(s)+O(1)
.
The distribution induced by P (s) over all strings is
called the Universal Distribution or Levin’s semi-measure,
because the measure is semi-computable and can only be
approximated from below and its sum does not add up to 1
to be a full measure.
2.2 Evolutionary Model
Broadly speaking, our evolutionary model is a tuple
?S, S,M0, f, t, ??, where:
• S is the state space,
• M0, with M0 ? S, is the initial state of the system,
• f : S 7? R+ is a function, called the fitness or
aptitude function, which goes from the state space to
the positive real numbers,
• t is a positive integer called the extinction threshold,
• ? is a real number called the convergence parameter,
and
• S : S 7? S × (Z+ ? {?,>}) is a non-deterministic
evolution dynamic such that if S(M,f, t) = (M ?, t?)
then f(M ?) < f(M) and t? ? t, where t? is the
number of steps or mutations it took S to produce M ?,
S(M,f, t) = (?, t?) if it was unable find M ? with
a better fitness in the given time, and S(M,f, t) =
(>, t?) if it finds M ? such that f(M ?) ? ?.
Specifically, the function S receives an individual M and
returns an evolved individual M ?, in the time specified by t,
that improves upon the value of the fitness function f and
the time it took to do so, ? if it was unable to do so and >
if it reached the convergence value.
A successful evolution is the sequence M0, (M1, t1) =
S(M0, f, t), ..., (>, tn)) and
?
ti is the total evolution time.
We say that the evolution failed, or that we got an extinction,
if instead we finish the process by (?, tn), with
?
ti being
the extinction time. The evolution is undetermined other-
wise. Finally, we will call each element (Mi, ti) an instance
of the evolution.
2.3 Experimental Setup: A Max One Problem Instance
For this experiment, our phase state is the set of all binary
matrices of sizes n × n, our fitness function is defined as
the Hamming distance f(M) = H(Mt,M), where Mt is the
target matrix, and our convergence parameter is ? = 0. In
other words, the evolution converges when we produce the
target matrix, guided only by the Hamming distance to it,
which is defined as the number of different bits between the
input matrix and the target matrix.
The stated setup was chosen since it allows us to easily
define and control the descriptive complexity of the fitness
function by controlling the target matrix and, therefore also
control the complexity of the evolutionary system itself. Is
important to note that our setup can be seen as a gener-
alization of the Max One problem, where the initial state is
a binary “target gene” and the target matrix is the “target
gene”; when we obtain a Hamming distance of 0 we have
obtained the gene equality.
3
2.4 Evolution Dynamics
The main goal of this project is to contrast the speed of
the evolution when choosing between two approaches to
determining the probability of mutations:
• When the probability of a given set of mutations has
a uniform distribution. That is, all possible mutations
have the same probability of occurrence, even if
under certain constraints.
• When the probability of a given mutation occurring
is given by an approximation to the Universal Distri-
bution (UD) [10], [14]. As the UD is non-computable,
we will approximate it by approximating the al-
gorithmic complexity K ( [12], [13]) by means of
the Block Decomposition Method (with no overlap-
ping) [15] based on the Coding Theorem Method
(CTM) [16], [17], [18] (see appendix A).
• We will also investigate the results by running the
same experiments using Shannon Entropy instead of
BDM to approximate K .
Each evolution instance was computed by iterating over the
same dynamic. We start by defining the set of possible
mutations as those that are within a fixed n number of bits
from the input matrix. In other words, for a given input
matrix M , the set of possible mutations in a single instance
is defined as the set
M(M) = {M ?|H(M ?,M) ? n}.
Then, for each matrix in M, we compute the probability
P (M ?) is defined as:
• P (M) = 1|M| in the case of the Uniform Distribution.
• P (M) = ?
2BDM(M)
for the BDM Distribution, where
? is a normalization factor such that the sum of
probabilities is 1, and
• P (M) = ?
?
h(M) or P (M) =
???
2h(M)
for Shannon
entropy (for an uninformed observer with no access
to the possible deterministic or stochastic nature of
the source).
Once the probability distribution is computed, we set the
number of steps as 0 and then, using a (pseudo)random
number generator (RNG), we proceed to stochastically draw
a matrix from the sated probability distributions and evalu-
ate its fitness with the function f , adding 1 to the number of
steps. If the resultant matrix does not show an improvement
in fitness, we draw another matrix and add another 1 to the
number of steps, not stopping the process until we obtain a
matrix with better fitness or reach the extinction threshold.
We can either replace the drawn matrix or leave it out of
the pool for the next iterations. A visualisation of the stated
work flow for a 2× 2 matrix is shown in Figure 1.
To get a complete evolution sequence, we iterate the
stated process until either convergence or extinction is
reached. As stated before, we can choose to not replace
an evaluated matrix from the set of possible mutations in
each instance, but we chose to not keep track of evaluated
matrices after an instance was complete. This was done
in order to keep open the possibility of dynamic fitness
functions in future experiments.
In this case, the evolution time is defined as the sum of
the number of steps (or draws) it took the initial matrix
to reach equality with the target matrix. When computing
the evolution dynamics by one of the different probability
distribution schemes we will denote ot by uniform strategy,
BDM strategy or h strategy, respectively. That is, the uniform
distribution, the distribution for the algorithmic probability
estimation by BDM, and the distribution by Shannon en-
tropy.
Fig. 1. An evolution instance. The instances are repeated by updating
the input matrix until convergence or extinction is reached.
2.5 The Speed-Up Quotient
We will measure how fast (or slow) a strategy is compared to
the uniform by the speed-up quotient, which we will define
as:
Definition 1. The speed-up quotient, or simply speed-up,
between the uniform strategy and a given strategy f is
defined as
? =
Su
Sf
,
where Su is the average number of steps it takes a sample (a
set of initial state matrices) to reach convergence under the
uniform strategy and Sf is the average number of steps it
takes under the f strategy.
2.6 Theoretical Considerations
In Chaitin’s evolutionary model [1], [5], [6], a successful mu-
tation is defined as a computable function µ, chosen accord-
ing to the probabilities stated by the Universal Distribution
[14], that changes the current state of the system (as an input
of the function) to a better approximation of the constant
? [13]. In order to be able to simulate this system we would
need to compute the Universal Distribution and the fitness
function. However, both the Universal Distribution and the
fitness function of the system require the solution of the
4
Halting Problem [19], which is incomputable. Nevertheless,
as with ? itself, this solution can be approximated [15], [20].
In this project we are proposing a model that, to the best
of our knowledge, is the first computable approximation to
Chaitin’s proposal.
For this first approximation we have made four impor-
tant initial concessions: one with respect to the real comput-
ing time of the system, and three with respect to Chaitin’s
model:
• We assume that building the probability distribu-
tions for each instance of the evolution takes no
computational time, while in the real computation this
is the single most resource-intensive step.
• The goal of our system is to approximate matrices,
which are objects of finite information content.
• We use BDM and Shannon’s entropy as approxima-
tions for the algorithmic information complexity K .
• We are not approximating the algorithmic proba-
bility of the mutation functions, but that of their
outputs.
We justify the first concession in a similar fashion as Chaitin:
if we assume that the interactions and mechanics of the
natural world are computable, then the probability of a
decidable event occurring is given by the Universal Dis-
tribution. The third one is a necessity, as the algorithmic
probability of an object is incomputable (it requires a so-
lution for HP too). In the next section we will show that
Shannon’s entropy is not as good as BDM for our purposes.
Finally, note that given the Universal Distribution and a
fixed input, the probability of a mutation is in inverse
proportion to the descriptive complexity of its output, up
to a constant error. In other words, it is highly probable
that a mutation may reduce the information content of the
input but improbable that it may increase the information
content. Therefore, the last concession yields an adequate
approximation, since a low information mutation can reduce
the descriptive complexity of the input but not increase it in
a meaningful way.
2.7 Our Expectations
It is important to note that, when compared to Chaitin’s
metabiology model [1], by changing the goal of our system
we also changed the expectations we had for its behaviour.
Chaitin’s evolution model [1] is faster than regular ran-
dom models despite targeting a highly random object,
thanks to the fact that positive mutations have low al-
gorithmic information complexity and hence a (relatively)
high probability of being stochastically chosen under the
Universal Distribution. The universally low algorithmic
complexity of these positive mutations relies on the fact that,
when assuming an oracle for HP, we are also implying a
constant algorithmic complexity for its evaluation function
and target, since we can write a program that verifies if
a change on a given approximation of ? is a positive one
without needing a codification of ? itself.
In contrast, we expected our model to be sensitive with
respect to the algorithmic complexity of the target matrix,
obtaining high speed-up for structured target matrices that
decreases as the algorithmic complexity of the target grows.
However, this change of behaviour remains congruent with
the main argument of metabiology [1] and our assertion
that, contrary to regular random mutations, algorithmic prob-
ability driven evolution tends to produce structured novelty
at a faster rate, which we hope to prove in the upcoming set
of experiments.
In summary, we expect that when using an approxima-
tion to the Universal Distribution:
• Convergence will be reached in significantly fewer
total mutations than when using the uniform distri-
bution for structured target matrices.
• The stated difference will decrease in relation to the
algorithmic complexity of the target matrix.
We also aimed to explore the effect of the number of allowed
shifts (mutations) on the expected behaviour.
3 RESULTS
3.1 Cases of Negative Speed-up
In order to better explain the choices we have made to our
experimental setup, first we will present a series of cases
where we obtained no speed-up or slow-down. Although
these cases were expected, they shed important light on the
behaviour of the system.
3.1.1 The Unsuitability of Shannon’s Entropy
As shown in [15], [21], when compared to BDM we can think
of Shannon’s entropy alone as a less accurate approximation
to the algorithmic complexity of an object (if its underlying
probability distribution is not updated by a method equiv-
alent to BDM, as it would not be by the typical uninformed
observer). Therefore we expect the entropy-induced speed-
up to be consistently outperformed by BDM when the target
matrix inclines away from algorithmic randomness and has
thus some structure. Furthermore, as random matrices are
expected to have a balanced number of 0’s and 1’s, we
anticipated the performance of single bit entropy to be
nearly identical to the uniform distribution on unstructured
(random) matrices. For block entropy, that is, the entropy
computed over submatrices rather than single bits, the prob-
ability of having repeated blocks is in inverse proportion to
their size, while blocks of smaller sizes approximate single
bit entropy, again yielding similar results to the uniform dis-
tribution. The results support our assumptions and claims.
As mentioned above, the probability distributions for the
set of possible mutations using entropy were built using
two heuristics: Let M ? be a possible mutation of M , then
the probability of obtaining M ? as a mutation is defined
as either, ?
?
h(M ?)+ or
???
2h(M?)
. The first definition assigns a
linearly higher probability to mutations with lower entropy.
The second definition is consistent with our use of BDM in
the rest of the experiments. The constant  is an arbitrary
small value that was included to avoid undefined (infinite)
probabilities, and ?? and ??? are normalization coefficients
defined so that the sum of probabilities over the possible
mutation space is always 1. For the following round of
experiments  was set at 1?10.
5
TABLE 1
Results obtained for the ‘Random Graphs’
Strategy Shifts Average SE
Uniform 1 214.74 3.55
hb 1 214.74 3.55
h 1 215.53 3.43
h2b 1 214.74 3.55
h2 1 213.28 3.33
Uniform 2 1867.10 78.94
hb 2 1904.52 79.88
h 2 2036.13 83.38
h2b 2 1882.46 78.63
h2 2 1776.25 81.93
3.1.2 Entropy vs. Uniform on Random Matrices
For the following experiments, we generated 200 random
matrices separated into two sets: initial matrices and target
matrices. After pairing them based on their generation order
we evolved them using 10 strategies: the uniform distribu-
tion, block Shannon’s entropy for blocks of size 4×4, denoted
below by hb, entropy for single bits denoted by h, and their
variants where we divide by h2 and h2b respectively. The
strategies were repeated for 1- and 2-bit shifts (mutations).
The results obtained are summarized in the table 1,
which lays out the strategy used for each experiment, the
number of shifts/mutations allowed, the average number
of steps it took to reach convergence, as well as the standard
error of the sample mean. As we can see, the differences
in the number of steps required to reach convergence are
not significant, validating our assertion that, for random
matrices, entropy evolution is not much different than the
uniform evolution.
Because the algorithmic complexity of a network makes
sense only in its unlabelled version in general, and in most
of the cases. In [15], [18], [22] we showed, both theoretically
and numerically, that approximations of algorithmic com-
plexity of adjacency matrices of labelled graphs are a good
approximation (up to a logarithmic term or the numerical
precision of the algorithm) of the algorithmic complexity of
the unlabelled graphs. This means that we can consider any
adjacency matrix of a network a good representation of the
network disregarding graph isomorphisms.
3.1.3 Entropy vs. Uniform on a Highly Structured Matrix
For this set of experiments, we took the same set of 100 8×8
initial matrices and evolved them into a highly structured
matrix, which is the adjacency matrix of the star with 8
nodes. For this matrix, we expected entropy to be unable
to capture its structure, and the results obtained accorded
with our expectations. The results are shown in table 2.
As we can see from the results, entropy was unable
to show a statistically significant speed-up compared to
the uniform distribution. Over the next sections we show
that we have obtained a statistically significant speed-up
by using the BDM approximation to algorithmic probability
distributions, which is expected because BDM manages to
better capture the algorithmic structures of a matrix rather than
just the distribution of the bits which entropy measures. Based on
the previous experiments, we conclude that entropy is thus
not a good approximation for K , and we will omit its use in
the rest of the article.
TABLE 2
Results obtained for the ‘Star’
Strategy Shifts Average SE
Uniform 1 216.24 3.48
hb 1 216.71 3.54
h 1 212.74 3.41
h2b 1 216.71 3.54
h2 1 211.74 3.69
Uniform 2 1811.84 85.41
hb 2 1766.69 88.18
h 2 1859.11 75.73
h2b 2 1764.03 84.52
h2 2 1853.04 74.48
3.1.4 Randomly Generated Graphs
For this set of experiments, we generated 200 random 8× 8
matrices and 600 16 × 16 matrices, both sets separated into
initial and target matrices. We then proceeded to evolve the
initial matrix into the corresponding target by the following
strategies: uniform and BDM within 2-bit and 3-bit shifts
(mutations) for the 8 × 8 matrices and only 2-bit shifts for
the 16 × 16 matrices due to computing time. The results
obtained are shown in the Figure 2. In all cases, we do not
replace drawn matrices and the extinction threshold was set
at 2500.
Random Target Graphs
0
2500
5000
7500
10000
110 115 120 125 130
S
te
ps
Type
BDM
Uniform
0
2500
5000
7500
10000
BDM Uniform
Extinction
0
2500
5000
7500
10000
110 115 120 125 130
S
te
ps
Extinction
0
1
Type
BDM
Uniform
2500
5000
7500
10000
BDM Uniform
Extinction
0
1
0
1000
2000
3000
4000
5000
470 480 490 500 510
BDM
S
te
ps
Type
BDM
Uniform
1000
2000
3000
4000
5000
BDM Uniform
Type
Extinction
0
8x8, 2?bits Shifts
8x8, 3?bits Shifts
16x16, 1?bits Shift
Fig. 2. Randomly generated 8× 8 and 16× 16 matrices.
From the results we can see two important behaviours
for the 8 × 8 matrices. The matrices generated are of high
BDM complexity and evolving the system using the uniform
strategy tends to be faster than using BDM for these highly
random matrices. Secondly, although increasing the number
of possible shifts by 1 seems, at a first glance, a small change
in our setup, it has a big impact on our results: the number of
6
extinctions has gone from 0 for both methods to 92 for the
uniform strategy and 100 for BDM. This means that most
evolutions will rise above our threshold of 2500 drafts for a
single successful evolutionary step, leading to an extinction.
As for the 16 × 16 matrices, we can see a formation of
two easily separable clusters that coincide perfectly with the
Uniform and BDM distributions respectively.
3.2 The Causes of Extinction
For the uniform distribution, the reason is simple: the num-
ber of 3-bit shifts on 8× 8 matrices gives a space of possible
mutations of
(8×8
3
)
= 41664 matrices, which is much larger
than the number of possible mutations present within 2-
shifts and 1-shift (mutation), which are
(8×8
2
)
= 2016 and
8 × 8 = 64 respectively. Therefore, as we get close to
convergence, the probability of getting the right evolution,
if the needed number of shifts is two or one, is about
0.04%, and removing repeated matrices does not help in
a significant way to avoid extinction,since 41 664 is much
larger than 2500.
Given the values discussed, we have chosen to set the
extinction threshold at 2500 and the number of shifts at 2
for 8× 8 matrices, as allowing just 64 possible mutations for
each stage is a number too small for showing a significant
difference in the evolutionary time between the uniform
and BDM strategies, while requiring evolutionary steps of
˜41 664 for an evolutionary stage is too computationally
costly. The threshold of 2500 is close to the number of
possible mutations and has been shown to consume a high
amount of computational resources. For 16 × 16 matrices,
we performed 1-bit shifts only, and occasionally 2-bit shifts
when computationally possible.
3.2.1 The BDM Strategy, Extinctions and Persistent Struc-
tures
The interesting case is the BDM strategy. As we can see
clearly in Figure 3 for the 8×8 3-bit case, the overall number
of steps needed to reach each extinction is often significantly
higher than 2500 under the BDM strategy. This behaviour
cannot be explained by the analysis done for the uniform
distribution, which predicts the sharp drop observed in the
blue curve.
0
10
20
30
0 2500 5000 7500 10000
Steps
F
itn
es
s Type
BDM
Uniform
Fig. 3. Fitness graph for random 8 × 8 matrices with 3-bit shifts (muta-
tions). Evolution convergence is reached at a fitness of 0. The curves
are polynomial approximations computed for visual aid purposes.
After analyzing the set of matrices drawn during failed
mutations (all the matrices drawn during a single failed
evolutionary stage), we found that most of these matrices
have in common highly regular structures. We will call these
structures persistent structures. Formally, regular structures
can be defined as follows:
Definition 2. LetM be the description used for an organism
or population and ? a substructure of M in a computable
position such that K(M) = K(?)+K(M??)?, where  is
a small number and M ?? is the codification M without the
contents of S. We will call ? a regular structure of degree
? if the probability of choosing a mutation M ? with the
subsequence ? is 1? 2?(??).
Now, note that ? grows in inverse proportion to K(?)
and the difference in algorithmic complexity of the mutation
candidates and K(?): Let M contain ? in a computable po-
sition. Then the probability of choosing M ? as an evolution
of M is
1
2K(M ?)
? 1
2K(M ???)+K(?)+O(1)
.
Furthermore, if the possible mutations ofM can only mutate
a bounded number of bits and there existsC such that, for
every other subsequence of ?? that can replace ? we have it
that K(??) ? K(?) + C , then:
P (M ? contains ?) ? 1?O(2?C).
The previous inequality is a consequence of the fact that
the possible mutations are finite and only a small number
of them, if any, can have a smaller algorithmic complexity
than the mutations that contain ?; otherwise we contradict
the existence of C . In other words, as ? has relatively low
complexity, the structures that contain ? tend to also have
low algorithmic complexity, and hence a higher probability
of being chosen.
Finally, as shown in the section 3.2, we can expect the
number of mutations with persistent structures to increase in
factorial order with the number of possible mutations and in
polynomial order with respect to the size of the matrices that
compose the state space.
Proposition 3. As a direct consequence of the last statement, we
have it that, for systems evolving as described in the section 2.4
under the Universal Distribution:
• Once a structure with low descriptive complexity is devel-
oped, it is exponentially hard to get rid of it.
• The probability of finding a mutation without the struc-
ture decreases in factorial order with respect to the set of
possible mutations.
• Evolving towards random matrices is hard (improbable).
• Evolving from and to unrelated regular structures is also
hard.
Given the fourth point, we will always choose random
initial matrices from now on, as the probability of drawing a
mutation other than an empty matrix (of zeroes), when one
is present in the set of possible mutations, is extremely low
(below 9× 10?6 for 8× 8 matrices with 2 shifts).
7
3.3 Positive Speed-Up Instances
In the previous section, we established that the BDM strat-
egy yields a negative speed-up when targeting randomly
generated matrices, which are expected to be of high al-
gorithmic information content or unstructured. However, as
stated in section 2.7, that behaviour is within our expecta-
tions. In the next section we will show instances of positive
speed-up, including cases where previously entropy failed
to show significant speed-up or was outperformed by BDM.
3.3.1 Manually Built Structured Matrices
For the following set of experiments we manually built
three 8 × 8 matrices that encode the adjacency matrices of
three undirected non-random graphs with 8 nodes that are
intuitively structured: the complete graph, the star graph and a
grid. The matrices used are shown in Figure 4.
Fig. 4. Adjacency matrices for the labelled complete, star, and grid
graphs.
After evolving the same set of 100 randomly generated
matrices for the three stated matrices, we can report that we
found varying degrees of positive speed-up, that correspond
to their respective descriptive complexities as approximated
by their BDM values. The complete graph, along with the
empty graph, is the graph that has the lowest approximated
descriptive complexity with a BDM value of just 24.01. As
expected, we get the best speed-up quotient in this case.
After the complete graph, the star intuitively seems to be
one of the less complex graphs we can draw. However,its
BDM value (105.434) is significantly higher than the grid
(83.503). Accordingly, the speed-up obtained is lower. The
results are shown in the Figure 5.
C
om
pl
et
e
G
rid
S
ta
r
0 1 2 3
1
2
3
1
2
3
1
2
3
log(Speed?Up)
S
hi
fts
Replacements
No
Yes
Fig. 5. The logarithm of the speed-up obtained for the matrices in
Figure 4.
As we can see from the Figure 5, a positive speed-up
quotient was consistently found within 2-bit shifts without
replacements. We have one instance of negative speed-up
with one shift with replacements for the grid, and negative
speed-up for all but the complete graph with two shifts.
However, is important to say that almost all the instances
of negative speed-up are not statistically significant, as we
have a very high extinction rate of over 90%, and the
difference between the averages is lower than two standard
errors of the mean. The one exception is the grid at 1-bit
shift, which had 45 extinctions for the BDM strategy. The
complete tables are presented in the appendix C.
3.3.2 Mutation Memory
The cause of the extinctions found in the grid are what
we will call maladaptive persistent structures (definition 2),
as they occur at a significantly higher rate under the BDM
distribution. Also, as the results suggest, a strategy to avoid
this problem is adding memory to the evolution. In our case,
we will not replace matrices already drawn from the set of
possible mutations.
We do not believe this change to be contradictory to
the stated goals, since another way to see this behaviour
is that the Universal Distribution dooms (with very high
probability) populations with certain mutations to extinction,
and evolution must find strategies to eliminate these mutations
fast from the population. This argument also implies that
extinction is faster under the Universal Distribution than
regular random evolution when a persistent maladaptive
mutation is present, which can be seen as a form of muta-
tion memory. This requirement has the potential to explain
evolutionary phenomena such as the Cambrian explosion,
as well as mass extinctions: once a positively structured
mutation is developed, further algorithmic mutations will
keep it (with a high probability), and the same applies to
negatively structured mutations. This can also explain the
recurring structures found in the natural world. Degrada-
tion of a structure is still possible, but will be relatively
slow. In other words, evolution will remember positive and
negative mutations (up to a point) when they are structured.
From now on, we will assume that our system has
memory and that mutations are not replaced when drawn
from the distribution.
3.3.3 The Speed-Up Distribution
Having explored various cases, and found several condi-
tions where negative and positive speed-up are present, the
aim of the following experiment was to offer a broader view
of the distribution of speed-up instances as functions of their
algorithmic complexity.
For the 8 × 8 case, we generated 28 matrices by starting
with the undirected complete graph with 8 nodes, repre-
sented by its adjacency matrix, and then we removed one
edge at a time until the empty graph (the diagonal matrix)
was left, obtaining our target matrix set. It is important to
note that the resultant matrices are always symmetrical. The
process was repeated for the 16 × 16 matrices, obtaining a
total of 120 16× 16 target matrices.
For each target matrix in the first target matrix set,
we generated 50 random initial matrices and evolved the
population until convergence was reached using the two
stated strategies: uniform and BDM, both without replace-
ments. We saved the number of steps it took for each of
the 2800 evolutions to reach convergence and computed
the average speed-up quotient for each target matrix. The
stated process was repeated for the second target matrix set,
8
but by generating 20 random matrices for each of the 120
target matrices to conserve computational resources. The
experiment was repeated for shifts of 1 and 2 bits and the
extinction thresholds used were 2500 for 8×8 and 10 000 for
16× 16 matrices.
Sequence of Generated Target Matrices
1.0
1.5
2.0
2.5
50 60 70 80 90 100 110
S
pe
ed
?
U
p
0
Ext.
8x8 Matrices, 1?bit shift
0
5
10
15
60 80 100
0
Ext.
8x8 Matrices,  2?bits shifts
1.0
1.5
2.0
2.5
3.0
50 100 150 200 250
BDM Complexity
S
pe
ed
?
U
p
0
1
2
3
4
Ext.
16x16 Matrices,  1?bit shifts
0
10
20
30
40
50 100 150 200 250
BDM Complexity
?15
?10
?5
Ext.
16x16 Matrices,  2?bits shifts
Fig. 6. The Speed-Up quotient is defined as ? = Su
SBDM
, where Su
is the average number of steps it took to reach convergence under
the uniform strategy and SBDM for BDM, and ‘Ext.’ is the difference
Eu ? EBDM where each factor is the number of extinctions obtained
for the universal and BDM distribution, respectively. In the case of an
extinction, the sample was not used to compute the average number
of steps. The red (dashed) line designates the Speed-Up threshold at
y = 1: above this line we have positive speed-up and below it we have
negative speed-up. The blue (continuous) line represents a cubic fit by
regression over the data points.
As we can see from the results in Figure 6, the average
number of steps required to reach convergence is significantly
lower when using the BDM distribution for matrices with
low algorithmic complexity, but the difference drops along
with the complexity of the matrices but never crosses the
extinction threshold. This suggests that symmetry over the di-
agonal is enough to guarantee a degree of structure that can
be captured by BDM. It is important to report that we found
no extinction case for the 8 × 8 matrices, 13 in the 16 × 16
matrices with 1-bit shifts, all for the BDM distribution, and
1794 with 2-bit shifts, mostly for the uniform distribution.
This last experiment was computationally very expen-
sive. Computing the data required for the 16 × 16, 2-bit
shifts sequence took 12 days, 6 hours and 22 minutes on
a single core of an i5-4570 PC with 8GB of RAM. Repeating
this experiment for 3-bit shifts is infeasible with our current
setup, as it would take us roughly two months shy of 3
years.
Now, by combining the data obtained for the previous
sequence and the random matrices used in section 3.1.4, we
can approximate the positive speed-up distribution. Given
the nature of the data, this approximation (Figure 7) is given
as two curves, each representing the expected evolution
time from a random initial matrix as a function of the
algorithmic information complexity of the target matrix for
both strategies, uniform and BDM respectively. The positive
speed-up instances are those where the the BDM curve is
below the uniform curve.
The Speed?Up Distribution
100
200
300
400
60 80 100 120
S
te
ps
Type
BDM
Exp. BDM
Uniform
8 × 8, 1?bit Shift
0
1000
2000
3000
4000
5000
100 200 300 400 500
BDM
S
te
ps
Type
BDM
Exp. BDM
Uniform
16 × 16, 1?bit Shift
Fig. 7. The positive speed-up instances are those where the coral curve,
computed as a cubic linear regression to all the evolution times for the
BDM strategy, are below the teal line, which is a cubic approximation
to the evolution times for the uniform strategy. The black line is the
expected BDM value for a randomly chosen matrix. The large gap in
the data reflects the fact that is hard to find structured (non-random)
objects.
The first result we get from Figure 7 is a confirmation
of an expected one: unlike the uniform strategy, the BDM
strategy is highly sensitive to the algorithmic information
content of the target matrix. In other words, it makes no
difference for a uniform probability mutation space whether the
solution is structured or not, while an algorithmic probability
driven mutation will naturally converge faster to structured
solutions.
The results obtained expand upon the theoretical devel-
opment presented in section 3.2.1. As the set of possible
mutations grows, so do the instances of persistent structures
and the slow-down itself. This behaviour is evident given
that, when we increase the dimension of the matrices, we
obtain a wider gap within the intersection point of the two
curves and the expected BDM value, which corresponds
to the expected algorithmic complexity of randomly gen-
erated matrices. However, we also increase the number of
structured matrices, ultimately producing a richer and more
interesting evolution space.
3.4 Chasing Biological and Synthetic Dynamic Nets
3.4.1 Chasing A Biological Network
We now set as target the adjacency matrix of a biological net-
work corresponding to the topology of an ERBB signalling
network [23]. The network is involved in responses ranging
from cell division, death, motility, and adhesion and when
dysregulated it has been found to be strongly related to
cancer [24], [25].
As one of our main hypotheses is that algorithmic prob-
ability is a better model for explaining biological diver-
sity, it is important to explore whethernaturally occurring
structures are more likely to be produced under the BDM
strategy than the uniform strategy, which is equivalent to
showing them evolving faster.
The binary target matrix is shown in Figure 8 and it
has a BDM of 349.91 bits. For the first experiment, we
9
RB
MYC
MAPK
IGFR
ESR
ERBB3
ERBB2
EGFR
CDKN1B
CDKN1A
CDK6
CDK4
CDK2
CCNE
CCND
AKT
A
K
T
C
C
N
D
C
C
N
E
C
D
K
2
C
D
K
4
C
D
K
6
C
D
K
N
1A
C
D
K
N
1B
E
G
F
R
E
R
B
B
2
E
R
B
B
3
E
S
R
IG
F
R
M
A
P
K
M
Y
C
R
B
Fig. 8. Adjacency matrix (left) of an ERBB signalling network (right).
generated 50 random matrices that were evolved using 1-
bit shift mutations for the Uniform and BDM distributions,
without repetitions. The BDM of the matrix is at the right of
the intersection point inferred by the cubic models shown
in Figure 7. Therefore we predict a slow-down. The results
obtained are shown in the table 3.
TABLE 3
Results obtained for the ERBB Network
Strategy Shifts Average SE Extinctions
Uniform 1 1222.62 23.22 0
BDM 1 1721.86 56.88 0
As the results show, we obtained a slow-down of 0.71,
without extinctions. However, as mentioned above, the
BDM of the target matrix is relatively high, so this result
is consistent with our previous experiments. However, the
strategy can be significantly improved.
3.4.2 Evolutionary Networks
An evolutionary network N is a tensor of dimension 4 of
nodesMi which are networks themselves with edges drawn
if Mk evolves into Mk? and weight corresponding to the
number of times that a network Mk has evolved into Mk? .
Fig. 9 shows a subnetwork of the full network for each
evolutionary strategy from 50 (pseudo-)randomly generated
networks with the biological ERBB signalling network as
target.
Mutations and overexpression of ERB receptors (ERBB2
and ERBB3 in this network) have been strongly associated
to more than 10 types of tissue-specific cancers and they can
be seen at the highest level upregulating most of the acyclic
network.
We call forward mutations, mutations that led to the target
network, and backward mutations, mutations that get away
from the target network through the same evolutionary
paths induced by forward mutations.
The forward mutations in the neighbourhood of the
target (the evolved ERBB network) for each strategy, are as
follow.
For the uniform distribution, each of the follow-
ing network forward mutations (regulating links) had
equal probability (1/5, assuming independence even if
unlikely): ESR?ERBB2, ERBB2?CDK4, CDK2 ?AKT,
CCND?EGFR, CCND?CDKN1A, as shown in Fig. 9.
For the BDM strategy, the network forward mutations in
the top 5 most likely immediate neighbourhood followed
and sorted by their occurring probability are: CCND?
CDK4, 0.176471; ESR? CCND, 0.137255; CDK6? RB,
0.137255; CDKN1B? CDK2, 0.0784314; IGFR?.
One of the mutations by BDM involves the break-
ing of the only network cycle of size 6: EGFR?ERBB3,
ERBB3?IGFR, IGFR?ESR, ESR?MYC, MYC?EGFR by
deletion of the interaction MYC?EGFR, with probability
0.05 among the possible mutations in the BDM immediate
neighbourhood of the target. In the cycle is involved ERBB3
which has been found to be related to many types of cancer
when overexpressed [24]
Fig. 9. Evolutionary convergence in evolutionary subnetworks closest to
the target matrix with edges shown only if they were used more than
once. The darker the edge colour the more times that an evolutionary
path was used to reach the target, the highest number is 7 for the
BDM-based mutation (top) and the lower 2 (e.g. all those in the uniform
random distribution).
For the local BDM strategy, the following were the
top 5 forward mutations: EGFR?ERBB2, 0.32; EGFR?
ERBB3, 0.107; IGFR? CCNE, 0.0714; ERBB3? ERBB2,
0.0714; EGFR? ESR, 0.0714; with ERBB2 and ERBB3 heavily
involved in 3 of the top 5 possible mutations with added
probability 0.49 and thus more likely than any other pair
and interaction of proteins in the network.
Under a hypothesis that mutations can be reversals to
states in past evolutionary pathways, then mutations to such
interactions may be the most likely backward mutations to
occur.
10
3.4.3 The Case for Localized Mutations and Modularity
As previously mentioned in the proposition 3, the main
causes of slow-down under the BDM distribution are mal-
adaptive persistent structures. These structures will nega-
tively impact the evolution speed in factorial order relative
to the size of the state space. One direct way to reduce the
size set of possible mutations is to reduce the size of the
matrices we are evolving. However, doing so will reduce the
number of interesting objects we can evolve towards too.
Another way to accomplish the objective while using the
same heuristic is to rely on localized (or modular) mutations.
That is, we force the mutation to take place on a submatrix
of the input matrix.
The way we implement the stated change is by adding
a single step in our evolution dynamics: at each iteration,
we will randomly draw, with uniform probability, one sub-
matrix of size 4 × 4 out of the set of adjacent submatrices
that compose the input matrix, with no overlap, and force
the mutation to be there by computing the probability
distribution over all the matrices that contain the bit-shift
only at the chosen place. We will call this method the local
BDM method.
It is important to note that, within 1-bit shifts (point
mutations), the space of total possible mutations remains the
same when compared to the uniform and BDM strategies.
Furthermore, the behaviour of the uniform strategy would
remain unchanged if the extra step is applied using the
Uniform distribution.
We repeated the experiment shown in the table 3 with
the addition of the local BDM strategy and the same 50
random initial matrices. Its results are shown in the table 4.
As we can see from the results obtained, local BDM obtains
a statistically significant speed-up of 1.25 when compared
to the uniform.
TABLE 4
Results obtained for the ERBB Network
Strategy Shifts Average SE Extinctions
Uniform 1 1222.62 23.22 0
BDM 1 1721.86 56.88 0
Local BDM 1 979 25.94 0
One potential explanation of why we failed to obtain
speed-up for the network with the BDM strategy is that,
as an approximation to K , the model depends on finding
global algorithmic structures, while the sample is based on
a substructure which might not have enough information
about the underlying structures we hypothesize govern
the natural world and make possible scientific models and
predictions.
However, biology evolves modular systems, such as
genes and cells, that in turn build building blocks such as
proteins and tissues. Therefore, local algorithmic mutation is
a better model. This is a good place to recall that local BDM
was devised as a natural solution to the problem presented
by maladaptive persistent structures in global algorithmic
mutation. Which also means that this type of modularity can
be evolved by itself given that it provides an evolutionary
advantage, as our results demonstrate.
We will further explore the relationship between BDM
and local BDM within the context of global structures in the
next section. Our current setup is not optimal for further
experimentation in biological and local structured matrices,
as the computational resources required to build the prob-
ability distribution for each instance grows in quadratic
order relative to matrix size, though these computational
resources are not needed in the real world (c.f. Conclusions).
3.4.4 Chasing Synthetic Evolving Networks
The aim of the next set of experiments was to follow, or
chase, the evolution of a moving target using our evolution-
ary strategies. In this case, we chased 4 different dynamical
networks: the ZK graphs [21], K-ary trees, an evolving N -
star graph and a star-to-path graph dynamic transition
artificially created for this project (see Appendix for code).
These dynamical networks are families of directed labelled
graphs that evolve over time using a deterministic algo-
rithm, some of which display interesting graph-theoretic
and entropy-fooling properties [21]. As the evolution dy-
namics of these graphs are fully deterministic, we expected
BDM to be significantly faster than the other two evolution-
ary strategies, uniform probability and local BDM.
We chased these dynamics in the following way: Let
S0, S1, . . ., Sn, . . . be the stages of the system we are
chasing. Then the initial state S0 was represented by a
random matrix and, for each evolution Si 7? Si+1, the
input was defined as the adjacency matrix corresponding
to Si, while the target was set as the adjacency matrix for
Si+1. In order to normalize the matrix size, we defined the
networks as always containing the same number of nodes
(16 for 16 × 16 matrices). We followed each dynamic until
the corresponding stage could not be defined in 16 nodes.
The results which were obtained, starting from 100
random graphs and 100 different evolution paths at each
stage, are shown in Figure 10. It is important to note that,
since the graphs were directed, the matrices used were non-
symmetrical.
Dynamic Graphs: Accumulated Averages
0
1000
2000
3000
0 1 2 3 4 5 6 7 8 9
S
te
ps
ZK Graph
0
1000
2000
3000
0 1 2 3 4 5 6 7 8 9 101112131415
Strategy
BDM
Local BDM
Uniform
K?Ary Tree
0
1000
2000
0 1 2 3 4 5
Graph Stage
S
te
ps
N?Star Graph
0
1000
2000
3000
4000
0 1 2 3 4 5 6 7 8 910111213141516
Graph Stage
Strategy
BDM
Local BDM
Uniform
Star Path
Fig. 10. Each point of the graph is composed of the average accumu-
lated time from 100 samples. All evolutions were carried out with 1-bit
shifts (mutations).
From the results we can see that local BDM consistently
outperformed the uniform probability evolution, but the
BDM strategy was the faster by a significant margin. The
results are as expected and confirm our hypothesis: uniform
evolution cannot detect any underlying algorithmic cause of
11
evolution, while BDM can, inducing a faster overall evolution.
Local BDM can only detect local regularities, which is good
enough to outrun uniform BDM in these cases. However, as
the algorithmic regularities are global, local BDM is slower
than (global) BDM.
4 DISCUSSION AND CONCLUSIONS
The mathematical mechanisms of biological information,
from Mendelian inheritance to Darwin’s evolution and the
discovery of the digital nature of the genetic code together
with the mechanistic nature of translation, transcription and
other inter cellular processes, not only suggest a strong
computational basis from algorithmic principles underlying
biological processes, but by taking it to the ultimate level,
they suggest that evolution by natural selection may not be
(very) different, and can thus be regarded and studied as
evolving programs in software space in the terms suggested
by Chaitin [5], [6] and by Wolfram [2].
Our findings demonstrate that key aspects of life may be
better explained with this algorithmic account and that the
empirical observation of the rate of biological evolutionary
convergence emerges naturally, unlike the alternative—and
generally assumed–hypothesis of uniformly distributed ran-
dom mutations. Here we have answered some questions by
abandoning this uniform distribution assumption, questions
ranging from the apparition of sudden major stages of
evolution, the emergence of ‘subroutines’ in the form of
modular persistent structures and the need of an evolving
memory carrying information organized in such modules
that drive the speed-up in the process of evolution by
selection.
The algorithmic distribution emerges naturally from the
interaction of deterministic systems [14], [16], [17], [26].
In other words, we are simulating the conditions of an
algorithmic/procedural world and there is no reason to
believe that it requires greater real-world (thus highly par-
allel) computation than is required by the assumption of
the uniform distribution. The Universal Distribution thus
emerges naturally and is the null hypothesis in algorithmic
space in which noise is only apparent in nature and a
consequence of other interacting open deterministic systems
in, e.g., the organism’s environment from which it harnesses
algorithmic structure to its own advantage [3].
The interplay of the evolvability of organisms from the
persistence of such structures also explains two opposed
phenomena: recurrent explosions of diversity and mass
extinctions, phenomena which have occurred during the
history of life on earth that have not been satisfactorily
explained under the uniform mutation assumption. How-
ever, taking the informational and computational aspects of
life based on modern synthesis to the ultimate and natural
consequences, the present approach based on weak as-
sumptions of deterministic dynamic systems offers a novel
framework of algorithmic evolution within which to study
both biological and artificial evolution.
APPENDIX A
APPROXIMATIONS TO ALGORITHMIC COMPLEXITY
The algorithmic complexity of a string K(s) (also known as
Kolmogorov-Chaitin complexity [12], [13]) is defined as the
length of the smallest program that produces s as an output
and halts. This measure of complexity is invariant—up to
a constant value— with respect to the choice of reference
universal Turing machine. Finding the exact value of K(s)
for any s is a lower semi-computable problem. This means
that there is no general effective method to find K(s) for
any given string, but upper bounds can be estimated.
Among the computable methods used to set an upper
bound are the Coding Theorem Method (CTM) [16], [17],
[27] and the Block Decomposition Method (BDM) [15], [18].
The CTM relies upon approximating the algorithmic
probability of an object by running every possible a large
set of small Turing machines, generating an empirical prob-
ability distribution for the produced strings by counting the
number of small Turing machines that produce each string
and halt. The algorithm can only be decided for a small
number of Turing machines and for those that can be decide
it runs in exponential time, therefore only approximations
of K(s) for small strings are feasible. However, this compu-
tation only needs to be done once to populate a lookup table
that allows its application in linear (constant in exchange of
memory) time.
BDM is an extension of CTM defined as
BDM(s) =
?
i
CTM(si) + log(ni),
where each si corresponds to a substring of s for which
its CTM value is known and ni is the number of times the
string si appears in s. A thorough discussion of BDM is
found in [15].
APPENDIX B
RECURSIVELY GENERATED GRAPHS
To test the speed of algorithmic evolution on recursive
dynamic networks we generated 3 other low algorithmic
graphs different from the ZK graph as defined in [21] that
is also of low algorithmic complexity. We needed graphs
that evolved over time in a low algorithmic complexity
fashion from and to low algorithmic complexity graphs. The
3 graphs were canonically labelled using the positive natural
numbers up to n by maximizing the number of nodes with
consecutive numbers, then a rule was applied from lowest
to highest number until the transformation was complete.
The Wolfram Language code used to generate
these recursively (hence of low algorithmic complex-
ity/randomness) evolving graphs are the following.For the
ZK graph [21] recursively generated by:
AddEdges[graph_]:=
EdgeAdd[graph,
Rule@@@Distribute[{Max[VertexDegree[graph]]
+1,
Table[i,{i,(Max[VertexDegree[graph]]+
2), (Max[VertexDegree[graph]]+
1)+(Max[VertexDegree[graph]]+1)
- VertexDegree[graph, Max[
VertexDegree[graph]] + 1]}]}, List]]
EdgeList/@NestList[AddEdges, Graph[{1->2}],n]
12
where n is the number of iterations. For the n-growing star
graph with n overlapping nodes:
Graph[Rule@@@
Flatten[Table[(List@@@EdgeList[
StarGraph[n]]) + 2 n,
{n, 3, i, 1}], 1]]
The star-to-path graph was encoded by:
EdgeList/@
FoldList[EdgeDelete[EdgeAdd[#1,
#2[[1]]], #2[[2]]] &,
Graph[EdgeList@StarGraph[n],
VertexLabels -> "Name"],
Thread[{Most@Flatten[{EdgeList@
CycleGraph[16], 1 <-> 16}],
Flatten[{EdgeList@StarGraph[n],
1 <-> 16}]}]]
where n is the size of the star graph in the 2 previous
evolving graphs.
The K-ary tree evolving graph was generated with the
Wolfram Language built-in function KaryTree[n], where n is
the size of the K-ary tree.
APPENDIX C
ALGORITHMIC PROBABILITY ESTIMATIONS OF SIM-
PLE GRAPHS
BDM for graphs is defined in [18].
The tables 5, 6 and 7 contain full statistical information
for the speed-up obtained for the simple graphs ‘complete’,
‘star’ and ‘grid’.
TABLE 5
Results obtained for the ‘Complete Graph’.
Strategy Shifts Average SE Extinctions Replacements
Uniform 1 216.14 3.70 0 No
BDM 1 75.82 1.67 0 No
Uniform 2 1828.29 73.76 0 No
BDM 2 68.40 2.38 0 No
Uniform 3 1996.15 236.98 87 No
BDM 3 47.39 2.02 0 No
Uniform 1 292.94 7.47 0 Yes
BDM 1 78.66 2.23 14 Yes
Uniform 2 1808.77 99.79 22 Yes
BDM 2 65.41 2.36 20 Yes
Uniform 3 2070.83 354.82 94 Yes
BDM 3 49.63 1.91 25 Yes
ACKNOWLEDGMENTS
We thank Francisco Herna?ndez-Quiroz for his continous
support and acknowledge the financial support of the
Mexican Science and Technology Council (CONACYT),
the Posgrado en Ciencia e Ingenier??a de la Computatio?n,
UNAM, and the research grant 221341SEP-CONACYT. HZ
acknowledges the support of the Swedish Research Council
(Vetenskapsra?det) grant No. 2015-05299 “Reglering och En-
tropisk Styrning av Biologiska Na?tverk for Applicering pa?
Immunologi och Cancer”.
TABLE 6
Results obtained for the ‘Grid’.
Strategy Shifts Average SE Extinctions Replacements
Uniform 1 215.67 3.48 0 No
BDM 1 162.66 2.86 0 No
Uniform 2 1798.93 80.39 0 No
BDM 2 819.89 29.79 0 No
Uniform 3 1996.15 236.98 93 No
BDM 3 2763.40 583.03 95 No
Uniform 1 304.24 8.48 0 Yes
BDM 1 639.33 61.17 45 Yes
Uniform 2 2055.99 102.86 21 Yes
BDM 2 1372.00 201.63 84 Yes
Uniform 3 2469.38 207.54 92 Yes
BDM 3 NaN NaN 100 Yes
TABLE 7
Results obtained for the ‘Star’.
Strategy Shifts Average SE Extinctions Replacements
Uniform 1 217.30 2.22 0 No
BDM 1 172.63 2.23 0 No
Uniform 2 1811.84 85.41 0 No
BDM 2 1026.76 45.78 0 No
Uniform 3 1942.89 262.68 91 No
BDM 3 2577.27 392.37 89 No
Uniform 1 294.27 7.43 0 Yes
BDM 1 268.87 12.68 7 Yes
Uniform 2 1952.54 111.16 32 Yes
BDM 2 1099.40 74.36 27 Yes
Uniform 3 1953.33 440.85 94 Yes
BDM 3 2563.00 753.07 98 Yes
REFERENCES
[1] G. J. Chaitin, Proving Darwin: Making Biology Mathematical. Vin-
tage, 2013.
[2] S. Wolfram, A New Kind of Science. Wolfram Media Inc., 2002.
[3] H. Zenil, C. Gershenson, J. A. R. Marshall, and D. A. Rosenblueth,
“Life as thermodynamic evidence of algorithmic structure in nat-
ural environments,” Entropy, vol. 14, no. 11, 2012.
[4] H. Zenil and J. Marshall, “Some aspects of computation essential
to evolution and life,” Ubiquity, vol. April 2013, pp. 1–16.
[5] G. J. Chaitin, “Evolution of mutating software,” Bulletin of the
EATCS, vol. 97, pp. 157–164, 2009.
[6] ——, “Life as evolving software,” in A Computable Universe: Under-
standing and Exploring Nature as Computation, H. Zenil, Ed. World
Scientific Publishing Company, 10 2012, ch. 16.
[7] S. Herna?ndez-Orozco, F. H. Quiroz, and H. Zenil, “The
limits of decidable states on open-ended evolution and
emergence,” CoRR, vol. abs/1606.01810, 2016. [Online]. Available:
http://arxiv.org/abs/1606.01810
[8] M. Bedau, “Four puzzles about life,” ARTLIFE: Artificial Life, vol. 4,
1998.
[9] R. Solomonoff, “A formal theory of inductive inference. part i.”
Information and control, vol. 7(1), pp. 1–22, 1964.
[10] L. Levin, “Laws of information conservation (nongrowth) and
aspects of the foundation of probability theory,” Problemy Peredachi
Informatsii, vol. 10(3), pp. 30–35, 1974.
[11] G. J. Chaitin, “On the length of programs for computing finite
binary sequences,” Journal of the ACM (JACM), vol. 13(4), pp. 547–
569, 1966.
[12] A. Kolmogorov, “Three approaches to the quantitative definition
of information,” Problems Inform. Transmission, vol. 1, pp. 1–7, 1965.
[13] G. J. Chaitin, “Information-theoretic limitations of formal sys-
tems,” Journal of the ACM, vol. 21, no. 3, pp. 403–424, Jul. 1974.
[14] W. Kirchherr, M. Li, and P. Vita?nyi, “The miraculous universal
distribution,” The Mathematical Intelligencer, vol. 19, no. 4, pp. 7–
15, 1997.
[15] H. Zenil, F. Soler-Toscano, N. A. Kiani, S. Herna?ndez-Orozco, and
A. Rueda-Toicen, “A decomposition method for global evaluation
of shannon entropy and local estimations of algorithmic complex-
ity,” arXiv preprint arXiv:1609.00110, 2016.
13
[16] J.-P. D. F. Soler-Toscano, H. Zenil and N. Gauvrit, “Calculating
Kolmogorov complexity from the output frequency distributions
of small Turing machines,” PLoS ONE, vol. 9, no. 5, p. e96223,
2014.
[17] J.-P. Delahaye and H. Zenil, “Numerical evaluation of the com-
plexity of short strings: A glance into the innermost structure
of algorithmic randomness,” Applied Mathematics and Computation,
vol. 219, pp. 63–77, 2012.
[18] H. Zenil, F. Soler-Toscano, K. Dingle, and A. A. Louis, “Corre-
lation of automorphism group size and topological properties
with program-size complexity evaluations of graphs and complex
networks,” Physica A: Stat. Mechanics and its Applications, vol. 404,
pp. 341–358, Jun. 2014.
[19] A. M. Turing, “On computable numbers, with an application to
the Entscheidungsproblem,” Proceedings of the London Mathematical
Society, vol. 42, pp. 230–265, 1936.
[20] C. S. Calude, M. J. Dinneen, C.-K. Shu et al., “Computing a glimpse
of randomness,” Experimental Mathematics, vol. 11, no. 3, pp. 361–
370, 2002.
[21] H. Zenil, N. A. Kiani, and J. Tegne?r, “Low-algorithmic-complexity
entropy-deceiving graphs,” Physical Review E, vol. 96, no. 1, p.
012308, 2017.
[22] N. K. H. Zenil and J. Tegne?r, “Methods of information theory and
algorithmic complexity for network biology,” Seminars in Cell and
Developmental Biology, vol. 51, pp. 32–43, 2016.
[23] N. A. Kiani and L. Kaderali, “Dynamic probabilistic threshold net-
works to infer signaling pathways from time-course perturbation
data,” BMC Bioinformatics, vol. 15, no. 1, p. 250, 2014.
[24] Y. Yarden and M. Sliwkowski, “Untangling the ErbB signalling
network,” Nat Rev Mol Cell Biol., vol. 2, no. 2, pp. 127–37, 2001.
[25] H. L. N. H. M.A. Olayioye, R.M. Neve, “The ErbB signaling net-
work: receptor heterodimerization in development and cancer,”
EMBO J, vol. 19, no. 3, pp. 3159–67, 2000.
[26] H. Zenil and J.-P. Delahaye, “On the algorithmic nature of the
world,” in Information and Computation, G. Dodig-Crnkovic and
M. Burgin, Eds. Singapore, 2010, pp. 477–496.
[27] F. Soler-Toscano and H. Zenil, “A computable measure of algo-
rithmic probability by finite approximations with an application
to integer sequences,” Complexity (Accepted), 2017.
Santiago Herna?ndez-Orozco is a Computer
Science PhD student at Posgrado en Ciencias
e Ingenier??a de la Computacio?n at the Universi-
dad Nacional Auto?noma de Me?xico (UNAM). He
received a B.Sc. in Mathematics from UNAM in
2011 and a M.Sc. in Computer Science in 2014
from the same institution.
Narsis A. Kiani has a PhD in Mathematics and
has been a postdoctoral researcher at Dresden
University of Technology and at the University
of Heidelberg in Heidelberg, Germany. She was
a VINNOVA Marie Curie Fellow and is currently
an Assistant Professor and co-leader of the Al-
gorithmic Dynamics Lab, Unit of Computational
Medicine, Department of Medicine Solna, Cen-
tre for Molecular Medicine and Science for Life
Laboratory (SciLifeLab) in Stockholm, Sweden.
She is also at the Algorithmic Nature Lab, LA-
BORES, Paris, France.
Hector Zenil holds a PhD in Theoretical Com-
puter Science (CNRS/Lille 1) and a PhD in
Philosophy and Epistemology (IHPST/Paris 1
Sorbonne/CNRS/ENS Ulm). He is an Assistant
Professor and lab co-leader of the Algorithmic
Dynamics Lab, Unit of Computational Medicine,
Science for Life Laboratory (SciLifeLab), Karolin-
ska Institute in Stockholm, Sweden and the
Head of the Algorithmic Nature Lab, LABORES,
Paris, France. He held postdoctoral positions at
the Behavioural and Evolutionary Lab at the Uni-
versity of Sheffield before joining the University of Oxford as a Senior
Researcher (Faculty member) in the Department of Computer Science.
He is also the Editor of the Complex Systems journal, the first journal in
the field founded in 1987.
