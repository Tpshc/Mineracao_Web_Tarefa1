ar
X
iv
:1
70
8.
09
51
7v
1 
 [
cs
.I
T
] 
 3
1 
A
ug
 2
01
7
1
Upper and Lower Bounds on the Capacity of
Amplitude-Constrained MIMO Channels
Alex Dytso?, Mario Goldenbaum?, Shlomo Shamai (Shitz)†,
and H. Vincent Poor?
?Department of Electrical Engineering, Princeton University
†Department of Electrical Engineering, Technion – Israel Institute of Technology
Abstract
In this work, novel upper and lower bounds for the capacity of channels with arbitrary constraints on the
support of the channel input symbols are derived. As an immediate practical application, the case of multiple-input
multiple-output channels with amplitude constraints is considered. The bounds are shown to be within a constant
gap if the channel matrix is invertible and are tight in the high amplitude regime for arbitrary channel matrices.
Moreover, in the high amplitude regime, it is shown that the capacity scales linearly with the minimum between
the number of transmit and receive antennas, similarly to the case of average power-constrained inputs.
I. INTRODUCTION
While the capacity of a multiple-input multiple-output (MIMO) channel with an average power con-
straint is well understood [1], surprisingly, little is known about the capacity of the more practically relevant
case in which the channel inputs are subject to amplitude constraints. The first major contribution to this
problem was a seminal work of Smith [2], in which it was shown that, for the scalar Gaussian noise
channel with an amplitude-constraint, the capacity achieving input is discrete with finite support. In [3],
this result was extended to peak-power-limited quadrature Gaussian channels. Using the approach of [3],
in [4] the optimal input distribution was shown to be discrete for MIMO channels with an identity channel
matrix and a Euclidian norm constraint on the input vector. Even though the optimal input distribution is
known to be discrete, very little is known about the number or the optimal positions of the corresponding
constellation points. To the best of our knowledge, the only exception is the work of [5] in which for
a scalar Gaussian noise channel it was shown that two point masses are optimal for amplitude values
smaller than 1.671 and three for amplitude values of up to 2.786.
Using a dual capacity expression, in [6] McKellips derived an upper bound on the capacity of a scalar
amplitude-constrained channel that is asymptotically tight in the high amplitude regime. By using a clever
choice of an auxiliary channel output distribution in the dual capacity expression, the authors of [7]
sharpened McKellips’ bound and extended it to parallel MIMO channels with a Euclidian norm constraint
on the input. The scalar version of the upper bound in [7] has been further sharpened in [8] by yet another
choice of auxiliary output distribution. In [9], asymptotic lower and upper bounds for a 2 × 2 MIMO
system were presented and the gap between the bounds was specified.
In this work, we make progress on this open problem by deriving several new upper and lower bounds
that hold for channels with arbitrary constraints on the support of the input distribution. We then apply
them to the special case of MIMO channels with amplitude-constrained inputs.
This work was supported in part by the U. S. National Science Foundation under Grants CCF-1420575 and CNS-1456793, by the German
Research Foundation under Grant GO 2669/1-1, and by the European Union’s Horizon 2020 Research And Innovation Programme, grant
agreement no. 694630.
2
A. Contributions and Paper Outline
Our contributions and paper outline are as follows. The problem is stated in Section II. In Section III,
we derive upper and lower bounds on the capacity of a MIMO channel with an arbitrary constraint on
the support of the input. In Section IV, we evaluate the performance of our bounds by studying MIMO
channels with invertible channel matrices. In particular, in Theorem 8 it is shown that our upper and
lower bounds are within n log(?) bits, where ? is the packing efficiency and n is the number of antennas.
For diagonal channel matrices, it is shown in Theorem 9 that the Cartesian product of pulse-amplitude
modulation (PAM) constellations achieves the capacity to within 1.64n bits. Section V is devoted to
MIMO channels with arbitrary channel matrices. It is shown that in the high amplitude regime, similarly
to the average power-constrained channel, the capacity scales linearly with the minimum of the number
of transmit and receive antennas. Section VI concludes the paper.
B. Notation
Vectors are denoted by bold lowercase letters, random vectors by bold uppercase letters, and matrices
by bold uppercase sans serif letters (e.g., x, X, X). For any deterministic vector x ? Rn, n ? N, we
denote the Euclidian norm of x by ?x?. For some X ? supp(X) ? Rn and any p > 0 we define
?X?pp :=
1
n
E[?X?p] , (1)
where supp(X) denotes the support of X. Note that for p ? 1, the quantity in (1) defines a norm. The
norm of a matrix H ? Rn×n is defined as
?H? := sup
x:x 6=0
?Hx?
?x? .
Let S be a subset of Rn. Then,
Vol(S) :=
?
S
dx
denotes its volume.
Let R+ := {x ? R : x ? 0}. We define an n-dimensional ball or radius r ? R+ centered at x ? Rn as
the set
Bx(r) := {y : ?x? y? ? r} .
Recall that for any x ? Rn and r ? R+,
Vol
(
Bx(r)
)
=
?
n
2
?
(
n
2
+ 1
)rn .
For any matrix H ? Rk×n and some S ? Rn we define
HS := {y : y = Hx , x ? S} .
Note that for an invertible H ? Rn×n we have
Vol(HS) = |det(H)|Vol(S) .
We define the maximum and minimum radius of a set S ? Rn that contains the origin as
rmax(S) := min{r ? R+ : S ? B0(r)} ,
rmin(S) := max{r ? R+ : B0(r) ? S} .
For a given vector a = (a1, . . . , an) ? Rn+ we define
Box(a) := {x ? Rn : |xi| ? ai, i = 1, . . . , n}
and the smallest box containing a given set S ? Rn as
Box(S) := inf{Box(a) : S ? Box(a)} ,
respectively. Finally, all logarithms are taken to the base 2, log+(x) := max{log(x), 0}, Q(x), x ? R,
denotes the Q-function, and ?x(y) the Kronecker delta, which is one for x = y and zero otherwise.
3
II. PROBLEM STATEMENT
Consider a MIMO system with nt ? N transmit and nr ? N receive antennas. The corresponding
nr-dimensional channel output for a single channel use is of the form
Y = HX+ Z ,
for some fixed channel matrix H ? Rnr×nt .1 Here and hereafter, we assume Z ? N (0, Inr) is independent
of the channel input X ? Rnt and H is known to both the transmitter and the receiver, where Inr denotes
the nr × nr identity matrix.
Now, in all that follows let X ? Rnt be a convex and compact channel input space that contains the
origin (i.e., the length-nt zero vector) and let FX denote the cumulative distribution function of X. As of
the writing of this paper, the capacity
C(X ,H) := max
FX:X?X
I(X;HX+ Z) (2)
of this channel is unknown and we are interested in finding novel lower and upper bounds. Even though
most of the results in this paper hold for arbitrary X , we are mainly interested in the two most important
special cases:
(i) per-antenna amplitude constraints; that is, X = Box(a) for some given a = (A1, . . . , Ant) ? Rnt+ ,
(ii) nt-dimensional amplitude constraint; that is, X = B0(A) for some given A ? R+.
Remark 1. Note that determining the capacity of a MIMO channel with average per-antenna power
constraints is also still an open problem and has been solved for some special cases only [?], [10]–[12].
III. UPPER AND LOWER BOUNDS ON THE CAPACITY
A. Upper Bounds
To establish our first upper bound on (2), we need the following result [14, Th. 1]:
Lemma 1. (Maximum Entropy Under p-th Moment Constraint) Let n ? N and p ? (0,?) be arbitrary.
Then, for any U ? Rn such that h(U) <? and ?U?p <?, we have
h(U) ? n log
(
kn,p n
1
p ?U?p
)
,
where
kn,p :=
?
? e
1
p
(
p
n
)
1
p?
(
n
p
+ 1
)
1
n
?
(
n
2
+ 1
)
1
n
.
Theorem 1. (Moment Upper Bound) For any channel input space X and any fixed channel matrix H,
we have
C(X ,H) ? C?M(X ,H) := inf
p>0
nr log
(
knr,p
(2?e)
1
2
n
1
p
r ?x?+ Z?p
)
,
where x? ? HX is chosen such that ?x?? = rmax(HX ).
Proof: Expressing (2) in terms of differential entropies results in
C(X ,H) = max
FX:X?X
h(HX+ Z)? h(Z)
a)
? max
FX:X?X
nr log
(
knr,p
(2?e)
1
2
n
1
p
r ?HX+ Z?p
)
b)
= nr log
(
knr,p
(2?e)
1
2
n
1
p
r max
FX:X?X
?HX+ Z?p
)
, (3)
1Considering a real-valued channel model is without loss of generality.
4
where a) follows from Lemma 1 with the fact that h(Z) = nr
2
log(2?e) and b) from the monotonicity of
the logarithm.
Now, notice that ?HX+Z?p is linear and bounded (and therefore continuous) in FX so that it attains its
maximum at an extreme point of the set FX := {FX : X ? X} (i.e., the set of all cumulative distribution
functions of X). As a matter of fact [15], the extreme points of FX are given by the set of degenerate
distributions on X ; that is, {FX(y) = ?x(y),y ? X}x?X . This allows us to conclude
max
FX:X?X
?HX+ Z?p = max
x?X
?Hx + Z?p .
Observe that the Euclidian norm is a convex function, which is therefore maximized at the boundary of
the set HX . Combining this with (3) and taking the infimum over p > 0 completes the proof.
The following theorem provides two alternative upper bounds that are based on duality arguments.
Theorem 2. (Duality Upper Bounds) For any channel input space X and any fixed channel matrix H
C(X ,H) ? C?Dual,1(X ,H) := log
(
cnr(d) +
Vol
(
B0(d)
)
(2?e)
nr
2
)
, (4)
where
d := rmax(HX ) , cnr(d) :=
nr?1
?
i=1
(
nr ? 1
i
)
?
(
nr?1
2
)
2
nr
2 ?
(
nr
2
)di ,
and
C(X ,H) ? C?Dual,2(X ,H) :=
nr
?
i=1
log
(
1 +
2Ai?
2?e
)
, (5)
where a = (A1, . . . , Anr) such that Box(a) = Box(HX ).
Proof: Using duality bounds, it has been shown in [7] that for any centered n-dimensional ball of
radius r ? R+
max
FX:X?B0(r)
I(X;X+ Z) ? log
(
cn(r) +
Vol
(
B0(r)
)
(2?e)
n
2
)
, (6)
where cn(r) :=
?n?1
i=1
(
n?1
i
) ?(n?12 )
2
n
2 ?(n2 )
ri.
Now, observe that
C(X ,H) = max
FX:X?X
h(HX+ Z)? h(HX+ Z|HX)
= max
FX:X?X
I(HX;HX+ Z)
= max
F
X?
:X??HX
I(X?; X?+ Z) (7)
a)
? max
F
X?
:X??B0(d),d:=rmax(HX )
I(X?; X?+ Z)
b)
? log
(
cnr(d) +
Vol
(
B0(d)
)
(2?e)
nr
2
)
.
Here, a) follows from enlarging the optimization domain and b) from using the upper bound in (6). This
proves (4).
5
In order to show the upper bound in (5), we proceed with an alternative upper bound to (7):
C(X ,H) = max
F
X?
:X??HX
I(X?; X?+ Z)
a)
? max
F
X?
:X??Box(HX )
I(X?; X?+ Z)
b)
? max
F
X?
:X??Box(HX )
nr
?
i=1
I(X?i; X?i + Zi)
c)
=
nr
?
i=1
max
F
X?i
:|X?i|?Ai
I(X?i; X?i + Zi)
d)
?
nr
?
i=1
log
(
1 +
2Ai?
2?e
)
,
where the (in)equalities follow from: a) enlarging the optimization domain; b) single-letterizing the mutual
information; c) choosing individual amplitude constraints (A1, . . . , Anr) =: a ? Rnr+ such that Box(a) =
Box(HX ); and d) using the upper bound in (6) for n = 1. This concludes the proof.
B. Lower Bounds
A classical approach to bound a mutual information from below is to use the entropy power inequality
(EPI).
Theorem 3. (EPI Lower Bounds) For any fixed channel matrix H and any channel input space X with
X absolutely continuous, we have
C(X ,H) ? CEPI(X ,H) := max
FX:X?X
nr
2
log
(
1 +
2
2
nr
h(HX)
2?e
)
. (8)
Moreover, if nt = nr = n, H ? Rn×n is invertible, and X is uniformly distributed over X , then
C(X ,H) ? CEPI(X ,H) :=
n
2
log
(
1 +
|det(H)| 2nVol(X ) 2n
2?e
)
. (9)
Proof: By means of the EPI
2
2
nr
h(HX+Z) ? 2 2nr h(HX) + 2 2nr h(Z) ,
we conclude
2
2
nr
C(X ,H) ? 1 + (2?e)?12
2
nr
max
FX:X?X
h(HX)
, (10)
which finalizes the proof of the lower bound in (8).
To show the lower bound in (9), all we need is to recall that
h(HX) = h(X) + log |det(H)| ,
which is maximized for X uniformly distributed over X . But if X is uniformly drawn from X , we have
2
2
n
h(HX) = Vol(HX ) 2n = |det(H)| 2nVol(X ) 2n ,
which completes the proof.
The results in [2]–[4] suggest that the channel input distribution that maximizes (2) might be discrete.
Therefore, there is a need for lower bounds that, unlike the bounds in Theorem 3, rely on discrete inputs.
6
Remark 2. We note that the problem of finding the optimal input distribution of a general MIMO channel
with an amplitude constraint is still open. The technical difficulty relies on the fact that the identity theorem
from complex analysis, a key tool in the method developed by Smith [2] for the scalar case and later
used by [16] for the MIMO channel, does not extend to Rn and Cn. The interested reader is referred to
[17] for a detailed discussion on this issue with examples of why the identity theorem fails in the MIMO
setting.
Theorem 4. (Ozarow-Wyner Type Lower Bound) Let XD ? supp(XD) ? Rnt be a discrete random
vector of finite entropy, g : Rnr ? Rnt a measurable function, and p > 0. Furthermore, let Kp be a set
of continuous random vectors, independent of XD, such that for every U ? Kp we have h(U) < ?,
?U?p <?, and
supp(U+ xi) ? supp(U + xj) = ? (11)
for all xi,xj ? supp(XD), i 6= j. Then,
C(X ,H) ? COW(X ,H) := [H(XD)? gap]+ ,
where
gap := inf
U?Kp
g measurable
p>0
(
G1,p(U,XD, g) +G2,p(U)
)
with
G1,p(U,XD, g) := nt log
(?U+XD ? g(Y)?p
?U?p
)
, (12)
G2,p(U) := nt log
?
?
knt,p n
1
p
t ?U?p
2
1
nt
h(U)
?
? , (13)
and knt,p as defined in Lemma 1, respectively.
Proof: The proof is identical to [14, Th. 2]. In order to make the manuscript more self-contained, we
repeat it here.
Let U and XD be statistically independent. Then, the mutual information I(XD;Y) can be lower
bounded as
I(XD;Y)
a)
? I(XD +U;Y)
= h(XD +U)? h(XD +U|Y)
b)
= H(XD) + h(U)? h(XD +U|Y) . (14)
Here, a) follows from the data processing inequality as XD +U ? XD ? Y forms a Markov chain in
that order and b) from the assumption in (11). By using Lemma 1, we have that the last term in (14) can
be bounded from above as
h(XD +U|Y) ? nt log
(
knt,p n
1
p
t ?XD +U? g(Y)?p
)
.
Combining this expression with (14) results in
I(XD;Y) ? H(XD)?
(
G1,p(U,XD, g) +G2,p(U)
)
,
with G1,p and G2,p as defined in (12) and (13), respectively. Maximizing the right-hand side over all
U ? Kp, measurable functions g : Rnr ? Rnt , and p > 0 provides the bound.
Interestingly, the bound of Theorem 4 holds for arbitrary channels and the interested reader is referred to
[14] for details.
7
We conclude the section by providing a lower bound that is based on Jensen’s inequality and holds for
arbitrary inputs.
Theorem 5. (Jensen’s Inequality Lower Bound) For any channel input space X and fixed channel matrix
H, we have
C(X ,H) ? CJensen(X ,H) := max
FX:X?X
log+
(
(
2
e
)
nr
2
E
[
exp
(
??H(X?X
?)?2
4
)]?1
)
,
where X? is an independent copy of X.
Proof: In order to show the lower bound, we follow an approach of [18]. Note that by Jensen’s
inequality
h(Y) = ?E[log fY(Y)] ? ? logE[fY(Y)] = ? log
?
Rnr
fY(y)fY(y) dy . (15)
Now, evaluating the integral in (15) results in
?
Rnr
fY(y)fY(y) dy =
1
(2?)nr
?
Rnr
E
[
e?
?y?HX?2
2
]
E
[
e?
?y?HX??2
2
]
dy
a)
=
1
(2?)nr
E
[?
Rnr
e?
?y?HX?2+?y?HX??2
2 dy
]
b)
=
1
(2?)nr
E
[
e?
?HX?HX??2
4
?
Rnr
e??y?
H(X?X?)
2
?2dy
]
c)
=
1
2nr?
nr
2
E
[
e?
?H(X?X?)?2
4
]
, (16)
where a) follows from the independence of X and X? and Tonelli’s theorem, b) from completing a square,
and c) from the fact that
?
Rnr
e??y?
H(X?X?)
2
?2dy =
?
Rnr
e??y?
2
dy = ?
nr
2 .
Finally, combining (15) and (16), subtracting h(Z) = nr
2
log(2?e), and maximizing over FX proves the
result.
IV. INVERTIBLE CHANNEL MATRICES
Consider the case of nt = nr = n antennas with H ? Rn×n being invertible. In this section, we evaluate
some of the lower and upper bounds given in the previous section for the special case of H being also
diagonal and then characterize the gap to the capacity for arbitrary invertible channel matrices.
A. Diagonal Channel Matrices
Suppose the channel inputs are subject to per-antenna or an n-dimensional amplitude constraint. Then,
the duality upper bound C?Dual,2(X ,H) of Theorem 2 is of the following form.
Theorem 6. (Upper Bounds) Let H = diag(h11, . . . , hnn) ? Rn×n be fixed. If X = Box(a) for some
a = (A1, . . . , An) ? Rn+, then
C?Dual,2(Box(a),H) =
n
?
i=1
log
(
1 +
2|hii|Ai?
2?e
)
. (17)
Moreover, if X = B0(A) for some A ? R+, then
C?Dual,2(B0(A),H) =
n
?
i=1
log
(
1 +
2|hii|A?
n
?
2?e
)
. (18)
8
Proof: The bound in (17) immediately follows from Theorem 2 by observing that Box(HBox(a)) =
Box(Ha). The bound in (18) follows from Theorem 2 by the fact that
Box
(
HB0(A)
)
? Box
(
HBox
(
B0(A)
))
= Box(h) ,
where h := A?
n
(|h11|, . . . , |hnn|). This concludes the proof.
For an arbitrary channel input space X , the EPI lower bound of Theorem 3 and Jensen’s inequality
lower bound of Theorem 5 evaluate to the following.
Theorem 7. (Lower Bounds) Let H = diag(h11, . . . , hnn) ? Rn×n be fixed and X arbitrary. Then,
CJensen(X ,H) = log+
(
(
2
e
)
n
2 1
?(H,b?)
)
, (19)
where
?(H,b?) := min
b?X
n
?
i=1
?(|hii|Bi)
with b := (B1, . . . , Bn) and ? : R+ ? R+,
?(x) :=
1
x2
(
e?x
2 ? 1 +??x
(
1? 2Q(
?
2x)
)
)
, (20)
and
CEPI(X ,H) =
n
2
log
(
1 + Vol(X ) 2n |
?n
i=1 hii|
2
n
2?e
)
. (21)
Proof: For some given values Bi ? R+, i = 1, . . . , n, let the i-th component of X = (X1, . . . , Xn)
be independent and uniformly distributed over the interval [?Bi, Bi]. Thus, the expected value appearing
in the bound of Theorem 5 can be written as
E
[
e?
?H(X?X?)?2
4
]
= E
[
e?
?n
i=1 h
2
ii(Xi?X
?
i)
2
4
]
= E
[
n
?
i=1
e?
h2ii(Xi?X
?
i)
2
4
]
=
n
?
i=1
E
[
e?
h2ii(Xi?X
?
i)
2
4
]
. (22)
Now, if X? is an independent copy of X, it can be shown that the expected value at the right-hand side
of (22) is of the explicit form
E
[
e?
h2ii(xi?x
?
i)
2
4
]
= ?(|hii|Bi)
with ? as defined in (20). Finally, optimizing over all b = (B1, . . . , Bn) ? X results in the bound (19).
The bound in (21) follows by inserting |det(H)| = |?ni=1 hii| into (9), which concludes the proof.
In Fig. 1, the upper bounds of Theorems 1 and 6 and the lower bounds of Theorem 7 are depicted for a
diagonal 2×2 MIMO channel with per-antenna amplitude constraints. It turns out that the moment upper
bound and the EPI lower bound perform well in the small amplitude regime while the duality upper bound
and Jensen’s inequality lower bound perform well in the high amplitude regime (note that the Jensen’s
inequality lower bound becomes strictly positive around 9 dB).
9
0 5 10 15 20 25 30
0
5
10
15
Amplitude Constraint, A, in dB
R
at
e
in
b
it
s/
s/
H
z
Moment upper bound, C?M
Duality upper bound, C?Dual,2
Jensen’s lower bound, CJensen
EPI lower bound, CEPI
Fig. 1. Comparison of the upper and lower bounds of Theorems 1, 6, and 7 evaluated for a 2×2 MIMO system with per-antenna amplitude
constraints A1 = A2 = A (i.e., a = (A,A)) and channel matrix H = ( 0.3 00 0.1 ).
B. Gap to the Capacity
Our first result bounds the gap between the capacity (2) and the lower bound in (9).
Theorem 8. Let H ? Rn×n be of full rank and
?(X ,H) := Vol
(
B0 (rmax(HX ))
)
Vol(HX ) .
Then,
C(X ,H)? CEPI(X ,H) ?
n
2
log
(
(?n)
1
n?(X ,H) 2n
)
.
Proof: For notational convenience let the volume of an n-dimensional ball of radius r > 0 be denoted
as
Vn(r) := Vol
(
B0(r)
)
= Vn(1)r
n =
?
n
2 rn
?
(
n
2
+ 1
) .
Now, observe that by choosing p = 2, the upper bound of Theorem 1 can further be upper bounded as
C?M(X ,H) ? n log
(
kn,2
(2?e)
1
2
n
1
2?x?+ Z?2
)
a)
=
n
2
log
(
1
n
E
[
?x?+ Z?2
]
)
b)
=
n
2
log
(
1 +
1
n
?x??2
)
,
10
where a) follows since kn,2 =
?
2?e
n
and b) since E[?Z?2] = n. Therefore, the gap between (9) and the
moment upper bound of Theorem 1 can be upper bounded as follows:
C?M(X ,H)? CEPI(X ,H) =
n
2
log
?
?
1 + 1
n
?x??2
1 + Vol(HX )
2
n
2?e
?
?
a)
=
n
2
log
?
?
?
1 + 1
n
(
Vn(?x??)
Vn(1)
)
2
n
1 + Vol(HX )
2
n
2?e
?
?
?
=
n
2
log
?
?
?
1 + 1
n
(
?(X ,H)Vol(HX )
Vn(1)
)
2
n
1 + Vol(HX )
2
n
2?e
?
?
?
b)
? n
2
log
(
1
n
2?e
(
?(X ,H)
Vn(1)
)
2
n
)
c)
? n
2
log
(
(?n)
1
n?(X ,H) 2n
)
.
Here, a) is due to the fact that ?x?? is the radius of an n-dimensional ball, b) follows from the inequality
1+cx
1+x
? c for c ? 1 and x ? R+, and c) follows from using Stirling’s approximation to obtain
(
1
Vn(1)
)
2
n ?
1
2e?1?
1
n
n1+
1
n .
The term ?(X ,H) is referred to as the packing efficiency of the set HX . In the following proposition,
we present the packing efficiencies for important special cases.
Proposition 1. (Packing Efficiencies) Let H ? Rn×n be of full rank, A ? R+, and a := (A1, . . . , An) ?
Rn+. Then,
?
(
B0(A), In
)
= 1 , (23)
?
(
B0(A),H
)
=
?H?n
|det(H)| , (24)
?
(
Box(a), In
)
=
?
n
2
?
(
n
2
+ 1
)
?a?n
?n
i=1Ai
, (25)
?
(
Box(a),H
)
? ?
n
2
?
(
n
2
+ 1
)
?H?n?a?n
|det(H)|?ni=1Ai
. (26)
Proof: The packing efficiency (23) follows immediately. Note that
rmax
(
HB0(A)
)
= max
x?B0(A)
?Hx? = ?H?A .
Thus, as H is assumed to be invertible we have Vol(HB0(A)) = |det(H)|Vol(B0(A)), which results in
(24). To show (25), observe that
Vol
(
B0
(
rmax
(
InBox(a)
)))
= Vol
(
B0(?a?)
)
=
?
n
2
?
(
n
2
+ 1
)?a?n .
The proof of (25) is concluded by observing that Vol(InBox(a)) =
?n
i=1Ai. Finally, observe that Box(a) ?
B0(?a?) implies rmax(HBox(a)) ? rmax(HB0(?a?)) so that
?
(
H,Box(a)
)
? Vol
(
B0(?H??a?)
)
Vol
(
HBox(a)
) =
?
n
2
?
(
n
2
+ 1
)
?H?n?a?n
|det(H)|?ni=1Ai
,
11
    
A?A X
2?
0
Fig. 2. Example of a pulse-amplitude modulation constellation with N = 4 points and amplitude constraint A (i.e., PAM(4, A)), where
? := A/(N ? 1) denotes half the Euclidean distance between two adjacent constellation points. In case N is odd, 0 is a constellation point.
which is the bound in (26).
We conclude this section by characterizing the gap to the capacity when H is diagonal and the channel
input space is the Cartesian product of n PAM constellations. In this context, PAM(N,A) refers to the
set of N ? N equidistant PAM-constellation points with amplitude constraint A ? R+ (see Fig. 2 for an
illustration), whereas X ? PAM(N,A) means that X is uniformly distributed over PAM(N,A) [14].
Theorem 9. Let H = diag(h11, . . . , hnn) ? Rn×n be fixed and X = (X1, . . . , Xn). Then, if Xi ?
PAM(Ni, Ai), i = 1, . . . , n, for some given a = (A1, . . . , An) ? Rn+, it holds that
C?Dual,2(Box(a),H)? COW(Box(a),H) ? c · n bits , (27)
where Ni :=
?
1 + 2Ai|hii|?
2?e
?
and
c := log(2) +
1
2
log
(?e
6
)
+
1
2
log
(
1 +
6
?e
)
? 1.64 .
Moreover, if Xi ? PAM(Ni, A), i = 1, . . . , n, for some given A ? R+, it holds that
C?Dual,2(B0(A),H)? COW(B0(A),H) ? c · n bits , (28)
where Ni :=
?
1 + 2A|hii|?
n
?
2?e
?
.
Proof: Since the channel matrix is diagonal, letting the channel input X be such that its elements
Xi, i = 1, . . . , n, are independent we have that
I(X;HX+ Z) =
n
?
i=1
I(Xi; hiiXi + Zi) .
Let Xi ? PAM(Ni, Ai) with Ni :=
?
1 + 2Ai|hii|?
2?e
?
and observe that half the Euclidean distance between
any pair of adjacent points in PAM(Ni, Ai) is equal to ?i := Ai/(Ni ? 1) (see Fig. 2), i = 1, . . . , n. In
order to lower bound the mutual information I(Xi; hiiXi+Zi), we use the bound of Theorem 4 for p = 2
and nt = 1. Thus, for some continuous random variable U that is uniformly distributed over the interval
[??i,?i) and independent of Xi we have that
I(Xi; hiiXi + Zi) ? H(Xi)?
1
2
log
(?e
6
)
? 1
2
log
(
E
[
(U +Xi ? g(Yi))2
]
E[U2]
)
. (29)
Now, note that the entropy term in (29) can be lower bounded as
H(Xi) = log
(?
1 +
2Ai|hii|?
2?e
?)
? log
(
1 +
2Ai|hii|?
2?e
)
+ log(2) , (30)
12
where we have used that ?x? ? x
2
for every x ? 1. On the other hand, the last term in (29) can be upper
bounded by upper bounding its argument as follows:
E
[
(U +Xi ? g(Yi))2
]
E[U2]
a)
= 1 +
3E
[
(Xi ? g(Yi))2
]
?2i
b)
? 1 + 3E[Z
2
i ](Ni ? 1)2
A2i |hii|2
= 1 +
3(Ni ? 1)2
A2i |hii|2
c)
? 1 +
3
(
2Ai|hii|?
2?e
)2
A2i |hii|2
= 1 +
6
?e
. (31)
Here, a) follows from using that Xi and U are independent and E[U
2] =
?2i
3
, b) from using the estimator
g(Yi) =
1
hii
Yi, and c) from Ni =
?
1 + 2Ai|hii|?
2?e
?
? 1 + 2Ai|hii|?
2?e
. Combining (29), (30), and (31) results in
the gap (27).
The proof of the gap in (28) follows along similar lines, which concludes the proof.
V. ARBITRARY CHANNEL MATRICES
For a MIMO channel with an arbitrary channel matrix and an average power constraint, the capacity
is achieved by a singular value decomposition (SVD) of the channel matrix (i.e., H = U?VT ) and
considering the equivalent channel model
Y? = ?X?+ Z? ,
where Y? := UTY, X? := VTX, and Z? := UTZ, respectively.
To provide lower bounds for channels with amplitude constraints and SVD precoding, we need the
following lemma.
Lemma 2. For any given orthogonal matrix V ? Rnt×nt and constraint vector a = (A1, . . . , Ant) ? Rnt+
there exists a distribution FX of X such that X? = V
TX is uniformly distributed over Box(a). Moreover,
the components X?1, . . . , X?nt of X? are mutually independent with X?i uniformly distributed over [?Ai, Ai],
i = 1, . . . , nt.
Proof: Suppose that X? is uniformly distributed over Box(a); that is, the density of X? is of the form
fX?(x?) =
1
Vol
(
Box(a)
) , x? ? Box(a) .
Since V is orthogonal, we have VX? = X and by the change of variable theorem for x ? VBox(a)
fX(x) =
1
|det(V)|fX?(V
Tx) =
1
|det(V)|Vol
(
Box(a)
) =
1
Vol
(
Box(a)
)
Therefore, such a distribution of X exists.
Theorem 10. (Lower Bounds with SVD Precoding) Let H ? Rnr×nt be fixed, nmin := min(nr, nt), and
X = Box(a) for some a = (A1, . . . , Ant) ? Rnt+ . Furthermore, let ?i, i = 1, . . . , nmin, be the i-th singular
value of H. Then,
CJensen(Box(a),H) = log
+
(
(
2
e
)
nmin
2 1
?(H,b?)
)
(32)
13
and
CEPI(Box(a),H) =
nmin
2
log
(
1 +
|?nmini=1 Ai?i|
2
nmin
2?e
)
, (33)
where
?(H,b?) := min
b?Box(a)
nmin
?
i=1
?(?iBi)
with b := (B1, . . . , Bnt) and ? as defined in (20).
Proof: Performing the SVD, the expected value in Theorem 5 can be written as
E
[
e?
?H(X?X?)?2
4
]
= E
[
e?
?U?VT (X?X?)?2
4
]
= E
[
e?
??VT (X?X?)?2
4
]
= E
[
e?
??(X??X??)?2
4
]
.
By Lemma 2 there exists a distribution FX such that the components of X? are independent and uniformly
distributed. Since ? is a diagonal matrix, we can use Theorem 7 to arrive at (32).
Note that by Lemma 2 there exists a distribution on X such that X? is uniform over Box(a) ? Rnt and
?X? is uniform over ?Box(a) ? Rnmin , respectively. Therefore, by the EPI lower bound given in (8) we
obtain
CEPI(Box(a),H) =
nmin
2
log
(
1 +
2
2
nmin
h(?X?)
2?e
)
=
nmin
2
log
?
?1 +
Vol
(
?Box(a)
)
2
nmin
2?e
?
?
=
nmin
2
log
(
1 +
(
?nmin
i=1 Ai)
2
nmin |?nmini=1 ?i|
2
nmin
2?e
)
,
which is exactly the expression in (33). This concludes the proof.
Remark 3. Notice that choosing the optimal b for the lower bound (32) is an amplitude allocation
problem, which is reminiscent of waterfilling in the average power constraint case. It would be interesting
to study whether the bound in (32) is connected to what is called mercury waterfilling in [19], [20].
In Fig. 3, the lower bounds of Theorem 10 are compared to the moment upper bound of Theorem 1 for
the special case of a 3 × 1 MIMO channel. Similarly to the example presented in Fig. 1, the EPI lower
bound performs well in the low amplitude regime while Jensen’s inequality lower bound performs well
in the high amplitude regime.
We conclude this section by showing that for an arbitrary channel input space X , in the large amplitude
regime the capacity pre-log is given by min(nr, nt).
Theorem 11. Let X be arbitrary and H ? Rnr×nt fixed. Then,
lim
rmin(X )??
C(X ,H)
log
(
1 + 2rmin(X )?
2?e
) = min(nr, nt) .
14
0 2 4 6 8 10 12 14 16
0
1
2
3
Amplitude Constraint, A, in dB
R
at
e
in
b
it
s/
s/
H
z
Moment upper bound
Jensen’s lower bound + SVD
EPI lower bound + SVD
Fig. 3. Comparison of the upper bound in Theorem 1 to the lower bounds of Theorem 10 for a 3 × 1 MIMO system with amplitude
constraints A1 = A2 = A3 = A (i.e., a = (A,A,A)) and channel matrix H = (0.6557, 0.0357, 0.8491).
Proof: Notice that there always exists a ? Rnt+ and c ? R+ such that Box(a) ? X ? cBox(a). Thus,
without loss generality we can consider X = Box(a), a = (A, . . . , A), for sufficiently large A ? R+. To
prove the result we therefore start with enlarging the constraint set of the bound in (5):
Box
(
HBox(a)
)
? B0
(
rmax
(
HBox(a)
))
? B0
(
rmax
(
HB0(
?
ntA)
))
= B0
(
rmax
(
U?V
TB0(
?
ntA)
))
= B0
(
rmax
(
U?B0(
?
ntA)
))
= B0
(
rmax
(
?B0(
?
ntA)
))
? B0(r)
? Box(a?) ,
where r :=
?
ntA
?
?nmin
i=1 ?
2
i and a
? :=
(
r?
nmin
, . . . , r?
nmin
)
? Rnmin+ . Therefore, by using the upper bound
in (5) it follows that
C(Box(a),H) ?
nr
?
i=1
log
(
1 +
2Ai?
2?e
)
? nmin log
(
1 +
2?
2?e
?
ntA
?
?nmin
i=1 ?
2
i?
nmin
)
.
Moreover,
lim
A??
C(Box(a),H)
log
(
1 + 2A?
2?e
) ? nmin lim
A??
log
(
1 + 2?
2?e
?
ntA
??nmin
i=1 ?
2
i?
nmin
)
log
(
1 + 2A?
2?e
) = nmin .
Next, using the EPI lower bound in (33), we have that
lim
A??
CEPI(Box(a),?)
log
(
1 + 2A?
2?e
) = nmin lim
A??
1
2
log
(
1 +
A|?nmini=1 ?i|
2
nmin
2?e
)
log
(
1 + 2A?
2?e
) = nmin .
This concludes the proof.
15
VI. CONCLUSION
In this work, we have focused on studying the capacity of MIMO systems with bounded channel input
spaces. Several new upper and lower bounds have been proposed and it has been shown that the lower
and upper bounds are tight in the high amplitude regime. An interesting direction for future work is to
determine the exact scaling in the massive MIMO regime (i.e., nmin ? ?).
Another interesting future direction is to study generalizations of our techniques to MIMO wireless
optical channels [21].
REFERENCES
[1] I. E. Telatar, “Capacity of multi-antenna Gaussian channels,” Europ. Trans. Telecommu., vol. 10, no. 6, pp. 585–595, Nov./Dec. 1999.
[2] J. G. Smith, “The information capacity of amplitude- and variance-constrained scalar Gaussian channels,” Inf. Control, vol. 18, no. 3,
pp. 203–219, Apr. 1971.
[3] S. Shamai (Shitz) and I. Bar-David, “The capacity of average and peak-power-limited quadrature Gaussian channels,” IEEE Trans. Inf.
Theory, vol. 41, no. 4, pp. 1060–1071, Jul. 1995.
[4] B. Rassouli and B. Clerckx, “On the capacity of vector Gaussian channels with bounded inputs,” IEEE Trans. Inf. Theory, vol. 62,
no. 12, pp. 6884–6903, Dec. 2016.
[5] N. Sharma and S. Shamai (Shitz), “Transition points in the capacity-achieving distribution for the peak-power limited AWGN and
free-space optical intensity channels,” Probl. Inf. Transm., vol. 46, no. 4, pp. 283–299, 2010.
[6] A. L. McKellips, “Simple tight bounds on capacity for the peak-limited discrete-time channel,” in Proc. IEEE Int. Symp. Inf. Theory
(ISIT), Chicago, IL, Jul. 2004, pp. 348–348.
[7] A. Thangaraj, G. Kramer, and G. Böcherer. (2015) Capacity bounds for discrete-time, amplitude-constrained, additive white Gaussian
noise channels. [Online]. Available: https://arxiv.org/abs/1511.08742v1
[8] B. Rassouli and B. Clerckx, “An upper bound for the capacity of amplitude-constrained scalar AWGN channel,” IEEE Commun. Lett.,
vol. 20, no. 10, pp. 1924–1926, Oct. 2016.
[9] A. ElMoslimany and T. M. Duman, “On the capacity of multiple-antenna systems and parallel Gaussian channels with amplitude-limited
inputs,” IEEE Trans. Commun., vol. 64, no. 7, pp. 2888–2899, Jul. 2016.
[10] M. Vu, “MISO capacity with per-antenna power constraint,” IEEE Trans. Commun., vol. 59, no. 5, pp. 1268–1274, May 2011.
[11] D. Tuninetti, “On the capacity of the AWGN MIMO channel under per-antenna power constraints,” in Proc. IEEE Int. Conf. Commun.
(ICC), Sydney, Australia, Jun. 2014, pp. 2153–2157.
[12] S. Loyka, “The capacity of Gaussian MIMO channels under total and per-antenna power constraints,” IEEE Trans. Commun., vol. 65,
no. 3, pp. 1035–1043, Mar. 2017.
[13] P. L. Cao and T. J. Oechtering, “Optimal transmit strategy for MIMO channels with joint sum and per-antenna power constraints,” in
Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., New Orleans, LA, USA, Mar. 2017, pp. 3569–3573.
[14] A. Dytso, M. Goldenbaum, H. V. Poor, and S. Shamai (Shitz), “A generalized Ozarow-Wyner capacity bound with applications,” in
Proc. IEEE Int. Symp. Inf. Theory (ISIT), Aachen, Germany, Jun. 2017, pp. 1058–1062.
[15] H. S. Witsenhausen, “Some aspects of convexity useful in information theory,” IEEE Trans. Inf. Theory, vol. 26, no. 3, pp. 265–271,
May 1980.
[16] T. H. Chan, S. Hranilovic, and F. R. Kschischang, “Capacity-achieving probability measure for conditionally Gaussian channels with
bounded inputs,” IEEE Trans. Inf. Theory, vol. 51, no. 6, pp. 2073–2088, Mar. 2005.
[17] J. Sommerfeld, I. Bjelakovic?, and H. Boche, “On the boundedness of the support of optimal input measures for Rayleigh fading
channels,” in Proc. IEEE Int. Symp. Inf. Theory (ISIT), Toronto, Canada, Jul. 2008, pp. 1208–1212.
[18] A. Dytso, D. Tuninetti, and N. Devroye, “Interference as noise: Friend or foe?” IEEE Trans. Inf. Theory, vol. 62, no. 6, pp. 3561–3596,
Jun. 2016.
[19] A. Lozano, A. M. Tulino, and S. Verdú, “Optimum power allocation for parallel Gaussian channels with arbitrary input distributions,”
IEEE Trans. Inf. Theory, vol. 52, no. 7, pp. 3033–3051, Jul. 2006.
[20] F. Pérez-Cruz, M. R. Rodrigues, and S. Verdú, “MIMO Gaussian channels with arbitrary inputs: Optimal precoding and power
allocation,” IEEE Trans. Inf. Theory, vol. 56, no. 3, pp. 1070–1084, Mar. 2010.
[21] S. M. Moser, M. Mylonakis, L. Wang, and M. Wigger. (2017) Capacity results for MIMO
wireless optical communication. Submitted to IEEE Int. Symp. Inf. Theory (ISIT). [Online]. Available:
http://perso.telecom-paristech.fr/~Ewigger/mosermylonakiswangwigger_2017-1.pdf
