Iteratively Linearized Reweighted Alternating Direction Method
of Multipliers for a Class of Nonconvex Problems
Tao Sun? Hao Jiang† Lizhi Cheng‡
September 7, 2017
Abstract
In this paper, we consider solving a class of nonconvex and nonsmooth problems frequently ap-
pearing in signal processing and machine learning research. The traditional alternating direction
method of multipliers encounters troubles in both mathematics and computations in solving the non-
convex and nonsmooth subproblem. In view of this, we propose a reweighted alternating direction
method of multipliers. In this algorithm, all subproblems are convex and easy to calculate. We also
provide several guarantees for the convergence and prove that the algorithm globally converges to
a critical point of an auxiliary function with the help of the Kurdyka- Lojasiewicz property. Several
numerical results are presented to demonstrate the efficiency of the proposed algorithm.
Keywords: Alternating direction method of multipliers, iteratively reweighted algorithm,
nonconvex and nonsmooth minimization, Kurdyka- Lojasiewicz property
Mathematical Subject Classification 90C30, 90C26, 47N10
1 Introduction
Minimization of composite functions with linear constrains finds various applications in signal and
image processing, statics, machine learning, to name a few. Mathematically, such a problem can be
presented as
min
x,y
{f(x) + g(y) s.t. Ax+By = c}, (1.1)
where A ? Rr×M , B ? Rr×N , and g is usually the regularization function and f is usually the loss
function.
The well-known alternating direction method of multipliers (ADMM) method [12, 15] is a powerful tool
for the problem mentioned above. The ADMM actually aims to focusing on the augmented Lagrangian
problem of (1.1) which reads as
L??(x, y, p) := f(x) + g(y) + ?p,Ax+By ? c?+
?
2
?Ax+By ? c?22, (1.2)
where ? > 0 is a parameter. The ADMM minimizes only one variable and fixing others in each iteration;
the variable p is updated by a feedback strategy. Mathematically, the standard ADMM method can be
presented as ??? y
k+1 = arg miny L??(xk, y, pk)
xk+1 = arg minx L??(x, yk+1, pk)
pk+1 = pk + ?(Axk+1 +Byk+1 ? c)
(1.3)
?College of Science, National University of Defense Technology, Changsha, 410073, Hunan, China. Email:
nudtsuntao@163.com
†College of Computer, National University of Defense Technology, Changsha, 410073, Hunan, China. Email:
haojiang@nudt.edu.cn
‡College of Science & The State Key Laboratory for High Performance Computation, National University of Defense
Technology, Changsha, 410073, Hunan, China. Email: clzcheng@nudt.edu.cn
1
ar
X
iv
:1
70
9.
00
48
3v
2 
 [
cs
.N
A
] 
 6
 S
ep
 2
01
7
The ADMM algorithm attracts increasing attentions for its efficiency in dealing with sparse-related
problems [40, 37, 39, 26, 35]. Obviously, the ADMM has a self-explanatory assumption; all the subprob-
lems shall be solved efficiently. In fact, if the proximal maps of the f and g are easy to calculate, the
linearized ADMM [8] proposes the linearized technique to solve the subproblem efficiently; the subprob-
lems all need to compute once proximal map of f or g. The core part of the linearized ADMM lies on
linearizing the quadratical terms ?2 ?Ax + By
k ? c?22 and ?2 ?Ax
k+1 + By ? c?22 in each iteration. The
linearized ADMM is also called as preconditioned ADMM in [11]; in fact, it is also a special case when
? = 1 in Chambolle-Pock primal dual algorithm [5]. In the latter paper, the linearized ADMM is more
generalized as the Bregman ADMM [34].
The convergence of the ADMM in convex case is also well studied; numerous excellent works have
made contributions to this field [16, 17, 19, 10]. Recently, the ADMM algorithm is even developed for
the infeasible problems [24]. The earlier analyses focus on the convex case, i.e., both f and g are all
convex. But as the nonconvex penalty functions perform efficiently in applications, nonconvex ADMM
is developed and studied: in paper [6], Chartrand and Brendt directly used the ADMM to the group
sparse problems. They replace the nonconvex subproblems as a class of proximal maps. Latter Ames
and Hong consider applying ADMM for certain non-convex quadratic problems [1]. The convergence is
also presented. A class of nonconvex problems are solved by Hong et al by a provable ADMM [20]. They
also allow the subproblems to be solved inexactly by taking gradient steps which can be regarded as a
linearized way. Recently, with weaker assumptions, [36] present new analysis for nonconvex ADMM by
novel mathematical techniques. With the Kurdyka- Lojasiewicz property, [22, 23] consider the convergence
of the generated iterative points. [32] consider a structure constrained problem and proposed the ADMM-
DC algorithm. In nonconvex ADMM literature, either the proximal maps of f and g or the subproblems
are assumed to be easily calculated.
1.1 Motivated example and problem formulation
This subsection contains two parts: the first one presents an example and discusses the problems in
directly using the ADMM; the second one describes the problem considered in this paper.
1.1.1 A motivated example: the problems in directly using ADMM
The methods mentioned above are feasibly applicable provided the subproblems are relatively solvable,
i.e., either the proximal maps of f and g or the subproblems are assumed to be easily calculated. However,
the nonconvex cases may not always promise such a convention. We recall the TVq? problem [18] which
arises in imaging science
min
u
{1
2
?f ??u?22 + ??Tu?qq,?}, (1.4)
where T is the total variation operator and ?y?qq,? :=
?
i(|yi| + ?)q. By denoting v = Tu, the problem
then turns to being
min
u,v
{1
2
?f ??u?22 + ??v?qq,?, s.t. Tu? v = 0}. (1.5)
The direct ADMM for this problem can be presented as???
vk+1 = arg minv{??v?qq,? + ?pk, v ? Tuk?+ ?2 ?v ? Tu
k?22},
xk+1 = arg minx{ 12?f ??u?
2
2 + ?pk, vk+1 ? Tu?+ ?2 ?v
k+1 ? Tu?22},
pk+1 = pk + ?(vk+1 ? Tuk+1).
(1.6)
The first subproblem in the algorithm needs to minimize a nonconvex and nonsmooth problem. If
q = 12 ,
2
3 , the point v
k can be explicitly calculated. This is because the proximal map of ? · ?qq,? can be
easily obtained. But for other q, the proximal map cannot be easily derived. Thus, we may must employ
iterative algorithms to compute vk+1. That indicates three drawbacks which cannot be ignored:
2
1. The stopping criterion is hard to set for the nonconvexity1.
2. The error may be accumulating in the iterations due to inexact solution of the subproblem.
3. Even the subproblem can be solved without any error, the nonconvexity always promises a critical
point for the subproblem, which is not “really” argmin.
In fact, the other penalty functions like Logistic function [38], Exponential-Type Penalty (ETP) [13],
Geman [14], Laplace [33] also encounter such a problem.
1.1.2 Optimization problem and basic assumptions
In this paper, we consider the following problem
min
x,y
f(x) +
N?
i=1
g[h(yi)] s.t. Ax+By = c, (1.7)
where A ? Rr×M , B ? Rr×N , and f , g and h satisfy the following assumptions:
A.1 f : RN ? R is a differentiable convex function with a Lipschitz continuous gradient, i.e.,
??f(x)??f(y)?2 ? Lf?x? y?2. (1.8)
A.2 h : R? R is convex and proximable.
A.3 g : Im(h) ? R is a differentiable concave function with a Lipschitz continuous gradient whose
Lipschitz continuity modulus is bounded by Lg > 0; that is
| g?(s)? g?(t) |? Lg | s? t |, (1.9)
and g?(t) > 0 when t ? Im(h).
It is easy to see that the TVq problem can be regarded as a special one of (1.7) if we set g(s) = (s+?)q
and h(t) = |t|. The augmented lagrange dual function of model (1.7) is
L?(x, y, p) = f(x) +
N?
i=1
g[h(yi)] + ?p,Ax+By ? c?+
?
2
?Ax+By ? c?22, (1.10)
where ? > 0 is a parameter.
1.2 Linearized ADMM meets the iteratively reweighted strategy: convexify-
ing the subproblems
In this part, we present the algorithm for solving problem (1.7). The term
?N
i=1 g[h(yi)] has a deep
relationship with several iteratively reweighted style algorithms [7, 9, 31, 41, 30]. Although the function?N
i=1 g[h(yi)] may be nondifferentiable itself, the reweighted style methods still propose an elegant way:
linearization of outside function g. Precisely, in (k + 1)-th iteration of the iteratively reweighted style
algorithms, the term
?N
i=1 g[h(yi)] is usually replaced by
?N
i=1 g
?[h(yki )] · [h(yi)?h(yki )] +
?N
i=1 g[h(y
k
i )],
where yk is obtained in the k-th iteration. Motivated by the iteratively reweighted strategy, we propose
the following scheme for solving (1.7) which reads as?????
yk+1 = arg miny{
?N
i=1 g
?[h(yki )]h(yi) + ?By, ?(Axk +Byk ? c) + pk?+ r12 ?y ? y
k?22},
xk+1 = arg minx{f(x) + ?Ax, ?(Axk +Byk+1 ? c) + pk?+ (x?x
k)>[D(~r2)??A>A](x?xk)
2 },
pk+1 = pk + ?(Axk+1 +Byk+1 ? c),
(1.11)
1The convex methods usually enjoy a convergence rate.
3
where D(~r2) = U
>diag(r2,1, r2,2, . . . , r2,N )U and r2,i > 0 for i ? [1, 2, . . . , N ], and U is the SVD matrix
of A>A. We combined both linearized ADMM and reweighted algorithm in the new scheme: for the
nonconvex part
?N
i=1 g[h(yi)], we linearize the outside function g and keep h, which aims to derive the
convexity of the subproblem; for the quadratical parts ?2 ?Ax + By
k ? c?22 and ?2 ?Ax
k+1 + By ? c?22,
linearizations are for the use of the proximal map of f and h. We call this new algorithm as Iteratively
Linearized Reweighted Alternating Direction Method of Multipliers (ILR-ADMM). It is easy to see that
each subproblem just needs to solve a convex problem in this scheme. With the expression of proximal
maps, scheme (1.11) can be equivalently presented as the following forms???????
yk+1i = prox g?[h(yki )]
r1
h
(yki ?
B>i (?(Ax
k+Byk?c)+pk)
r1
), i ? [1, 2, . . . , N ]
xk+1 = arg minx{f(x) + ?Ax, ?(Axk +Byk+1 ? c) + pk?+ (x?x
k)>[D(~r2)??A>A](x?xk)
2 },
pk+1 = pk + ?(Axk+1 +Byk+1 ? c),
(1.12)
where Bi denotes the i-th column of the matrix B. In many applications, f is the quadratical function,
and then solving xk+1 is also very easy. With this form, the algorithm can be programmed with the
absence of inner loops. In fact, if A, B and c all vanish, ILR-ADMM immediately reduces to the
proximal reweighted algorithm [?].
Algorithm 1 Iteratively Linearized Reweighted Alternating Direction Method of Multipliers (ILR-
ADMM)
Require: parameters ? > 0, r1, r2,1, r2,1, . . . , r2,N
Initialization: x0, y0, p0
for k = 0, 1, 2, . . .
yk+1i = prox g?[h(yki )]
r1
h
(yki ?
B>i (?(Ax
k+Byk?c)+pk)
r1
), i ? [1, 2, . . . , N ]
xk+1 = arg minx{f(x) + ?Ax, ?(Axk +Byk+1 ? c) + pk?+ (x?x
k)>[D(~r2)??A>A](x?xk)
2 }
pk+1 = pk + ?(Axk+1 +Byk+1 ? c)
end for
1.3 Contribution and Organization
In this paper, we consider a class of nonconvex and nonsmooth problems which are ubiquitous in
applications. Direct use of ADMM algorithms will lead troubles in both computations and mathematics
for the nonconvexity of the subproblem. In view of this, we propose the iteratively linearized reweighted
alternating direction method of multipliers for these problems. The new algorithm is an organic combi-
nation of iteratively reweighted strategy and the linearized ADMM. All the subproblems in the proposed
algorithm are convex and easy to be solved if the proximal maps of h is easy to calculate and f is quadrat-
ical. Compared with the direct application of ADMM to problem (1.7), we now list the advantages of
the new algorithm:
1. Computational perspective: each subproblem just needs to compute once proximal map of g and
minimize a quadratical problem, the computational cost is low in each iteration.
2. Practical perspective: without any inner loop, the programming is very easy.
3. Mathematical perspective: all the subproblems is convex and exactly solved. Thus, we get “really”
argmin everywhere, which makes the mathematical convergence analysis solid and meaningful.
With the help of the Kurdyka- Lojasiewicz property, we provide the convergence results of the algorithm
with proper selections of the parameters. The applications of the new algorithm to the signal and image
processing are presented. The numerical results demonstrate the efficiency of the proposed algorithm.
4
The rest of this paper is organized as follows. Section 2 introduces the preliminaries including the
definitions of subdifferential and the Kurdyka- Lojasiewicz property. Section 3 provides the convergence
analysis. The core part is using an auxiliary Lyapunov function and bounding the generated sequence.
Section 4 applies the proposed algorithm to signal and image processing. And several comparisons are
reported. Finally, section 5 concludes the paper.
2 Preliminaries
We introduce the basic tools in the analysis: the subdifferential and Kurdyka- Lojasiewicz property.
These two definitions play important roles in the variational analysis.
2.1 Subdifferential
Given a lower semicontinuous function J : RN ? (??,+?], its domain is defined by
dom(J) := {x ? RN : J(x) < +?}.
The graph of a real extended valued function J : RN ? (??,+?] is defined by
graph(J) := {(x, v) ? RN × R : v = J(x)}.
Now, we are prepared to present the definition of subdifferential. More details can be found in [27].
Definition 1. Let J : RN ? (??,+?] be a proper and lower semicontinuous function.
1. For a given x ? dom(J), the Fre?chet subdifferential of J at x, written as ??J(x), is the set of all
vectors u ? RN satisfying
lim
y 6=x
inf
y?x
J(y)? J(x)? ?u, y ? x?
?y ? x?2
? 0.
When x /? dom(J), we set ??J(x) = ?.
2. The (limiting) subdifferential, or simply the subdifferential, of J at x ? RN , written as ?J(x), is
defined through the following closure process
?J(x) := {u ? RN : ?xk ? x, J(xk)? J(x) and uk ? ??J(xk)? u as k ??}.
When J is convex, the definition agrees with the subgradient in convex analysis [28] which is defined
as
?J(x) := {v ? RN : J(y) ? J(x) + ?v, y ? x? for any y ? RN}.
It is easy to verify that the Fre?chet subdifferential is convex and closed while the subdifferential is closed.
Denote that
graph(?J) := {(x, v) ? RN × RN : v ? ?J(x)},
thus, graph(?J) is a closed set. Let {(xk, vk)}k?N be a sequence in RN×R such that (xk, vk) ? graph (?J).
If (xk, vk) converges to (x, v) as k ? +? and J(xk) converges to v as k ? +?, then (x, v) ? graph (?J).
This indicates the following simple proposition.
Proposition 1. If vk ? ?J(xk), limk vk = v and limk xk = x. Then, we have that
v ? ?J(x). (2.1)
A necessary condition for x ? RN to be a minimizer of J(x) is
0 ? ?J(x). (2.2)
When J is convex, (2.2) is also sufficient.
5
Definition 2. A point that satisfies (2.2) is called (limiting) critical point. The set of critical points of
J(x) is denoted by crit(J).
Proposition 2. If (x?, y?, p?) is a critical point of L?(x, y, p) with any ? > 0, it must hold that
?B>p? ? W ??h(y?),
?A>p? = ?f(x?),
Ax? +By? ? c = 0,
where L?(x, y, p) is defined in (1.10) and W ? = Diag{g?[h(y?i )]}1?i?N .
2.2 Kurdyka- Lojasiewicz function
The domain of a subdifferential is given as
dom(?J) := {x ? RN : ?J(x) 6= ?}.
Definition 3. (a) The function J : RN ? (??,+?] is said to have the Kurdyka- Lojasiewicz property at
x ? dom(?J) if there exist ? ? (0,+?), a neighborhood U of x and a continuous function ? : [0, ?)? R+
such that
1. ?(0) = 0.
2. ? is C1 on (0, ?).
3. for all s ? (0, ?), ??(s) > 0.
4. for all x in U
?
{x|J(x) < J(x) < J(x) + ?}, it holds
?
?
(J(x)? J(x)) · dist(0, ?J(x)) ? 1. (2.3)
(b) Proper lower semicontinuous functions which satisfy the Kurdyka- Lojasiewicz property at each
point of dom(?J) are called KL functions.
The readers can find [25, 21, 3] for more details. In the following part of the paper, we use KL
for Kurdyka- Lojasiewicz for short. Direct checking whether a function is KL or not is hard, but the
semi-algebraic functions [3] do much help.
Definition 4. (a) A subset S of RN is a real semi-algebraic set if there exists a finite number of real
polynomial functions gij , hij : RN ? R such that
S =
p?
j=1
q?
i=1
{u ? RN : gij(u) = 0 and hij(u) < 0}.
(b) A function h : RN ? (??,+?] is called semi-algebraic if its graph
{(u, t) ? RN+1 : h(u) = t}
is a semi-algebraic subset of RN+1.
Better yet, the semi-algebraicity enjoys many quite nice properties and various kinds of functions are
KL [2]. We just put a few of them here:
• Real polynomial functions.
• Indicator functions of semi-algebraic sets.
6
• Finite sums and product of semi-algebraic functions.
• Composition of semi-algebraic functions.
• Sup/Inf type function, e.g., sup{g(u, v) : v ? C} is semi-algebraic when g is a semi-algebraic
function and C a semi-algebraic set.
• Cone of PSD matrices, Stiefel manifolds and constant rank matrices.
Lemma 1 ([2]). Let J : RN ? R be a proper and lower semicontinuous function. If J is semi-algebraic
then it satisfies the KL property at any point of dom(J). In particular, if J is semi-algebraic and dom(J) =
dom(?J), then it is a KL function.
The previous definition and property of KL is about a certain point in dom(J). In fact, the property
has been extended to a certain closed set [4]. And this property makes previous convergence proofs
related to KL property much easier.
Lemma 2. Let J : RN ? R be a proper lower semi-continuous function and ? be a compact set. If J
is a constant on ? and J satisfies the KL property at each point on ?, then there exists function ? and
?, ? > 0 such that for any x ? ? and any x satisfying that dist(x,?) < ? and f(x) < f(x) < f(x) + ?, it
holds that
?
?
(J(x)? J(x)) · dist(0, ?J(x)) ? 1. (2.4)
3 Convergence analysis
In this part, the function L?(x, y, p) is defined in (1.10). We provide the convergence guarantee and
the convergence analysis of ILR-ADMM (Algorithm 1). We first present a sketch of the proofs, which is
also a big picture for the purpose of each lemma and theorem, :
• In the first step, we bound the dual variables by the primal points (Lemma 3).
• In the second step, the sufficient descent condition is derived for a new Lyapunov function (Lemma
4).
• In the third step, we provide several conditions to bound the points (Lemma 5).
• In the fourth step, the relative error condition is proved (Lemma 6).
• In the last step, we prove the convergence under semi-algebraic assumption (Thoerem 1).
Lemma 3. If
Im(B)
?
{c} ? Im(A) (3.1)
and
r2,i = ??
2
i (A) + r2, (3.2)
where r2 > 0. Then, we have
?pk ? pk+1?22 ? ?1?xk+1 ? xk?22 + ?2?xk ? xk?1?22, (3.3)
where ?1 = 2(
r2+Lf
? )
2, ?2 = 2(
r2
? )
2, and ? is the smallest strictly-positive eigenvalue of (A>A)1/2.
Proof. The second step in each iteration actually gives
D(~r2)(x
k+1 ? xk) +?f(xk+1) = ?A>(?(Axk +Byk+1 ? c) + pk). (3.4)
With the expression of pk+1,
D(~r2)(x
k+1 ? xk) +?f(xk+1) = ?A>pk+1 + ?A>Axk+1 ? ?A>Axk. (3.5)
7
Replacing k + 1 with k, we can obtain
D(~r2)(x
k ? xk?1) +?f(xk) = ?A>pk + ?A>Axk ? ?A>Axk?1. (3.6)
Under condition (3.1), pk+1 ? pk ? Im(A); and substraction of the two equations above gives
?pk ? pk+1?2 ?
1
?
?A>(pk ? pk+1)?2
? ?(D(~r2)? ?A
>A)(xk+1 ? xk)?2
?
+
?(D(~r2)? ?A>A)(xk ? xk?1)?2
?
+
??f(xk+1)??f(xk)?2
?
? r2 + Lf
?
?xk+1 ? xk?2 +
r2
?
?xk ? xk?1?2. (3.7)
In the third inequality, we used the fact
D(~r2)? ?A>A = r2I. (3.8)
Remark 1. If p0 ? Im(A), we have pk ? Im(A). Then, from (3.6), we have that
?pk?22 ?
2
?2
??f(xk)?22 + ?2?xk ? xk?1?22. (3.9)
We will use this inequality in bounding the sequence.
Remark 2. In many applications, the matrix A is usually full row-rank.
The condition (3.13) is satisfied if A is full row-rank. We return the example presented in Section 1.1,
and we can see that T is full row-rank. In applications, the condition (3.13) can always hold. Now, we
introduce several notations to present the following lemma. Denote the variable d and the sequence dk
as
d := (x, y, p, x?), dk := (xk, yk, pk, xk?1), zk := (xk, yk). (3.10)
The Lyapunov function is given as
?(d) := L?(x, y, p) +
?2
?
?x? x??22. (3.11)
An auxiliary function is always used in the proof
Lk?(x, y, p) := f(x) +
N?
i=1
g?[h(yki )]h(yi) + ?p,Ax+By ? c?+
?
2
?Ax+By ? c?22. (3.12)
Lemma 4 (Descent). Let the sequence {(xk, yk, pk)}k=0,1,2,... be generated by ILR-ADMM. If condition
(3.1) and the following condition
min{r2 ? 4
(r2 + Lf )
2
??2
? 4r
2
2
??2
, r1 ? ??B?22} > 0 (3.13)
hold, then there exists ? > 0 such that
?(dk)? ?(dk+1) ? ??zk+1 ? zk?22. (3.14)
Proof. Direct calculation shows that the first step is actually minimizing the function Lk?(xk, y, pk) +
(y?yk)(r1I??B>B)(y?yk)
2 with respect to y. Thus, we have
Lk?(xk, yk+1, pk) +
r1 ? ??B?22
2
?yk+1 ? yk?22
? Lk?(xk, yk+1, pk) +
(y ? yk)(r1I? ?B>B)(yk+1 ? yk)
2
? Lk?(xk, yk, pk).
8
Similarly,
Lk?(xk+1, yk+1, pk) +
r2
2
?xk+1 ? xk?22 ? Lk?(xk, yk+1, pk). (3.15)
Direct calculation yields
Lk?(xk+1, yk+1, pk+1) = Lk?(xk+1, yk+1, pk) + ?pk+1 ? pk, Axk+1 +Byk+1 ? c?
= Lk?(xk+1, yk+1, pk) +
1
?
?pk+1 ? pk?22. (3.16)
Combining the equations above, we can have
Lk?(xk, yk, pk) ? Lk?(xk+1, yk+1, pk+1) +
r2
2
?xk+1 ? xk?22
+
r1 ? ??B?22
2
?yk+1 ? yk?22 ?
1
?
?pk+1 ? pk?22. (3.17)
Noting g is concave, we have
N?
i=1
g[h(yki )]?
N?
i=1
g[h(yk+1i )] =
N?
i=1
{g[h(yki )]? g[h(yk+1i )]}
?
N?
i=1
g?[h(yki )][h(y
k
i )? h(yk+1i )] (3.18)
=
N?
i=1
g?[h(yki )]h(y
k
i )?
N?
i=1
g?[h(yki )]h(y
k+1
i ).
Then, we can derive
L?(xk, yk, pk)? L?(xk+1, yk+1, pk+1)
=
N?
i=1
g[h(yki )]?
N?
i=1
g[h(yk+1i )] + f(x
k) + ?pk, Axk +Byk ? c?
+
?
2
?Axk +Byk ? c?22 ? {f(xk+1) + ?pk+1, Axk+1 +Byk+1 ? c?
+
?
2
?Axk+1 +Byk+1 ? c?22}
?
N?
i=1
g?[h(yki )]h(y
k
i )?
N?
i=1
g?[h(yki )]h(y
k+1
i ) + f(x
k) + ?pk, Axk +Byk ? c?
+
?
2
?Axk +Byk ? c?22 ? {f(xk+1) + ?pk+1, Axk+1 +Byk+1 ? c?
+
?
2
?Axk+1 +Byk+1 ? c?22}
= Lk?(xk, yk, pk)? Lk?(xk+1, yk+1, pk+1)
? r2
2
?xk+1 ? xk?22 +
r1 ? ??B?22
2
?yk+1 ? yk?22 ?
1
?
?pk+1 ? pk?22.
With Lemma 3, we then have
L?(xk, yk, pk) ? L?(xk+1, yk+1, pk+1) ?
r2
2
?xk+1 ? xk?22 +
r1 ? ??B?22
2
?yk+1 ? yk?22
? ?1
?
?xk+1 ? xk?22 ?
?2
?
?xk ? xk?1?22.
9
Then, we can further obtain
L?(xk, yk, pk) +
?2
?
?xk ? xk?1?22
? (L?(xk+1, yk+1, pk+1) +
?2
?
?xk+1 ? xk?22)
? (r2
2
? ?1 + ?2
?
)?xk+1 ? xk?22 +
r1 ? ??B?22
2
?yk+1 ? yk?22.
Letting ? := min{ r22 ?
?1+?2
? ,
r1???B?22
2 }, we then prove the result.
In fact, condition (3.13) can be always satisfied in applications because the parameters r1, r2 and ?
are all selected by the user. For example, we can set the parameters as
r1 = ??B?22 + 1, r2,i = 1 + ??2i (A), i ? [1, 2, . . . , N ], ? >
4L2f + 8Lf + 8
?2
. (3.19)
Different with the ADMMs in convex setting, the parameter ? is nonarbitrary. After fixing r1 and r2,
the ? should be sufficiently large.
Lemma 5 (Boundedness). If p0 ? Im(A) and conditions (3.1) and (3.13) hold, and there exists ?0 > 0
such that
inf{f(x)? ?0??f(x)?22} > ??, (3.20)
and
0 < ? ? ?0,
1
??2
? ? ? 2
??2
. (3.21)
The sequence {dk}k=0,1,2,... is bounded, if one of the following conditions hold:
B1. g(y) is coercive, and f(x)? ?0??f(x)?22 is coercive.
B2. g(y) is coercive, and A is invertible.
B3. inf{g(y)} > ??, f(x)? ?0??f(x)?22 is coercive, and A is invertible.
Proof. We have
L?(dk) = f(xk) + g(yk) + ?pk, Axk +Byk ? c?+
?
2
?Axk +Byk ? c?22 +
?2
?
?xk ? xk?1?22
= f(xk) + g(yk)? ?p
k?22
2?
+
?
2
?Axk +Byk ? c+ p
k
?
?22 +
?2
?
?xk ? xk?1?22
= f(xk) + g(yk)? ??
2
2
?pk?22 + (
??2
2
? 1
2?
)?pk?22
+
?
2
?Axk +Byk ? c+ p
k
?
?22 +
?2
?
?xk ? xk?1?22
(3.9) ? f(xk) + g(yk)? ?0??f(xk)?22 + ?2(
1
?
? ??
2
2
)?xk ? xk?1?22 + (?0 ? ?)??f(xk)?22
+ (
??2
2
? 1
2?
)?pk?22 +
?
2
?Axk +Byk ? c+ p
k
?
?22. (3.22)
We then can see {g(yk)}k=0,1,2,..., {pk}k=0,1,2,..., {Axk + Byk ? c + p
k
? }k=0,1,2,... are all bounded. It is
easy to see that one of the three conditions holds, {dk}k=0,1,2,... will be bounded.
Remark 3. The condition (3.20) holds for many quadratical functions [22, 29]. This condition also
implies the function f is similar to quadratical function and its property is “good”.
10
Remark 4. Both conditions B2 and B3 actually overlap condition (3.9). If using B2 and B3, condition
(3.9) could be absent.
Remark 5. The intersection between conditions (3.21) and (3.13) can be always nonempty. This is
because we can always choose small ?. For example, we still use the setting (3.19). In this case, we just
set that
? ? min{ 1
2L2f + 4Lf + 4
, ?0}. (3.23)
Lemma 6 (Relative error). If conditions (3.1) and (3.13) hold, for any k ? Z+, there exists ? > 0 such
that
dist(0, ??(dk+1)) ? ?(?zk ? zk+1?2 + ?zk+1 ? zk+2?2). (3.24)
Proof. Due to that h is convex, h is Lipschitz continuous with some constant if being restricted to some
bounded set. Thus, there exists Lh > 0 such that
|h(yk+1i )? h(y
k
i )| ? Lh|yk+1i ? y
k
i |.
Updating yk+1 in each iteration certainly yields
r1(y
k ? yk+1)?B>(?(Axk +Byk ? c) + pk) ?W k?h(yk+1), (3.25)
where h(y) :=
?N
i=1 h(yi) and W
k := Diag{g?[h(yk1 )], g?[h(yk2 )], . . . , g?[h(ykN )]}. Noting the boundedness
of the sequence and h(yki ), the continuity of g
? indicates there exist ?1, ?2 > 0 such that
?1 ? g?[h(yki )] ? ?2, i ? [1, 2, . . . , N ], k ? Z+. (3.26)
Easy computation gives
W k+1(W k)?1[r1(y
k ? yk+1) ? ?B>(Axk +Byk ? c)?B>pk]
+ B>pk+1 + ?B>(Axk+1 +Byk+1 ? c) ? ?y?(dk+1). (3.27)
The left side of (3.27) can be rewritten as
(W k+1 ?W k)(W k)?1[r1(yk ? yk+1)? ?B>(Axk +Byk ? c)?B>pk]
+ r1(y
k ? yk+1) +B>(pk+1 ? pk) + ?B>[A(xk+1 ? xk) +B(yk+1 ? yk)] ? ?y?(dk+1). (3.28)
With the boundedness of the generated points, there exist R1 > 0 such that
?r1(yk ? yk+1)? ?B>(Axk +Byk ? c)?B>pk?2 ? R1. (3.29)
Thus, we have
dist(0, ?y?(d
k+1)) ? R1
?1
?W k+1 ?W k?2 + ?B>pk+1 ?B>pk?2
+ ??B>A(xk+1 ? xk)?2 + ??B>B(yk+1 ? yk)?2 + r1?yk+1 ? yk?2
? R1
?1
?W k+1 ?W k?2 + ?B?2 · ?pk+1 ? pk?2
+ ??B?2 · ?A?2 · ?xk+1 ? xk?2 + (??B?22 + r1) · ?yk+1 ? yk?2. (3.30)
Obviously, it holds that
?W k+1 ?W k?2 ? max
i
|g?[h(yk+1i )]? g
?[h(yki )]|
? Lg max
i
|h(yk+1i )? h(y
k
i )|
? LgLh?yk+1 ? yk?? ? LgLh?yk+1 ? yk?2.
11
Thus, with Lemma 3, we derive that
dist(0, ?y?(d
k+1)) ? ?y(?zk+1 ? zk?2 + ?zk ? zk?1?2), (3.31)
for ?y = max{LgLhR1?1 + ??B?
2
2 + r1, ??B?2?A?2 + ?B?2
?
?1, ?B?2
?
?2}.
From the second step in each iteration,
?f(xk+1) = r2(xk ? xk+1)?A>pk ? ?A>(Axk +Byk+1 ? c). (3.32)
Direct calculation gives
?f(xk+1) +A>pk+1 + ?A>(Axk+1 +Byk+1 ? c) + 2?2
?
(xk+1 ? xk) ? ?x?(dk+1). (3.33)
With Lemma 3, we have
dist(0, ?x?(d
k+1)) ? ??A>A(xk+1 ? xk)?2 + ?A>pk+1 ?A>pk?2 +
2?2
?
?xk+1 ? xk+2?2
? ?A?2 · ?pk+1 ? pk?2 + (
2?2
?
+ ??A?22 + r2)?xk+1 ? xk?2
? ?x(?zk+1 ? zk?2 + ?zk ? zk?1?2),
where ?x = max{ 2?2? + ??A?
2
2 + r2 + ?A?
?
?1, ?A?
?
?2}.
It is easy to see that
Axk+1 +Byk+1 ? c ? ?p?(dk+1). (3.34)
And we have
dist(0, ?p?(d
k+1)) ? ?p(?zk+1 ? zk?2 + ?zk ? zk?1?2), (3.35)
for ?p = max{
?
?1
? ,
?
?2
? }.
Noting that
2?2
?
(xk?1 ? xk) ? ?x??(dk+1), (3.36)
we then have that
dist(0, ?x??(d
k+1)) ? 2?2
?
(?zk+1 ? zk?2 + ?zk ? zk?1?2). (3.37)
With the deductions above,
dist(0, ??(dk+1)) ? (?x + ?y + ?p +
2?2
?
)(?zk+1 ? zk?2 + ?zk ? zk?1?2). (3.38)
Denote that ? = ?x + ?y + ?p +
2?2
? , we then finish the proof.
Lemma 7. If the sequence {(xk, yk, pk)}k=0,1,2,... is bounded and conditions (3.1) and (3.13) hold, then
we have
lim
k
?zk+1 ? zk?2 = 0. (3.39)
For any cluster point (x?, y?, p?), it is also a critical point of L?(x, y, p).
Proof. We can easily see that {dk}k=0,1,2,... is also bounded. The continuity of ? indicates that {?(dk)}k=0,1,2,...
is bounded. From Lemma 4, ?(dk) is decreasing. Thus, the sequence {?(dk)}k=0,1,2,... is convergent, i.e.,
limk[?(d
k)? ?(dk+1)] = 0. With Lemma 4, we have
lim
k
?zk+1 ? zk?2 ? lim
k
?
?(dk)? ?(dk+1)
?
= 0. (3.40)
12
From the scheme of the ILR-ADMM, we also have
lim
k
?pk+1 ? pk?2 = 0. (3.41)
For any cluster point (x?, y?, p?), there exists {kj}j=0,1,2,... such that limj(xkj , ykj , pkj ) = (x?, y?, z?).
Then, we further have limj z
kj+1 = (x?, y?). From Lemma 3, we also have limj p
kj+1 = p?. That also
means
lim
j
W kj = W ?. (3.42)
From the scheme, we have the following conditions
(W kj )?1[r1(y
kj ? ykj+1) ? B>(?(Axkj +Bykj ? c) + pkj )] ? ?h(ykj+1),
r2(x
kj ? xkj+1)?A>pkj ? ?A>(Axkj +Bykj+1 ? c) = ?f(xkj+1),
pkj+1 = pkj + ?(Axkj+1 +Bykj+1 ? c).
Letting j ? +?, with Proposition 1, we have
(W ?)?1[?B>p?] ? ?h(y?),
?A>p? = ?f(x?),
Ax? +By? ? c = 0.
The first relation above is actually ?B>p? ? W ??h(y?). From Proposition 2, z? is a critical point of
L.
In the following, to prove the convergence result, we first establish some results about the limit points
of the sequence generated by ILR-ADMM. There results are presented for the use of Lemma 2. We recall
a definition about the limit point which is introduced in [4].
Definition 5. Define that
M(d0) := {u ? RN : ? an increasing sequence of integers {kj}j?N such that dkj ? u as j ??},
where d0 ? Rn is an arbitrary starting point.
Lemma 8. Suppose that {dk}k=0,1,2,... is generated by scheme (1.11). Then, we have the following
results.
(1) M(d0) is nonempty and M(d0) ? cri(?).
(2) limk dist(d
k,M(d0)) = 0.
(3) The objective function is finite and constant on M(d0).
Proof. (1) Due to that {dk}k=0,1,2,... is bounded, M(d0) is nonempty. Assume that d? ? M(d0), from
the definition, there exists a subsequence dki ? d?. From Lemma 4, we have dki?1 ? d?. From Lemma
6, we have ?ki ? ??(dki) and ?ki ? 0. Proposition 1 indicates that 0 ? ??(d?), i.e. d? ? cri(?).
(2) This item follows as a consequence of the definition of the limit point.
(3) The continuity of ? directly yields this result.
Theorem 1 (Convergence result). Suppose that f and g are all semi-algebraic functions and dom(f) =
dom(?f), and dom(g) = dom(?g). Assume that conditions (3.1), (3.2), (3.13), (3.20), (3.21) and one of
B1, B2, B3 hold. Let the sequence {(xk, yk, pk)}k=1,2,3,... generated by ILR-ADMM be bounded. Then,
the sequence {zk = (xk, yk)}k=1,2,3,... has finite length, i.e.
+??
k=0
?zk+1 ? zk?2 < +?. (3.43)
And {(xk, yk)}k=1,2,3,... converges to (x?, y?), where (x?, y?, p?) is a critical point of L?(x, y, p).
13
Proof. Obviously, ? is also semi-algebraic. And with Lemma 1, ? is KL. From Lemma 8, ? is constant on
M(d0). Let d? be a stationary point of {dk}k=0,1,2,.... Also from Lemma 8, we have dist(dk,M(d0)) < ?
and ?(dk) < ?(d?) + ? if any k > K for some K. Hence, from Lemma 2, we have
dist(0, ??(dk)) · ??(?(dk)? ?(d?)) ? 1, (3.44)
which together with Lemma 6 gives
1
??(?(dk)? ?(d?))
? dist(0, ??(dk)) ? ?(?zk ? zk?1?2 + ?zk+1 ? zk?2). (3.45)
Then, the concavity of ? yields
?(dk)? ?(dk+1) = ?(dk)? ?(d?)? [?(dk+1)? ?(d?)]
? ?[?(d
k)? ?(d?)]? ?[?(dk+1)? ?(d?)]
??[?(dk)? ?(d?)]
? {?[?(dk)? ?(d?)]? ?[?(dk+1)? ?(d?)]} × ?(?zk ? zk?1?2 + ?zk+1 ? zk?2).
With Lemma 4, we have
??zk+1 ? zk?22 ? {?[?(dk)? ?(d?)]? ?[?(dk+1)? ?(d?)]} × ?(?zk ? zk?1?2 + ?zk+1 ? zk?2),
which is equivalent to
3
?
?
?zk+1 ? zk?2 ? 2×
3
2
?
?[?(dk)? ?(d?)]? ?[?(dk+1)? ?(d?)]
×
?
?
?
?
?zk ? zk?1?2 + ?zk+1 ? zk?2. (3.46)
Using the Schwartz’s inequality, we then derive that
3
?
?
?zk+1 ? zk?2 ?
9
4
{?[?(dk)? ?(d?)]? ?[?(dk+1)? ?(d?)]}
+
?
?
(?zk ? zk?1?2 + ?zk+1 ? zk?2). (3.47)
Summing (3.47) from K to K + j yields that
?
?
K+j?1?
k=K
?zk+1 ? zk?2 +
2?
?
?zK+j+1 ? zK+j?2
? 9
4
?[?(dK)? ?(d?)]? 9
4
?[?(dK+j+1)? ?(d?)]. (3.48)
Letting j ? +?, with Lemma 6, we have
?
?
+??
k=K
?zk+1 ? zk?2 ?
9
4
?[?(dK)? ?(d?)] < +?. (3.49)
From Lemma 8, there exists a critical point (x?, y?, p?) of L?(x, y, p). Then, {zk}k=0,1,2,... is convergent
and (x?, y?) is a stationary point of {zk}k=0,1,2,.... That is to say {zk}k=0,1,2,... converges to (x?, y?).
4 Applications and numerical results
In this part, we consider using (1.4) for signal and image denoising. Considering the proximal map of
? · ?qq,? is easy to derive if q = 12 ,
2
3 , and hard when q 6=
1
2 ,
2
3 . The numerical results in this section consist
of two parts: the first one is about the case q = 12 ; and the second one is about q ? (0,
1
2 )
?
( 12 , 1).
14
0 50 100 150 200 250 300
0
0.5
1
1.5
2
2.5
(a) Original signal
0 50 100 150 200 250 300
-0.5
0
0.5
1
1.5
2
2.5
3
(b) Noised signal
0 50 100 150 200 250 300
-0.5
0
0.5
1
1.5
2
2.5
(c) Recovery by ILR-ADMM,
SNR=76.3dB
0 50 100 150 200 250 300
-0.5
0
0.5
1
1.5
2
2.5
(d) Recovery by L ? 1/2 ADMM,
SNR=74.6dB
Figure 1: Recovered signal and original signal
4.1 The parameter q = 1
2
In fact, the proximal map of ?·?qq,? also exists when q = 23 . But it is a little harder than the case q =
1
2 .
This part just wants to demonstrate the performance of the proposed algorithm when the direct use of
the ADMM without any computational hindrance for solving the subproblems. Thus, we just present the
case q = 12 . Two problems are considered in this part. In the first one, we consider the case ? = I. In the
iteration, the parameters are chose as q = 1/2, ? = 0.001. The observed vector is b = x?+e. We compare
Algorithm 2 with the direct use of ADMM for the TV- 12 denoising. We can directly use the algorithm
because the proximal map of ? · ?
1
2
1
2 ,?
is easy to obtain. We choose the parameter ? = 0.1. The noise
e ? R256 is generated by a random variable. Figure 1 presents the numerical results of a 1-dimensional
signal denoising. From the results, we can see that ILR-ADMM performs better.
In the second test, we consider the deblurring problem. The blurring operator ? is generated by the
Matlab order fspecial(’gaussian’,.,.). The parameter is set as ? = 10?4, e is generated by the
Gaussian noise N (0, 0.01) and b = ?x? + e. Figure 2 presents the numerical results of a 2-dimensional
signal deblurring and denoising. And we can see that the ILR-ADMM outperforms the direct nonconvex
ADMM.
4.2 The parameter q 6= 1
2
, 2
3
This subsection contains three parts. In the first one, we present the performance of the algorithm
for different q; the second one is the comparison with the nonconvex ADMM. Because the proximal map
of ? · ?qq,? does not enjoy explicit format, an inner loop is used for the subproblem. Precisely, the inner
loop is constructed by the proximal reweighted algorithm.
In the first test, the blurring operator ? is generated by the Matlab order fspecial(’motion’,.,.).
The parameter is set as ? = 10?4, e is generated by the Gaussian noise N (0, 0.01) and b = ?x? + e. For
q = 0.2, 0.4, 0.6, 0.8, Figure 3 presents the deblurred images.
15
(a) Original signal (b) Blurred and noised signal
(c) Recovery by ILR-ADMM,
SNR=13.09dB
(d) Recovery by 1
2
-ADMM,
SNR=11.55dB
Figure 2: Recovered signal and original signal
16
(a) Original signal (b) Blurred and noised signal
(c) Recovery by ILR-ADMM with q =
0.2, SNR=11.21dB
(d) Recovery by ILR-ADMM with q =
0.4, SNR=11.35dB
(e) Recovery by ILR-ADMM with q =
0.6, SNR=11.42dB
(f) Recovery by ILR-ADMM with q =
0.8, SNR=11.37dB
Figure 3: Recovered images with different q
17
In the second test, we compare our algorithm with the nonconvex ADMM. We focus on the case
q = 0.4. The blurring operator ? is generated by the Matlab orders fspecial(’gaussian’,.,.). The
parameter is set as ? = 10?4, e is generated by the Gaussian noise N (0, 0.01). The result reconstructed
by the convex method (i.e., q = 1) is also reported. Figure 4 presents the results with different algorithms.
And we can see that ILR-ADMM performs best in the perspective of SNR. ILR-ADMM is faster than
the nonconvex ADMM due to that the nonconvex ADMM employs inner loops. And LIR-ADMM is just
a little slower than the convex ADMM but with a better effect.
5 Conclusion
In this paper, we consider a class of nonconvex and nonsmooth minimizations with linear constrains
which have applications in signal processing and machine learning research. The classical ADMM method
for these problems always encounter both computational and mathematical barriers in solving the sub-
problem. We organically combined the reweighted algorithm and linearized techniques, and then designed
a new ADMM. In the proposed algorithm, each subproblem just needs to calculate the proximal maps.
The convergence is proved under several assumptions on the parameters and functions. And numerical
results demonstrate the efficiency of our algorithm.
Acknowledgments
We are grateful for the support from the National Science Foundation of China (No.61402495).
References
[1] Brendan PW Ames and Mingyi Hong. Alternating direction method of multipliers for penalized
zero-variance discriminant analysis. Computational Optimization and Applications, 64(3):725–754,
2016.
[2] Hedy Attouch, Je?ro?me Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-
algebraic and tame problems: proximal algorithms, forward–backward splitting, and regularized
gauss–seidel methods. Mathematical Programming, 137(1-2):91–129, 2013.
[3] Je?ro?me Bolte, Aris Daniilidis, and Adrian Lewis. The Lojasiewicz inequality for nonsmooth subana-
lytic functions with applications to subgradient dynamical systems. SIAM Journal on Optimization,
17(4):1205–1223, 2007.
[4] Je?ro?me Bolte, Shoham Sabach, and Marc Teboulle. Proximal alternating linearized minimization
for nonconvex and nonsmooth problems. Mathematical Programming, 146(1-2):459–494, 2014.
[5] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with
applications to imaging. Journal of mathematical imaging and vision, 40(1):120–145, 2011.
[6] Rick Chartrand and Brendt Wohlberg. A nonconvex admm algorithm for group sparsity with sparse
groups. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference
on, pages 6009–6013. IEEE, 2013.
[7] Rick Chartrand and Wotao Yin. Iteratively reweighted algorithms for compressive sensing. In
Acoustics, speech and signal processing, 2008. ICASSP 2008. IEEE international conference on,
pages 3869–3872. IEEE, 2008.
[8] Gong Chen and Marc Teboulle. A proximal-based decomposition method for convex minimization
problems. Mathematical Programming, 64(1-3):81–101, 1994.
18
(a) Original signal (b) Blurred and noised signal
(c) Recovery by ILR-ADMM,
SNR=11.05dB 9.8s
(d) Recovery by nonconvex ADMM,
SNR=12.13dB 36.4s
(e) Recovery by convex ADMM,
SNR=10.84dB 7.7s
Figure 4: Reconstructed images and time cost by different methods
19
[9] Ingrid Daubechies, Ronald DeVore, Massimo Fornasier, and C Sinan Gu?ntu?rk. Iteratively reweighted
least squares minimization for sparse recovery. Communications on Pure and Applied Mathematics,
63(1):1–38, 2010.
[10] Wei Deng and Wotao Yin. On the global and linear convergence of the generalized alternating
direction method of multipliers. Journal of Scientific Computing, 66(3):889–916, 2016.
[11] Ernie Esser, Xiaoqun Zhang, and Tony F Chan. A general framework for a class of first order primal-
dual algorithms for convex optimization in imaging science. SIAM Journal on Imaging Sciences,
3(4):1015–1046, 2010.
[12] Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational
problems via finite element approximation. Computers & Mathematics with Applications, 2(1):17–
40, 1976.
[13] Cuixia Gao, Naiyan Wang, Qi Yu, and Zhihua Zhang. A feasible nonconvex relaxation approach to
feature selection. In Aaai, pages 356–361, 2011.
[14] Donald Geman and Chengda Yang. Nonlinear image recovery with half-quadratic regularization.
IEEE Transactions on Image Processing, 4(7):932–946, 1995.
[15] Roland Glowinski and A Marroco. Sur l’approximation, par e?le?ments finis d’ordre un, et la re?solution,
par pe?nalisation-dualite? d’une classe de proble?mes de dirichlet non line?aires. Revue franc?aise
d’automatique, informatique, recherche ope?rationnelle. Analyse nume?rique, 9(R2):41–76, 1975.
[16] Bingsheng He and Xiaoming Yuan. On the o(1/n) convergence rate of the douglas–rachford alter-
nating direction method. SIAM Journal on Numerical Analysis, 50(2):700–709, 2012.
[17] Bingsheng He and Xiaoming Yuan. On non-ergodic convergence rate of douglas–rachford alternating
direction method of multipliers. Numerische Mathematik, 130(3):567–577, 2015.
[18] Michael Hintermu?ler and Tao Wu. Nonconvex tvq-models in image restoration: Analysis and a trust-
region regularization–based superlinearly convergent solver. SIAM Journal on Imaging Sciences,
6(3):1385–1415, 2013.
[19] Mingyi Hong and Zhi-Quan Luo. On the linear convergence of the alternating direction method of
multipliers. Mathematical Programming, 162(1-2):165–199, 2017.
[20] Mingyi Hong, Zhi-Quan Luo, and Meisam Razaviyayn. Convergence analysis of alternating direction
method of multipliers for a family of nonconvex problems. SIAM Journal on Optimization, 26(1):337–
364, 2016.
[21] Krzysztof Kurdyka. On gradients of functions definable in o-minimal structures. In Annales de
l’institut Fourier, volume 48, pages 769–784. Chartres: L’Institut, 1950-, 1998.
[22] Guoyin Li and Ting Kei Pong. Global convergence of splitting methods for nonconvex composite
optimization. SIAM Journal on Optimization, 25(4):2434–2460, 2015.
[23] Guoyin Li and Ting Kei Pong. Douglas–rachford splitting for nonconvex optimization with applica-
tion to nonconvex feasibility problems. Mathematical programming, 159(1-2):371–401, 2016.
[24] Yanli Liu, Ernest K Ryu, and Wotao Yin. A new use of douglas-rachford splitting and admm for iden-
tifying infeasible, unbounded, and pathological conic programs. arXiv preprint arXiv:1706.02374,
2017.
[25] Stanislas  Lojasiewicz. Sur la ge?ome?trie semi-et sous-analytique. Ann. Inst. Fourier, 43(5):1575–1595,
1993.
20
[26] Michael K Ng, Pierre Weiss, and Xiaoming Yuan. Solving constrained total-variation image restora-
tion and reconstruction problems via alternating direction methods. SIAM journal on Scientific
Computing, 32(5):2710–2736, 2010.
[27] R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science &
Business Media, 2009.
[28] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 2015.
[29] Tao Sun, Roberto Barrio, Lizhi Cheng, and Hao Jiang. Precompact convergence of the nonconvex
primal-dual hybrid gradient algorithm. Journal of Computational and Applied Mathematics, 2017.
[30] Tao Sun, Hao Jiang, and Lizhi Cheng. Convergence of proximal iteratively reweighted nuclear norm
algorithm for image processing. IEEE Transactions on Image Processing, 2017.
[31] Tao Sun, Hao Jiang, and Lizhi Cheng. Global convergence of proximal iteratively reweighted algo-
rithm. Journal of Global Optimization, pages 1–12, 2017.
[32] Tao Sun, Penghang Yin, Lizhi Cheng, and Hao Jiang. Alternating direction method of multipliers
with difference of convex functions. Advances in Computational Mathematics, pages 1–22, 2017.
[33] Joshua Trzasko and Armando Manduca. Highly undersampled magnetic resonance image recon-
struction via homotopic `0-minimization. IEEE Transactions on Medical imaging, 28(1):106–121,
2009.
[34] Huahua Wang and Arindam Banerjee. Bregman alternating direction method of multipliers. In
Advances in Neural Information Processing Systems, pages 2816–2824, 2014.
[35] Yilun Wang, Junfeng Yang, Wotao Yin, and Yin Zhang. A new alternating minimization algorithm
for total variation image reconstruction. SIAM Journal on Imaging Sciences, 1(3):248–272, 2008.
[36] Yu Wang, Wotao Yin, and Jinshan Zeng. Global convergence of admm in nonconvex nonsmooth
optimization. arXiv preprint arXiv:1511.06324, 2015.
[37] Zaiwen Wen, Donald Goldfarb, and Wotao Yin. Alternating direction augmented lagrangian methods
for semidefinite programming. Mathematical Programming Computation, 2(3):203–230, 2010.
[38] Jason Weston, Andre? Elisseeff, Bernhard Scho?lkopf, and Mike Tipping. Use of the zero-norm with
linear models and kernel methods. Journal of machine learning research, 3(Mar):1439–1461, 2003.
[39] Yangyang Xu, Wotao Yin, Zaiwen Wen, and Yin Zhang. An alternating direction algorithm for
matrix completion with nonnegative factors. Frontiers of Mathematics in China, 7(2):365–384, 2012.
[40] Junfeng Yang, Yin Zhang, and Wotao Yin. A fast alternating direction method for tvl1-l2 signal
reconstruction from partial fourier data. IEEE Journal of Selected Topics in Signal Processing,
4(2):288–297, 2010.
[41] Tong Zhang. Analysis of multi-stage convex relaxation for sparse regularization. Journal of Machine
Learning Research, 11(Mar):1081–1107, 2010.
21
