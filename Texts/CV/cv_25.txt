1
Compressed Sensing MRI Reconstruction with
Cyclic Loss in Generative Adversarial Networks
Tran Minh Quan, Student Member, IEEE, Thanh Nguyen-Duc, and Won-Ki Jeong, Member, IEEE
Abstract—Compressed Sensing MRI (CS-MRI) has provided
theoretical foundations upon which the time-consuming MRI ac-
quisition process can be accelerated. However, it primarily relies
on iterative numerical solvers which still hinders their adaptation
in time-critical applications. In addition, recent advances in deep
neural networks have shown their potential in computer vision
and image processing, but their adaptation to MRI reconstruction
is still in an early stage. In this paper, we propose a novel
deep learning-based generative adversarial model, RefineGAN,
for fast and accurate CS-MRI reconstruction. The proposed
model is a variant of fully-residual convolutional autoencoder
and generative adversarial networks (GANs), specifically de-
signed for CS-MRI formulation; it employs deeper generator
and discriminator networks with cyclic data consistency loss
for faithful interpolation in the given under-sampled k-space
data. In addition, our solution leverages a chained network to
further enhance the reconstruction quality. RefineGAN is fast
and accurate – the reconstruction process is extremely rapid, as
low as tens of milliseconds for reconstruction of a 256×256 image,
because it is one-way deployment on a feed-forward network, and
the image quality is superior even for extremely low sampling
rate (as low as 10%) due to the data-driven nature of the method.
We demonstrate that RefineGAN outperforms the state-of-the-art
CS-MRI methods by a large margin in terms of both running
time and image quality via evaluation using several open-source
MRI databases.
Index Terms—GAN, ReconGAN, DiscoGAN, CycleGAN
I. INTRODUCTION
MAGNETIC resonance imaging (MRI) has been widelyused as an in-vivo imaging technique because it is non-
intrusive, high-resolution, and safe to living organisms. Even
though MRI does not use dangerous radiation for imaging, its
long acquisition time causes discomfort to patients and hinders
applications in time-critical diagnoses, such as strokes. To speed
up acquisition time, various acceleration techniques have been
developed. One approach is using parallel imaging hardware [1]–
[3] to reduce time-consuming phase-encoding steps. Another
approach is adopting the Compressive Sensing (CS) theory [4]
to MRI reconstruction [5] so that only a small fraction of data
is needed to generate full reconstruction via a computational
method. To apply the CS theory to MRI reconstruction, we
must find a proper sparsifying transformation to make the
signal sparse, e.g., wavelet transformation, and solve an `1
minimization problem with regularizers.
Early work on CS-MRI primarily focused on applying pre-
defined universal sparsifying transforms, such as the discrete
Fourier transform (DFT), discrete cosine transform (DCT), total
Tran Minh Quan, Thanh Nguyen-Duc and Won-Ki Jeong are with Ulsan
National Institute of Science and Technology (UNIST).
E-mail: {quantm, thanhnguyencsbk, wkjeong}@unist.ac.kr
variation (TV), or discrete wavelet transform (DWT) [6], and
developing efficient numerical algorithms to solve nonlinear
optimization problems [7], [8]. More recently, data-driven
sparsifying transforms (i.e., dictionary learning) have gained
much attention in CS-MRI due to their ability to express local
features of reconstructed images more accurately compared to
pre-defined universal transforms. In dictionary learning based
CS-MRI, the reconstructed images are approximated using
either patch-based atoms or convolution filters, and the results
are generated by training the dictionary jointly (blindly) with
the reconstructed images or by using a pre-trained set of atoms
from the database. Even though dictionary learning based
methods show much improved image quality, the reconstruction
process still suffers from longer running time due to the extra
computational overhead for dictionary training and sparse
coding.
In the last five years, deep learning [9] has gained sub-
stantial attention, primarily because it has surpassed human
performance in solving many complex problems. Deep learning
is accomplished by forming a deep neural network from many
perceptron layers and then training the model for a particular
task. In visual recognition tasks, this architecture is able to
learn hierarchically to extract patterns such as handwritten
digits and other features of interest [10] from images [11].
The main drawback of deep neural networks is that they
require an enormous amount of data for adequate training.
To overcome this issue, researchers have begun to collect large
databases [12] containing millions of images from hundreds
of categories. These rich databases have enabled several
advanced architectures, including VGG [13], GoogLeNet [14],
ResNet [15], and many more. With such improvements, comput-
ers became able to mimic artistic painting styles by transferring
complicated properties from one image to another [16]. In
addition, researchers have extended deep learning methods to
medical image analysis [17], enabling automatic classification
and segmentation of data from modalities such as computed
tomography (CT) [18] and MRI [19].
The primary motivation for the proposed work stems from
the following observations: recent advances in deep learning
can “shift” the time-consuming computing process into the
training (pre-processing) phase and save prediction time by
performing only one-pass deployment of neural network instead
of using iterative methods. In addition, the past success of deep
learning in single-image super resolution, denoising, and in-
painting are in line with the analogy of CS-MRI reconstruction,
which is about prediction of missing information from the
incomplete (corrupted) image. Especially, we discovered that
the recently introduced Cycle-Consistent Adversarial Networks
ar
X
iv
:1
70
9.
00
75
3v
1 
 [
cs
.C
V
] 
 3
 S
ep
 2
01
7
2
(CycleGAN [20]) map naturally to CS-MRI problems to enforce
consistency in measurement and reconstruction.
Based on these observations, in this paper, we propose a
novel GAN-based deep architecture for CS-MRI reconstruc-
tion that is fast and accurate. The proposed method builds
upon several state-of-the-art deep neural networks, such as
convolutional autoencoder, residual networks, and generative
adversarial networks (GANs), with a novel cyclic loss for data
consistency constraints that promotes accurate interpolation of
the given undersampled k-space data. Our proposed network
architecture is fully residual, where inter- and intra-layers are
linked via addition-based skip connections to learn residuals, so
the network depth can be effectively increased without suffering
from the gradient vanishing problem and have more expressive
power. In addition, our generator consists of multiple end-to-end
networks chained together where the first network translates
a zero-filling reconstruction image to a full reconstruction
image, and the following networks improve accuracy of full
reconstruction image (i.e., refining the result). To the best of
our knowledge, the proposed work is the first CS-MRI method
employing a cyclic loss with fully residual convolutional
GANs that achieved real-time performance (reconstruction
of a 256×256 image can be done under 100 ms) with superior
image quality (over 42 dB in average for the 40% sampling
rate), which we believe it has a huge potential for time-critical
applications. We demonstrate that our method outperforms
recent CS-MRI methods in terms of both running time and
image quality via performance assessment on several open-
source MRI databases.
The rest of this paper is organized as follows. In Section II,
we review the recent work related to CS-MRI algorithms, with
and without support from deep learning. Section III introduces
the background of CS-MRI, followed by our proposed method
in detail in Section IV. Finally, we show the results and compare
the performance of our method with the performance of other
methods in Section V. We summarize our work and suggest
future research directions in Section VI.
II. RELATED WORK
The current CS-MRI methods can be broadly classified into
three categories: conventional `1 energy minimization approach
using universal sparsifying transformation, machine learning-
based approach using the dictionary and sparse coding, and
deep learning-based approach using state-of-the-art deep neural
networks.
Universal transform-based methods: The long acquisition
time is a fundamental challenge in MRI, so the compressed
sensing (CS) theory has been proposed and successfully
applied to speed up the acquisition process. Conventional
CS-MRI reconstruction methods have been developed to
leverage the sparsity of signal by using universal sparsifying
transforms, such as Fourier transform, Total Variation (TV), and
Wavelets [5], and to exploit the spatio-temporal correlations,
such as k? t FOCUSS [21], [22] This sparsity-based CS-MRI
method introduces computational overhead in the reconstruction
process due to solving expensive nonlinear `1 minimization
problem, which leads to developing efficient numerical al-
gorithms [7] and adopting parallel computing hardware to
accelerate the computational time [1]. The nuclear norm and
low-rank matrix completion techniques have been employed
for CS-MRI reconstruction as well [23]–[25] .
Dictionary learning-based methods: The main limitation
of universal transform-based methods is that the transformation
is general and not specifically designed for the input data.
In contrast, dictionary learning [26] (DL) can generate data-
specific dictionary and improve the image quality. Earlier
work using DL in CS-MRI is using image patches to train
dictionary [27]–[29], which may suffer from redundant atoms
and longer running time. More recently, convolutional sparse
coding (CSC), a new learning-based sparse representation,
approximates the input signal with a superposition of sparse
feature maps convolved with a collection of filters. CSC is
shift-invariant and can be efficiently computed using parallel
algorithms in the frequency domain. 2D and 3D CSC have
been successfully adopted in dynamic CS-MRI [2], [3], and
efficient numerical algorithms, such as the alternating direction
method of multipliers, are proposed to further accelerate CSC
computation in CS-MRI [30], [31]
Deep learning-based methods: Deep learning-based CS-
MRI is aimed to design fast and accurate method that
reconstruct high-quality MR images from under-sampled k-
space data using multi-layer neural networks. Earlier work
using deep learning in CS-MRI is mostly about the direct
mapping between a zero-filling reconstruction image to a full-
reconstruction image using a deep convolutional autoencoder
network [32]. Lee et al. [33] proposed a similar autoencoder-
based model but the method learns noise (i.e., residual) from a
zero-filling reconstruction image to remove undersampling
artifacts. Another interesting deep learning-based CS-MRI
approach is Deep ADMM-Net [34], which is a deep neural
network architecture that learns parameters of the ADMM
algorithm (e.g., penalty parameters, shrinkage functions, etc.)
by using training data. This deep model consists of multiple
stages, each of which corresponds to a single iteration of the
ADMM algorithm. Recently, Generative Adversarial Nets [35]
(GANs), a general framework for estimating generative models
via an adversarial process, has shown outstanding performance
in image-to-image translation. Unsupervised variants of GANs,
such as DiscoGAN [36] and CycleGAN [20], have been
proposed for mapping different domains without matching data
pairs. Inspired by their success in image processing, GANs have
been employed for reconstructing zero-filling under-sampled
MRI with [37] and without [38] the consideration of data
consistency during the training process. As shown above, deep
learning has proven itself very promising in CS-MRI as well
for reducing reconstruction time while maintaining superior
image quality. However, its adaptation in CS-MRI is still in
its early stage, which leaves room for improvement.
III. BACKGROUND
In this section, we will briefly overview the basics of
Compressed Sensing MRI. In the following discussion, we use
the subscript f to denote the result of applying a masked Fourier
transform to a given variable. For example, under-sampled raw
MRI data (or measurements) that we wish to reconstruct are
3
Database
Measurement
RF F
H
R
H
F
H
R
H
Generator G
Discriminator D
Real Fake Fake
ReconGAN RefineGAN 
G
en
er
at
iv
e 
A
dv
er
sa
ria
l L
os
s
Data Consistency Loss
Fig. 1. Overview of the proposed method: it aims to reconstruct the images which are satisfied the constraint of under-sampled measurement data; and
whether those look similar to the fully aliasing-free results. Additionally, if the fully sampled images taken from the database go through the same process of
under-sampling acceleration; we can still receive the reconstruction as expected to the original images.
denoted as mf . Then, the zero-filling reconstruction m can be
obtained by the following equation:
m = FHRH (mf ) (1)
where F is the Fourier operator, R is the sampling mask,
and superscript H indicates the conjugated transpose of a
given operator. Intuitively, turning any image s into under-
sampled measurement sf can be done via the inverse of the
reconstruction process:
sf = RF (s) (2)
Compressed sensing MRI reconstruction, which is a process
of reconstructing image s from under-sampled k-space data
mf , can be described as follows:
min
s
J (s) s.t. RF (s) = mf (3)
and the above constrained problem can be reformulated in an
unconstrained fashion with weighting parameters as follows:
min
s
?RF (s)?mf?22 + ?J (s) (4)
where J (s) is a regularizer required for ill-posed optimization
problems. Many classical priors can be used for J (s), such
as Tikhonov regularization (IID Gaussian prior) [39], edge-
preserving regularization [40], total variation (TV) [41], non-
local mean filter (NLM) [42], wavelets [43], curvelets [44],
etc. By enforcing `p norm where 0 ? p ? 1 for regularization,
compressed sensing theory [4], [45] can be applied to MRI
reconstruction from highly undersampled k-space data [5].
This sparsity-induced image reconstruction method aims to
find the solution (images s) that satisfies not only the under-
sampled measurement constraints but also sparsity in the
transformed domain by decomposing the signals with the
designated universal transform sparsity basis. For example, the
seminal work by Lustig et al. [46] proposed that J (s) is equal
to ??s?0, in which ? is the wavelet transform, and further
relaxed the `0 norm by `1 norm substitution. Solving such
nonlinear optimization problems usually involves an iterative
process to minimize the data consistency energy and the
sparsity energy. Therefore, the primary goal of the present work
is leveraging state-of-the-art deep learning for modeling `1-
regularized CS-MRI formulations to avoid the time-consuming
iterative optimization process as well as to improve the quality
of the reconstructed image.
IV. METHOD
A. Overview of the Proposed Method
Figure 1 represents an overview of the proposed method: Our
generator G consists of two-fold chained networks that try to
reconstruct the MRI image directly from under-sampled k-space
(zero-filled reconstruction). The generated result is favorable
to the fully-sampled data taken from an extensive database
and put through the same under-sampling process. In contrast,
the discriminator D attempts to differentiate between the real
MRI instances from the database and the fake results output
generated by G. The entire system involves training G and
D adversarially until a balance is reached at the convergence
stage. Details of each component will be discussed shortly.
B. Generative Adversarial Loss
Our objective is to train generator G, which can transform
any zero-filling reconstruction m = FHRH(mf ), mf ?Mf ,
where Mf is the collection of under-sampled k-space data, to
a fully-reconstructed image s under the constraint that s is
indistinguishable from all images s ? S reconstructed from
full k-space data. To accomplish this aim, a discriminator D
is attached to distinguish whether the image is synthetically
generated from m by G (s, which is considered fake) or
is reconstructed from fully-sampled k-space data (s, which
is considered real). We wish to train D so that it can
maximize the probability of assigning the correct true or
false label to images. Note that the objective function for
4
Fig. 2. Two learning processes are trained adversarially to achieve better reconstruction from generator G and to fool the ability of recognizing the real or
fake MR image from discriminator D.
Fig. 3. The cyclic data consistency loss, which is a combination of under-sampled frequency loss and the fully reconstructed image loss.
D can be interpreted as maximizing the log-likelihood for
estimating the conditional probability, where the image comes
from: D (s) = D (G (m)) = 0 (fake), and D (s) = 1
(real). Simultaneously, generator G is trained to minimize
[1? logD (s)] or [1? logD (G (m))]. This can be addressed
by formally defining an adversarial loss Ladv, for which we
wish to find the solutions of its minimax problem:
min
G
max
D
Ladv (G,D) (5)
where
Ladv (G,D) = E
m?Mf
[1? logD (G (m))] + E
s?S
[logD (s)]
(6)
Figure 2 is a schematic depiction of our adversarial process:
G tries to generate images s = G (m) that look similar to
the images s that have been reconstructed from full k-space
data, while D aims to distinguish between s and s. Once the
convergence stage is achieved, G can produce the result s that
is close to s. Once the training converges, G can produce the
result s that is close to s, and D is unable to differentiate
between them, which results in bringing the probability for
both real and fake labels to 50%.
C. Data Consistency Loss
In an extreme case, with large enough resources and data,
the network can map the zero-filling reconstruction m to any
existing fully reconstructed images s ? S. Therefore, the
adversarial loss alone is not sufficient to correctly map the
under-sampled data mf [n] and the full reconstruction s[n] for
all n. To strengthen the bridge connection between m[n] and
s[n], we introduce an additional constraint, the data consistency
loss Lcyc, which is a combination of under-sampled frequency
loss Lfreq and fully reconstructed image loss Limag in a cyclic
fashion. The first term Lfreq guarantees that when we perform
another under-sampled operator RF on reconstructed images
s[i] to get sf [i], the difference between sf [i] and mf [i] should
be minimal. The second energy term Limag ensures that for any
other images s[j] ? S taken from the fully reconstructed data,
if s[j] goes through the under-sampling process (by applying
RF ), and the generator G takes zero-filling reconstruction
m[j] (by applying FHRH to mf [j] or sf [j]) to produce the
reconstruction s[j], then both s[j] and s[j] should appear to be
similar. Those two losses are described in a cyclic fashion in
Figure 3. In practice, various distance metrics, such as mean-
square-error (MSE), mean-absolute-error (MAE), etc, can be
employed to implement Lcyc:
Lcyc (G) = Lfreq (G) + Limag (G)
= d (mf [i] ,mf [i]) + d (s [j] , s [j])
(7)
Note that the data consistency loss only affects the generator
G and not the discriminator D. Furthermore, each individual
loss Lfreq or Limag is evaluated on its own samples: mf [i]
and s[j] are drawn independently from Mf and S.
D. Model Architecture
In this section, we introduce the details of our neural network
architecture, which is a variant of a convolutional autoencoder
and deep residual network.
1) Fundamental blocks: To begin discussing the model
architecture, we first introduce three fundamental components
in our generative adversarial model: encoder, decoder, and
residual blocks. The details of each block are described
pictorially in Figure 5. The encoder block, shaded in blue,
accepts a 4D tensor input and performs 2D convolution
with filter_size 4×4, and stride is equal to 2 so
that it performs down-sampling with convolution without
5
Fig. 4. Generator G can reconstruct inverse amplitude of the noise causes by reconstruction from under-sampled k-space data. The final result is obtained by
adding the zero-filling reconstruction to the output of G
Fig. 5. Basic building blocks of the proposed network architecture.
a separate max-pooling layer. The number of feature maps
filter_number is denoted in the top right. The decoder
block, shaded in red, functions as the convolution transpose,
which enlarges the resolution of the 4D input tensor by two
times. Its fixed settings have filter_size 4×4, stride
is equal to 1, the number of feature maps filter_number
is NUMBER, and the remaining parameter output_shape
indicates the desired shape of the 4D output tensor. The
residual block, shaded in violet, is used to increase the depth
of generator G and discriminator D networks, and it consists
of three convolution layers: the first layer conv_1×1i with
filter_size is 1×1, stride is 1 and the number of
feature maps is NUMBER/2, which reduces the dimension of
the tensor by half. The second layer conv_4×4m performs
the filtering via a 4×4 convolution with the same number of
feature maps NUMBER/2. The remaining conv_1×1o, which
has filter_size 1×1, stride is 1 and NUMBER, feature
maps, retrieves the input tensor shape so that they can be
combined to form a residual bottleneck. This residual block
allows us to effectively construct a deeper generator G and
discriminator D without suffering from the gradient vanishing
problem [15].
2) Generator architecture: Figure 4 illustrates the architec-
ture of our generator G. It is built based on the design of a
convolutional autoencoder, which consists of an encoding path
(left half of the network) to retrieve the compressed information
in latent space, and a symmetric decoding path (right half of
the network) that enables the prediction of synthesis. The
convolution mode we used is “same”, which leads the final
reconstruction to have a size identical to the input images. The
encoding and decoding paths consist of multiple levels, i.e.,
image resolutions, to extract features in different scales. Three
types of introduced building blocks (i.e., encoder, residual,
decoder and their related parameters) are used to construct the
proposed generator G.
It is worth noting that the proposed generator G does
not attempt to reconstruct the image directly. Instead, it
is trained to produce the inverse amplitude of the noise
caused by reconstruction from under-sampled data. The final
reconstruction is obtained by adding the zero-filling input to
the output from the generator G, which is similar to other
current machine learning-based CS-MRI methods [33], [37],
[38].
3) Discriminator architecture: For the discriminator D, we
use an architecture identical to that of the encoding path of
the generator G. The output of the last residual block is used
to evaluate the mentioned adversarial loss Ladv (Equation 6).
To reiterate, if the discriminator receives the image s ? S,
it will result in D (s) = 1, as a true result. Otherwise,
the reconstruction s will be recognized as a fake result, or
equivalently, D (s) = D (G (m)) = 0.
E. Full Objective Function and Training Specifications
In summary, our system involves two sub-networks which
are trained adversarially to minimize the following loss:
Ltotal = Ladv (G,D) + ?Lfreq (G) + ?Limag (G) (8)
where ? and ? are the weights which help to control the balance
between each contribution. We set ? = 0.01 and ? = 10.0 for
all the experiments. The Adam optimizer [47] is used with the
initial learning rate of 1e?4, which decreases monotonically
over 500 epochs. Our source code will be tentatively published
and available 1. The entire framework was implemented using
a system-oriented programming wrapper (tensorpack 2) of the
tensorflow 3 library.
1http://hvcl.unist.ac.kr/RefineGAN/
2http://tensorpack.readthedocs.io/
3http://www.tensorflow.org/
6
(a) ReconGAN (b) RefineGAN
Fig. 6. One-fold (a) and chaining two-fold architectures (b) of the proposed generator G.
F. Chaining with Refinement Network
The proposed generator G by itself can perform an end-
to-end reconstruction from the zero-filling MRI to the final
prediction. However, in the real-world setup, many iterative
methods also take extra steps to go through the current result
and then attempt to “correct” the mistakes. Therefore, we
introduce an additional step in refining the reconstruction
by concatenating a chain of multiple generators to resolve
the ambiguities of the initial prediction from the generator.
For example, Figure 6 shows the two-fold chaining generator
in a self-correcting form. By forcing the desired ground-
truth between them, the entire solution becomes a target-
driven approach. This enables our method to be a single-
input and multi-output model, where each checkpoint in
between attempts to produce better a reconstruction. Because
the architecture of each sub-generator is the same, we can
think of the proposed model is another variant of the recurrent
neural network that treats the entire sub-generator as a single
state without sharing the weights after unfolding. The loss
training curves of the checkpoints decrease as the number of
checkpoints increases, and they eventually converge as the
number of training iterations (epochs) increases. Interestingly,
our generator can be considered a V-cycle in the multigrid
method that is commonly used in numerical analysis, where the
contraction in the encoding path is similar to restriction heads
from fine to coarse grid. The expansion in the decoding path
spans along the prolongation toward the final reconstruction,
and the skip connections act as the relaxation. We refer to the
first check point as ReconGAN and the second check point in
our two-fold chaining network as RefineGAN.
V. RESULT
(a) 10% (b) 20% (c) 30% (d) 40%
Fig. 7. Radial sampling masks used in our experiments.
We trained many versions of the proposed networks with
different factors of under-sampling masks (10%, 20%, 30%,
0 100 200 300 400 500
Epochs
25
30
35
40
45
P
S
N
R
s 
(d
B
)
Rate 10%
Rate 20%
Rate 30%
Rate 40%
Fig. 8. PSNR curves of RefineGAN with different undersampling rates on
the brain training set over 500 epochs.
40%) on the training sets. As shown in Figure 8, those trained
models (for RefineGAN) reach a convergence stage in a few
hundred training iterations. The performances on the test sets
are expected to show similar results.
TABLE I
RUNNING TIME COMPARISON OF VARIOUS CS-MRI METHODS ON TWO
TESTING DATASETS (IN SECONDS).
Abbv. Methods Brain Chest
CSCMRI [2], [3] 8.56808 9.37082
DLMRI [28], [29], [48] 604.24623 613.84531
DeepADMM [34] 0.31725 0.28677
DeepDirect [32], [33]
SingleGAN [37], [38] 0.08711 0.08446
ReconGAN —
RefineGAN — 0.16181 0.18116
We used two sets of MR images from the IXI database4 (the
brain dataset) and from the Data Science Bowl challenge 5
(the chest dataset) to assess the performance of our method by
comparing our results with state-of-the-art CS-MRI methods
(e.g., Convolutional sparse coding-based [2], [3], patch-based
dictionary [28], [29], [48], deep learning-based [32]–[34], and
GAN-based [37], [38]). The image resolution of each image
is 256×256. From each database, we randomly selected 100
images for training the network and another 100 images for
testing (validating) the result. We conducted the experiments for
various sampling rates (i.e., 10%, 20%, 30%, and 40% of the
4http://brain-development.org/ixi-dataset/
5https://www.kaggle.com/c/second-annual-data-science-bowl/data
7
Full reconstruction Zero-filling CSCMRI DeepDirect RefineGAN
Fig. 9. Image quality comparison at a low sampling rate (10%). Note that our method (RefineGAN) can generate significantly better results compared to other
state-of-the-art methods.
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
22
24
26
28
30
32
(a) Rate 10%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
26
28
30
32
34
36
(b) Rate 20%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
28
30
32
34
36
38
40
42
(c) Rate 30%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
32
34
36
38
40
42
44
(d) Rate 40%
Fig. 10. PSNRs evaluation on the brain test set. Unit: dB
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
0.4
0.5
0.6
0.7
0.8
0.9
(a) Rate 10%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
0.5
0.6
0.7
0.8
0.9
(b) Rate 20%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
(c) Rate 30%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
0.70
0.75
0.80
0.85
0.90
0.95
1.00
(d) Rate 40%
Fig. 11. SSIMs evaluation on the brain test set
original k-space data), corresponding to 10×, 5×, 3.3×, and
2.5× factors of acceleration. We assume the target MRI data
type is static, and radial sampling masks are used (Figure 7). It
is worth noting that our experimental data are real-valued MRI
images, which require pre-processing of the actual acquisition
from the MRI scanner because the actual MRI data is complex-
valued. Additional data preparation steps, such as data range
normalization and imaginary channel concatenation, are also
required.
Running Time Evaluation: Table I summarizes the running
times of our method and other state-of-the-art learning-based
CS-MRI methods. Even though dictionary learning-based ap-
proaches leverage pre-trained dictionaries, their reconstruction
time depends on the numerical methods used. For example,
CSCMRI by Quan and Jeong [2], [3] employed a GPU-based
ADMM method, which is considered one of the state-of-
the-art numerical methods, but the running time is still far
from interactive (about 9 seconds). Another type of dictionary
learning-based method, DLMRI [28], [29], [48], solely relies
on the CPU implementation of a greedy algorithm, so their
reconstruction times are significantly longer (around 600
seconds) than those of the others with GPU-acceleration. Deep
learning-based methods, including DeepADMM, DeepDirect,
and our method, are extremely fast (e.g., less than a second)
because deploying a feed-forward convolutional neural network
is a single-pass image processing that can be accelerated using
GPUs reasonably well. DeepADMM significantly accelerated
time-consuming iterative computation to as low as 0.2 second.
8
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
20
22
24
26
28
30
32
(a) Rate 10%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
24
26
28
30
32
34
(b) Rate 20%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
26
28
30
32
34
36
38
40
42
(c) Rate 30%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
30
32
34
36
38
40
42
44
46
(d) Rate 40%
Fig. 12. PSNRs evaluation on the chest test set. Unit: dB
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
0.5
0.6
0.7
0.8
0.9
(a) Rate 10%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
0.65
0.70
0.75
0.80
0.85
0.90
(b) Rate 20%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
0.80
0.85
0.90
0.95
(c) Rate 30%
Zer
o­fi
llin
g
CS
CM
RI
DL
MR
I
De
epA
DM
M
De
epD
irec
t
Sin
gle
GA
N
Re
con
GA
N
Re
fine
GA
N
0.88
0.90
0.92
0.94
0.96
0.98
1.00
(d) Rate 40%
Fig. 13. SSIMs evaluation on the chest test set
The running times of DeepDirect [32], [33], SingleGAN [37],
[38] and our ReconGAN are, all similarly, about 0.08 second
because they share the same network architecture (i.e., single-
fold generator G). The running time of RefineGAN is about
twice as long because two identical generators are serially
chained in a single architecture, but it is still around 0.1 second.
As shown in this experiment, we observed that deep learning-
based approaches are well-suited for CS-MRI in a time-critical
clinical setup due to their extremely low running times.
Image Quality Evaluation: To assess the quality of recon-
structed images, we use two metrics, Peak-Signal-To-Noise
ratio (PSNR) and Structural Similarity (SSIM). Figure 14 and
15 show the representative reconstruction of the brain and chest
test sets, respectively, using various reconstruction methods at
different sampling rates (10% and 30%) and their error plots
using a jet color map (blue: low, red: high error). Additionally,
Figure 10, 12 and 11, 13 show, respectively, their PSNR and
SSIM error graphs. Overall, our methods (ReconGAN and
RefineGAN) was able to reconstruct images with higher PSNRs
and SSIMs. Note that we used the identical generator and
discriminator networks (i.e., the same number of neurons) for
DeepDirect, SingleGAN, and our own method to enable a
fair comparison. We observed that our cyclic loss increases
the PSNR by around 1dB, and the refinement network further
reduces the error to a similar degree. We also observed that our
cyclic loss seems more helpful for higher sampling rates (see
the difference between SingleGAN and ReconGAN) which
is due to data consistency constraints enforced by the loss.
We think that for lower sampling rates, not much information
can be interpolated from the data, so the final results become
similar over different methods, mostly synthesized from learned
information. However, for higher sampling rates, our method
can accurately interpolate the input sampling data, which
eventually improves the total image quality to close to that of
the full reconstruction.
By qualitatively comparing the reconstructed results, we
found that deep learning-based methods generate more natural
images than dictionary-based methods. For example, CSCMRI
and DLMRI produce cartoon-like piecewise linear images with
sharp edges, which is mostly due to sparsity enforcement.
In comparison, our method generates results that are much
closer to full reconstructions while edges are still preserved; in
addition, noise is significantly reduced. Note also that, unlike
other CS-MRI methods, our method can generate reasonably
good results even at extremely low sampling rates (as low as
10%, see Figure 9).
VI. CONCLUSION
In this paper, we introduced a novel deep learning-based
generative adversarial model for solving the Compressed Sens-
ing MRI reconstruction problem. The proposed architecture,
RefineGAN, which is inspired by the most recent advanced
neural networks, such as U-net, Residual CNN, and GANs,
is specifically designed to have a deeper generator network
G and is trained adversarially with the discriminator D with
cyclic data consistency loss to promote better interpolation of
the given undersampled k-space data for accurate end-to-end
MR image reconstruction. We demonstrated that RefineGAN
outperforms the state-of-the-art CS-MRI methods in terms of
running time and image quality, thus indicating its usefulness
for time-critical clinical applications.
In the future, we plan to conduct an in-depth analysis of
RefineGAN to better understand the architecture, as well as
constructing incredibly deep multi-fold chains with the hope of
further improving reconstruction accuracy based on its target-
driven characteristic. Extending RefineGAN to handle dynamic
9
MRI is an immediate next research direction. Developing a
distributed version of RefineGAN for parallel training and
deployment on a cluster system is another research direction
we wish to explore.
REFERENCES
[1] T. M. Quan, S. Han, H. Cho, and W.-K. Jeong, “Multi-GPU reconstruction
of dynamic compressed sensing MRI.” in Proceeding of MICCAI, 2015,
pp. 484–492.
[2] T. M. Quan and W.-K. Jeong, “Compressed sensing dynamic MRI
reconstruction using GPU-accelerated 3D convolutional sparse coding.”
in Proceeding of MICCAI, 2016, pp. 484–492.
[3] T. M. Quan and W.-K. Jeong, “Compressed sensing reconstruction of
dynamic contrast enhanced MRI using GPU-accelerated convolutional
sparse coding,” in Proceeding of IEEE ISBI, 2016, pp. 518–521.
[4] D. L. Donoho, “Compressed sensing,” IEEE Transactions on information
theory, vol. 52, no. 4, pp. 1289–1306, 2006.
[5] M. Lustig, D. L. Donoho, J. M. Santos, and J. M. Pauly, “Compressed
sensing MRI,” IEEE signal processing magazine, vol. 25, no. 2, pp.
72–82, 2008.
[6] I. Daubechies, Ten Lectures on Wavelets, ser. CBMS-NSF Regional
Conference Series in Applied Mathematics. Society for Industrial and
Applied Mathematics, 1992.
[7] T. Goldstein and S. Osher, “The split bregman method for l1-regularized
problems,” SIAM journal on imaging sciences, vol. 2, no. 2, pp. 323–343,
2009.
[8] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
optimization and statistical learning via the alternating direction method
of multipliers,” Foundations and Trends R© in Machine Learning, vol. 3,
no. 1, pp. 1–122, 2011.
[9] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,
no. 7553, pp. 436–444, May 2015.
[10] “ImageNet Classification with Deep Convolutional Neural Networks,” in
Proceedings of NIPS, 2012.
[11] M. D. Zeiler and R. Fergus, “Visualizing and Understanding Convolu-
tional Networks,” in Proceedings of ECCV, 2014, pp. 818–833.
[12] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei,
“ImageNet Large Scale Visual Recognition Challenge,” International
Journal of Computer Vision, vol. 115, no. 3, pp. 211–252, Dec. 2015.
[13] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” CoRR, vol. abs/1409.1556, 2014.
[14] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in
Proceedings of IEEE CVPR, 2015, pp. 1–9.
[15] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of IEEE CVPR, 2016, pp. 770–778.
[16] L. A. Gatys, A. S. Ecker, and M. Bethge, “Image Style Transfer Using
Convolutional Neural Networks,” in Proceeding of IEEE CVPR, 2016,
pp. 2414–2423.
[17] O. Cicek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
“3D U-Net: Learning Dense Volumetric Segmentation from Sparse
Annotation,” in Proceedings of MICCAI, 2016, pp. 424–432.
[18] Y. Zheng, D. Liu, B. Georgescu, H. Nguyen, and D. Comaniciu, “3D Deep
Learning for Efficient and Robust Landmark Detection in Volumetric
Data,” in Proceeding of MICCAI, 2015, pp. 565–572.
[19] “Review of MRI-based brain tumor image segmentation using deep
learning methods,” Procedia Computer Science, vol. 102, pp. 317 – 324,
2016.
[20] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired Image-to-Image
Translation using Cycle-Consistent Adversarial Networks,” arXiv preprint
arXiv:1703.10593, 2017.
[21] H. Jung, J. C. Ye, and E. Y. Kim, “Improved k–t BLAST and k–t SENSE
using FOCUSS,” Physics in medicine and biology, vol. 52, no. 11, p.
3201, 2007.
[22] H. Jung, K. Sung, K. S. Nayak, E. Y. Kim, and J. C. Ye, “k-t FOCUSS:
A general compressed sensing framework for high resolution dynamic
MRI,” Magnetic resonance in medicine, vol. 61, no. 1, pp. 103–116,
2009.
[23] J. Yao, Z. Xu, X. Huang, and J. Huang, “Accelerated dynamic MRI
reconstruction with total variation and nuclear norm regularization,” in
Proceeding of MICCAI, 2015, pp. 635–642.
[24] R. Otazo, E. Candès, and D. K. Sodickson, “Low-rank plus sparse
matrix decomposition for accelerated dynamic MRI with separation of
background and dynamic components,” Magnetic Resonance in Medicine,
vol. 73, no. 3, pp. 1125–1136, 2015.
[25] B. Trémoulhéac, N. Dikaios, D. Atkinson, and S. R. Arridge, “Dynamic
MR Image Reconstruction–Separation From Undersampled (k-t)-Space
via Low-Rank Plus Sparse Prior,” IEEE Transactions on Medical Imaging,
vol. 33, no. 8, pp. 1689–1701, 2014.
[26] M. Aharon, M. Elad, and A. Bruckstein, “K-SVD: An algorithm for
designing overcomplete dictionaries for sparse representation,” IEEE
Transactions on Signal Processing, vol. 54, no. 11, pp. 4311–4322, 2006.
[27] S. P. Awate and E. V. DiBella, “Spatiotemporal dictionary learning
for undersampled dynamic MRI reconstruction via joint frame-based
and dictionary-based sparsity,” in Proceeding of IEEE ISBI, 2012, pp.
318–321.
[28] J. Caballero, A. N. Price, D. Rueckert, and J. V. Hajnal, “Dictionary
learning and time sparsity for dynamic MR data reconstruction,” IEEE
Transactions on Medical Imaging, vol. 33, no. 4, pp. 979–994, 2014.
[29] S. Ravishankar and Y. Bresler, “MR image reconstruction from highly
undersampled k-space data by dictionary learning,” IEEE Transactions
on Medical Imaging, vol. 30, no. 5, pp. 1028–1041, 2011.
[30] H. Bristow, A. Eriksson, and S. Lucey, “Fast convolutional sparse coding,”
in Proceedings of the IEEE CVPR, 2013, pp. 391–398.
[31] B. Wohlberg, “Efficient convolutional sparse coding,” in Proceeding of
IEEE ICASSP, 2014, pp. 7173–7177.
[32] S. Wang, Z. Su, L. Ying, X. Peng, S. Zhu, F. Liang, D. Feng, and
D. Liang, “Accelerating magnetic resonance imaging via deep learning,”
in Proceeding of IEEE ISBI, 2016, pp. 514–517.
[33] D. Lee, J. Yoo, and J. C. Ye, “Deep residual learning for compressed
sensing MRI,” in Proceeding of IEEE ISBI, 2017, pp. 15–18.
[34] J. Sun, H. Li, Z. Xu et al., “Deep ADMM-net for compressive sensing
MRI,” in Proceeding of NIPS, 2016, pp. 10–18.
[35] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative Adversarial Nets,” in
Proceeding of NIPS, 2014, pp. 2672–2680.
[36] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim, “Learning to Discover
Cross-Domain Relations with Generative Adversarial Networks,” arXiv
preprint arXiv:1703.05192, 2017.
[37] M. Mardani, E. Gong, J. Y. Cheng, S. Vasanawala, G. Zaharchuk,
M. Alley, N. Thakur, S. Han, W. Dally, J. M. Pauly et al., “Deep
Generative Adversarial Networks for Compressed Sensing Automates
MRI,” arXiv preprint arXiv:1706.00051, 2017.
[38] S. Yu, H. Dong, G. Yang, G. Slabaugh, P. L. Dragotti, X. Ye, F. Liu,
S. Arridge, J. Keegan, D. Firmin et al., “Deep De-Aliasing for Fast
Compressive Sensing MRI,” arXiv preprint arXiv:1705.07137, 2017.
[39] L. Chaâri, J.-C. Pesquet, A. Benazza-Benyahia, and P. Ciuciu, “Autocali-
brated regularized parallel MRI reconstruction in the wavelet domain,”
in Proceeding of ISBI, 2008, pp. 756–759.
[40] Z. Tian, X. Jia, K. Yuan, T. Pan, and S. B. Jiang, “Low-dose ct
reconstruction via edge-preserving total variation regularization,” Physics
in medicine and biology, vol. 56, no. 18, p. 5949, 2011.
[41] C. Chen, Y. Li, and J. Huang, “Calibrationless parallel MRI with joint
total variation regularization,” in Proceeding of MICCAI, 2013, pp. 106–
114.
[42] J. V. Manjón, J. Carbonell-Caballero, J. J. Lull, G. García-Martí, L. Martí-
Bonmatí, and M. Robles, “MRI denoising using non-local means,”
Medical image analysis, vol. 12, no. 4, pp. 514–523, 2008.
[43] A. Pizurica, A. M. Wink, E. Vansteenkiste, W. Philips, and B. J. Roerdink,
“A review of wavelet denoising in MRI and ultrasound brain imaging,”
Current medical imaging reviews, vol. 2, no. 2, pp. 247–260, 2006.
[44] H. Bhadauria and M. Dewal, “Medical image denoising using adaptive
fusion of curvelet transform and total variation,” Computers & Electrical
Engineering, vol. 39, no. 5, pp. 1451–1460, 2013.
[45] E. J. Candès and M. B. Wakin, “An introduction to compressive sampling,”
IEEE signal processing magazine, vol. 25, no. 2, pp. 21–30, 2008.
[46] M. Lustig, D. Donoho, and J. M. Pauly, “Sparse MRI: The application
of compressed sensing for rapid mr imaging,” Magnetic Resonance in
Medicine, vol. 58, no. 6, pp. 1182–1195, 2007.
[47] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
2014.
[48] J. Caballero, D. Rueckert, and J. V. Hajnal, “Dictionary learning and
time sparsity in dynamic MRI,” in Proceeding of MICCAI, 2012, pp.
256–263.
10
10% 30%
Fu
ll
re
co
n
Z
er
o-
fil
lin
g
C
SC
M
R
I
D
L
M
R
I
D
ee
pA
D
M
M
D
ee
pD
ir
ec
t
Si
ng
le
G
A
N
R
ec
on
G
A
N
R
efi
ne
G
A
N
Fig. 14. The results on brain test set.
From left to right: the reconstructions of 10% and 30% under-sampling rates.
From top to bottom: Full reconstruction, Zero-filling reconstruction, CSCMRI,
DLMRI, DeepADMM, DeepDirect, SingleGAN, ReconGAN and RefineGAN.
10% 30%
Fu
ll
re
co
n
Z
er
o-
fil
lin
g
C
SC
M
R
I
D
L
M
R
I
D
ee
pA
D
M
M
D
ee
pD
ir
ec
t
Si
ng
le
G
A
N
R
ec
on
G
A
N
R
efi
ne
G
A
N
Fig. 15. The results on chest test set.
From left to right: the reconstructions of 10% and 30% under-sampling rates.
From top to bottom: Full reconstruction, Zero-filling reconstruction, CSCMRI,
DLMRI, DeepADMM, DeepDirect, SingleGAN, ReconGAN and RefineGAN.
