Lensless-camera	based	machine	learning	for	image	
classification	
GANGHUN	KIM,1	STEFAN	KAPETANOVIC,1	RACHAEL	PALMER,2	RAJESH	MENON,*	
1Department	of	Electrical	and	Computer	Engineering,	University	of	Utah,	Salt	Lake	City,	UT	84112	
2Department	of	BioEngineering,	University	of	Utah,	Salt	Lake	City,	UT	84112	
*Corresponding	author:	rmenon@eng.utah.edu	
Received	XX	Month	XXXX;	revised	XX	Month,	XXXX;	accepted	XX	Month	XXXX;	posted	XX	Month	XXXX	(Doc.	ID	XXXXX);	published	XX	Month	XXXX	
	
Machine	learning	(ML)	has	been	widely	applied	to	image	
classification.	 Here,	 we	 extend	 this	 application	 to	 data	
generated	 by	 a	 camera	 comprised	 of	 only	 a	 standard	
CMOS	 image	 sensor	 with	 no	 lens.	 We	 first	 created	 a	
database	of	 lensless	 images	of	handwritten	digits.	Then,	
we	 trained	 a	 ML	 algorithm	 on	 this	 dataset.	 Finally,	 we	
demonstrated	 that	 the	 trained	 ML	 algorithm	 is	 able	 to	
classify	 the	 digits	 with	 accuracy	 as	 high	 as	 99%	 for	 2	
digits.	 Our	 approach	 clearly	 demonstrates	 the	 potential	
for	 non-human	 cameras	 in	 machine-based	 decision-
making	scenarios.		
OCIS	codes:	(150.0150)	Machine	Vision;	(100.3190)	Inverse	Problems;	
(110.1758)	Computational	Imaging.		
	
Recently,	 it	has	become	common	to	train	machine-learning	(ML)	
algorithms	to	recognize	objects	in	images	by	exposing	them	to	vast	
databases	 of	 labeled	 images	 [1-4].	 Spectacular	 gains	 in	
classification	accuracy	have	been	obtained	and	ML	algorithms	are	
now	 able	 to	 make	 complex	 decisions	 based	 upon	 object	
recognition	in	image	and	video	data.	These	algorithms	are	typically	
educated	 on	 conventional	 (what	 we	 refer	 to	 as	 human-centric)	
images.	 Recently,	 there	 have	 also	 been	 significant	 advances	 in	
lensless	imaging,	where	a	sensor	that	does	not	have	a	lens	captures	
information	 from	a	 scene	or	 object	 [5-7].	 Such	 lensless	 cameras	
offer	advantages	of	simplicity,	low	cost,	reduced	weight	and	small	
form	factors.	 	Previously,	we	demonstrated	that	frames	captured	
by	a	bare	CMOS	sensor	can	be	used	to	reconstruct	images	that	are	
recognizable	by	humans.	Here,	we	show	that	such	human-centric	
reconstruction	is	actually	not	necessary	for	machine-based	image	
recognition	and	classification.	Specifically,	we	train	a	ML	algorithm	
on	 a	 database	 of	 frames	 created	 by	 a	 lensless	 camera	 and	
demonstrate	that	the	algorithm	is	capable	of	 image	classification	
using	the	data	directly	from	the	lensless	camera.		
	
Our	 lensless	 camera	 is	 simply	 a	 conventional	 bare	 CMOS	 sensor	
(The	 Imaging	 Source,	 DFM	 22BUC03-ML).	 We	 use	 a	 liquid-crystal	
display	(LCD)	to	show	various	images	of	handwritten	digits	from	0	to	9	
[8].	 Sample	 images	 of	 handwritten	 digits	 were	 obtained	 from	 the	
MNIST	 database.	 MNIST	 database	 has	 been	 used	 for	 image	
classification	 test	 since	 the	 1990’s,	 and	 is	widely	 used	 for	 assessing	
image	classification	accuracy.	The	LCD	is	placed	about	250mm	away	
from	and	facing	the	CMOS	sensor	as	shown	in	Fig.	1(a).	The	exposure	
time	is	150ms	and	we	average	100	frames	to	reduce	noise.	Using	this	
procedure,	we	created	a	database	of	70,000	images	with	appropriate	
labels.	 	 Examples	 of	 the	 original	 MNIST	 images	 and	 their	
corresponding	lensless	images	are	shown	in	Fig.	1(b).	
	
	
Figure	 1:	 (a)	 Schematic	 of	 our	 experiment.	 Handwritten	 digits	 are	
displayed	on	an	LCD,	and	lensless	images	are	captured	by	a	bare	CMOS	
sensor.	 Approximately	 70,000	 training	 images	 were	 created.	 (b)	
Example	of	one	handwritten	digit	(“3”)	and	its	corresponding	lensless	
image.	
		
As	raw	images	are	fairly	large	(307,200	pixels	in	each	image),	it	is	
not	 suitable	 for	 large	 scale	 training	 in	 its	 raw	 form.	 Hence,	 SURF	
(speeded	up	robust	 features)	extraction	[9]	and	subsequent	K-mean	
clustering	[10]	are	performed	to	reduce	data	dimension.	Note	that	we	
used	 2000	 to	 3000	 images	 to	 perform	 feature	 extraction	 as	 they	
provide	sufficient	information	to	approximate	image	features	that	are	
present	throughout	the	whole	set.	Extracted	features	are	then	piped	to	
the	Matlab	machine	learning	toolbox	(also	called	classification	learner)	
[11],	 after	 data	 is	 properly	 formatted	 for	 the	 tool,	 to	 perform	 the	
training.	Subsequently,	we	used	a	variety	of	learning	methods	to	train	
the	model.	 Algorithms	 used	 for	 the	 training	 include	 support	 vector	
machine	(SVM)	[1],	decision	tree	[12],	k-nearest	neighbors	(KNN)	[3],	
and	their	variants.	Once	trained,	each	trained	model	is	measured	for	
accuracy	using	set	number	of	test	images	as	described	below.	We	then	
picked	 the	 model	 with	 the	 highest	 prediction	 accuracy,	 which	 we	
report	in	Fig.	2.	In	most	cases,	the	SVM	algorithm	achieved	the	highest	
accuracy.		
	
Next,	we	 also	 generated	 lensless	 images	 in	 the	 same	manner	 for	
testing	 the	 trained	 algorithm.	We	 ensured	 that	 the	 images	 used	 for	
testing	were	 completely	different	 than	 the	ones	used	during	 feature	
extraction	and	ML	training.	The	training	and	testing	procedures	were	
separately	conducted	for	databases	comprising	of	2	digits	(“0”	and	“1”),	
5	digits	(“0”	to	“”)	and	10	digits	(“0”	to	“9”).		The	accuracy	with	which	
the	ML	 algorithm	 classifies	 the	 lensless	 frames	 into	 the	 appropriate	
digit	class	was	verified	against	the	known	image	labels,	allowing	us	to	
measure	 the	 classification	 accuracy.	 The	 classification	 accuracy	 is	
plotted	for	the	3	cases	(2,	5	and	10	digits)	as	a	function	of	the	number	
of	training	images	in	Fig.	2.	As	the	number	of	training	images	increases,	
the	classification	accuracy	increases	and	seems	to	saturate	for	all	cases.	
The	classification	accuracy	decreases	as	the	number	of	digits	increases.	
The	best	classification	accuracy	is	about	99%	for	2	digits	with	7,800	
training	images.		Although	the	classification	accuracy	is	lower	for	more	
number	of	digits,	 this	 can	be	 improved	by	using	deep	convolutional	
networks	or	related	algorithms.	
	
	
Figure	 2:	 Best	 classification	 accuracy	 as	 a	 function	 of	 number	 of	
training	images	used.		
	
In	 conclusion,	 we	 demonstrate	 that	 machine	 learning	 can	 be	
effectively	 applied	 to	 classify	 lensless	 images.	 Such	 non-human	
image-based	 decision-making	 could	 lead	 to	 significant	
improvements	in	the	ability	of	autonomous	agents	to	navigate	and	
make	sense	of	the	external	world.		
	
Funding.	National	Science	Foundation	(Grant	#	10037833).	Utah	
Science	 Technology	 and	Research	 Initiative.	 University	 of	 Utah’s	
Undergraduate	Research	Opportunities	Program	(UROP).	
REFERENCES	
1.	O.	Chapelle,	P.	Haffner	and	V.	N.	Vapnik,	IEEE	Trans.	Neural	Netw.	10,	1055	
(1999).	
2.	D.	Ciregan,	U.	Meier	and	J.	Schmidhuber,	2012	IEEE	Conference	on	
Computer	Vision	and	Pattern	Recognition	(IEEE,	2012),	pp.	3642.	
3.	O.	Boiman,	E.	Shechtman	and	M.	Irani,	2008	IEEE	Conference	on	Computer	
Vision	and	Pattern	Recognition	(IEEE,	2008),	pp.	1.	
4.	I.	Laptev,	M.	Marszalek,	C.	Schmid	and	B.	Rozenfeld,	2008	IEEE	Conference	
on	Computer	Vision	and	Pattern	Recognition	(IEEE,	2008),	pp.	1.	
5.	G.	Kim,	K.	Isaacson,	R.	Palmer,	and	R.	Menon,	Appl.	Opt.	56,	6450	(2017).	
6.	M.	S.	Asif,	A.	Ayremlou,	A.	Sankaranarayanan,	A.	Veeraraghavan,	R.	
Baraniuk,	https://arxiv.org/abs/1509.00116	
7.	P.	R.	Gill	and	D.	G.	Stork,	Proceedings	of	Computational	Optical	Sensing	and	
Imaging,	(Optical	Society	of	America,	2013),	paper	CW4C.3	
8.	Y.	LeCun,	C.	Cortes,	C.	Burges,	http://yann.lecun.com/exdb/mnist/	
9.	H.	Bay,	T.	Tuytelaars,	and	L.	V.	Gool,	Computer	vision-ECCV.	2006,	404	
(2006).	
10.	C.	W.	Chen,	J.	Luo	and	K.	J.	Parker,	IEEE	Trans.	Image	Process.	7(12),	1673	
(1998).	
11.	Mathwork,	https://www.mathworks.com/products/statistics.html	
12.	S.	R.	Safavian	and	D.	Landgrebe,	IEEE	Trans.	Syst.,	Man,	and	Cybern.	21(3),	
660	(1991).		
	 	
REFERENCES	
1.	O.	Chapelle,	P.	Haffner	and	V.	N.	Vapnik,	"Support	vector	machines	for	
histogram-based	image	classification,"	IEEE	Trans.	Neural	Netw.	10,	1055-
1064	(1999).	
2.	D.	Ciregan,	U.	Meier	and	J.	Schmidhuber,	"Multi-column	deep	neural	
networks	for	image	classification,"	in	2012	IEEE	Conference	on	Computer	
Vision	and	Pattern	Recognition	(IEEE,	2012),	pp.	3642-3649.	
3.	O.	Boiman,	E.	Shechtman	and	M.	Irani,	"In	defense	of	Nearest-Neighbor	
based	image	classification,"	in	2008	IEEE	Conference	on	Computer	Vision	and	
Pattern	Recognition	(IEEE,	2008),	pp.	1-8.	
4.	I.	Laptev,	M.	Marszalek,	C.	Schmid	and	B.	Rozenfeld,	"Learning	realistic	
human	actions	from	movies,"	in	2008	IEEE	Conference	on	Computer	Vision	
and	Pattern	Recognition	(IEEE,	2008),	pp.	1-8.	
5.	G.	Kim,	K.	Isaacson,	R.	Palmer,	and	R.	Menon,	"Lensless	photography	with	
only	an	image	sensor,"	Appl.	Opt.	56,	6450-6456	(2017)	
6.	M.	S.	Asif,	A.	Ayremlou,	A.	Sankaranarayanan,	A.	Veeraraghavan,	R.	
Baraniuk,	“FlatCam:	Thin,	Bare-Sensor	Cameras	using	Coded	Aperture	and	
Computation”,	https://arxiv.org/abs/1509.00116	
7.	P.	R.	Gill	and	D.	G.	Stork,	“Lensless	ultra	miniature	imagers	using	odd-	
symmetry	phase	gratings,”	in	Proceedings	of	Computational	Optical	Sensing	
and	Imaging,	(Optical	Society	of	America,	2013),	paper	CW4C.3	
8.	Y.	LeCun,	C.	Cortes,	C.	Burges,	“The	MNIST	Database	of	Handwritten	
Digits”,	http://yann.lecun.com/exdb/mnist/	
9.	H.	Bay,	T.	Tuytelaars,	and	L.	V.	Gool,	“SURF:	Speeded	Up	Robust	Features,”	
in	Computer	vision-ECCV.	2006,	404-417	(2006).	
10.	C.	W.	Chen,	J.	Luo	and	K.	J.	Parker,	"Image	segmentation	via	adaptive	K-
mean	clustering	and	knowledge-based	morphological	operations	with	
biomedical	applications,"	IEEE	Trans.	Image	Process.	7(12),	1673-1683	(1998).	
11.	Mathwork,	“Statistics	and	Machine	Learning	Toosbox”,	
https://www.mathworks.com/products/statistics.html	
12.	S.	R.	Safavian	and	D.	Landgrebe,	"A	survey	of	decision	tree	classifier	
methodology,"	IEEE	Trans.	Syst.,	Man,	and	Cybern.	21(3),	660-674	(1991).	
