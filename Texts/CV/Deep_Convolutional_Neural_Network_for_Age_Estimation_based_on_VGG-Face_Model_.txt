Deep Convolutional Neural Network for Age Estimation based on 
VGG-Face Model  
 
Zakariya Qawaqneh(1), Arafat Abu Mallouh(1), Buket D. Barkana(2) 
(1)Department of Computer Science and Engineering, University of Bridgeport, 
(2)Department of Electrical Engineering, University of Bridgeport, 
Technology Building, Bridgeport CT 06604 USA 
Emails: {zqawaqneh; aabumall@my.bridgeport.edu}, bbarkana@bridgeport.edu 
 
Abstract— Automatic age estimation from real-world and unconstrained face images is rapidly gaining 
importance. In our proposed work, a deep CNN model that was trained on a database for face recognition task is 
used to estimate the age information on the Adience database. This paper has three significant contributions in this 
field. (1) This work proves that a CNN model, which was trained for face recognition task, can be utilized for age 
estimation to improve performance; (2) Over fitting problem can be overcome by employing a pretrained CNN on a 
large database for face recognition task; (3) Not only the number of training images and the number subjects in a 
training database effect the performance of the age estimation model, but also the pre-training task of the employed 
CNN determines the model’s performance.  
 
 Index Terms—Age classification, estimation, Convolutional Neural Network, face images, unconstrained database.  
I. INTRODUCTION 
Recently, many applications from biometrics, security control to entertainment use the information extracted from 
face images that contain information about age, gender, ethnic background, and emotional state. Automatic age 
estimation from facial images is one of the popular and challenging tasks that have different fields of applications 
such as controlling the content of the watched media depending on the customer's age [1-3].  
Automatic estimation of the age is a challenging process since the aging process among humans is non-uniform. 
In addition, extracting an effective feature set from a 2D image for age estimation is another challenge to overcome.  
CNNs showed significant success in face recognition, image classification, and object recognition. It consists of 
different convolutional layers where each layer processes the output of the previous layer in order to produce a 
robust and compact output. CNNs can be described as deep networks if the number of layers inside the network is 
relatively a large number. If a CNN is characterized as a deep network, hence a large database is needed to optimize 
the parameters during the training process [4]. 
In this work, a pretrained deep CNN is utilized to estimate the age from unconstrained face images. The deep 
network was trained initially for face recognition task on a large database other than the one used for age estimation. 
Deep CNNs are capable to perform tasks efficiently with a condition in which they are trained on large databases. 
Currently, there is no relatively large database for age estimation task.  
II. LITERATURE REVIEW 
Earliest methods used the size and proportions on the human face. These methods are limited to young ages due 
to the nature of the human head that does change significantly in the adulthood [5]. Later, the active appearance 
model (AAM) [6], ages pattern subspace (AGES) [7, 8], age manifold [9, 11], 3D morphable model [10] are 
proposed. Semantic-level description model for facial features characterization was used in [12, 13]. The effective 
texture descriptor and local binary patterns (LBP) were used by [14] for appearance feature extraction.  
Table I summarizes the other related works. The previously mentioned literature utilized relatively small 
databases. Moreover, the type of the face images was lab and constrained images. Recently, new benchmarks for age 
estimation using unconstrained facial images have been designed. The unconstrained face images increase the 
challenge of age estimation due to the high variation of the real-world images which are taken in different 
environments. 
 
TABLE I  
PREVIOUS AGE ESTIMATION METHODS 
Work Feature set Classifier Accuracy Database 
[15] Gabor feature  Fuzzy-
LDA 
91% Private 
[16, 17] Spatial 
Flexible  
Patch  
GMM 4.94 MAE FG-NET 
[18] Graphical 
facial features,  
topology, 
geometry 
ANN 5.974 MAE FG-NET 
[19] Bio-inspired 
features 
 (BIF) 
SVM 4.77 MAE FG-NET 
[20] BIF and the 
age manifold 
SVM 2.61 MAE - f 
2.58 MAE - m 
YGA 
 
Two examples of recent benchmarks are the Group Photos [21] and the Adience benchmark [22]. The Adience is 
considered as the newest and the most challenging benchmark for age estimation using face images. The studies [2, 
23, 24] were evaluated on the Alience.  
III. CNNS ARCHITECTURE AND TRAINING FOR AGE ESTIMATION 
In this section we explain our proposed deep CNN that estimates the age of a subject from 2D images. Deep 
CNNs are powerful models, which are capable to capture effective information. One of the most challenging 
problems in the machine learning is the overfitting problem that occurs when using small databases. In deep neural 
networks, the problem of the overfitting becomes even worse due to the fact that deep networks have millions of 
parameters, since they have several number of layers with thousands of nodes. All databases that are built for age 
classification and prediction are relatively small in size. They are not comparable in size with other databases 
designed for face recognition and image classification tasks. To overcome the overfitting problem, we architect our 
proposed deep CNN for age estimation by using a deep CNN model trained for face recognition on a very large 
database. 
A: Architecture  
Using a trained CNN as a facial feature extractor is expected to be useful as a keystone for training CNN to 
estimate the age from the face images. The proposed CNN architecture relies on a very deep face recognition CNN 
architecture which is capable of extracting facial features distinctively and robustly. As well as, it will be less prone 
to overfitting.     
There are a few CNN models that were successfully trained for face recognition task. In this work, we use the 
VGG-face model proposed by [25] which achieved the state-of-the-art results on the LFW [26] and YFT [27] 
databases. VGG-Face consists of 11 1ayers, eight convolutional layers and 3 fully connected layers. As shown in 
Table II, each convolutional layer is followed by a rectification layer, whereas a max pool layer is operated at the 
end of each convolutional block.  
 
TABLE II 
ARCHITECTURE AND CONFIGURATION OF THE PROPOSED MODEL 
Layer# 0 1 2 3 4 
Name n/a Conv1 Relu1 Norm1 Pool1 
Support n/a 3 1 3 1 
Filt dim n/a 3 n/a 64 n/a 
Num filts n/a 64 n/a 64 n/a 
Stride n/a 1 1 1 1 
pad n/a 1 0 1 0 
Layer# 5 6 7 8 9 
Name Conv2 Relu2 Norm2 Pool2 Conv3 
Support 2 3 1 3 1 
Filt dim n/a 64 n/a 128 n/a 
Num filts n/a 128 n/a 128 n/a 
Stride 2 1 1 1 1 
pad 0 1 0 1 0 
Layer# 10 11 12 13 14 
Name Relu3 Conv4 Relu4 Conv5 Relu3_2 
Support 2 3 1 3 1 
Filt dim n/a 128 n/a 256 n/a 
Num filts n/a 256 n/a 256 n/a 
Stride 2 1 1 1 1 
pad 0 1 0 1 0 
Layer# 15 16 17 18 19 
Name Conv3_3 Relu3_3 Mpool3 Conv4_1 Relu4_1 
Support 3 1 2 3 1 
Filt dim 256 n/a n/a 256 n/a 
Num filts 256 n/a n/a 512 n/a 
Stride 1 1 2 1 1 
pad 1 0 0 1 0 
Layer# 20 21 22 23 24 
Name Onv4_2 Relu4_2 Conv4_3 Relu4_3 Mpool4 
Support 3 1 3 1 2 
Filt dim 512 n/a 512 n/a n/a 
Num filts 512 n/a 512 n/a n/a 
Stride 1 1 1 2 2 
pad 1 0 1 0 0 
Layer# 25 26 27 28 29 
Name Conv5_1 Relu5_1 Conv5_2 Relu5_2 Conv5_3 
Support 3 1 3 1 3 
Filt dim 512 n/a 512 n/a 512 
Num filts 512 n/a 512 n/a 512 
Stride 1 1 1 1 1 
pad 1 0 1 0 1 
Layer# 30 31 32 33 34 
Name relu5_3 pool5 fc6 relu6 drop6 
Support 1 2 7 1 1 
Filt dim n/a n/a 512 n/a n/a 
Num filts n/a n/a 4096 n/a n/a 
Stride 1 2 1 1 1 
pad 0 0 0 0 0 
Layer# 35 36 37 38 39 
Name fc7 relu7 drop7 fc8 relu8 
Support 1 1 1 1 1 
Filt dim 4096 n/a n/a 5000 n/a 
Num filts 5000 n/a n/a 5000 n/a 
Stride 1 1 1 1 1 
pad 0 0 0 0 0 
Layer# 40 41 42   
Name drop8 fc9 prob   
Support 1 1 1   
Filt dim n/a 5000 n/a   
 
The fully c
data are th
followed b
The last fu
the labels (
keeping th
fully conne
size of the
5000. The 
 
B: Training
The inp
optimizatio
of size 256
regularize 
decreased 
between th
10-2 stand
network. T
output laye
The stoc
the predict
layers’ par
parameters
trained and
 
C: Predict
A test 
obtained fr
and upper-
extracted i
the final pr
averaged. T
N
onnected laye
e same. The 
y a dropout lay
lly connected 
classes) in the
e convolutiona
cted layers. T
 first fully con
output size of 
  
ut images are
n of our prop
 and momentu
the network p
by a factor of
e newly added
ard deviation, 
hen the outpu
r (last layer) o
hastic gradien
ion of the sof
ameter are n
 to predict the
 optimized for
ion  
image is resca
om the center
right corners 
mages are fed
obability vecto
his method re
um filts 
Stride 
pad 
rs are a specia
number of the
er with a drop
layer of this m
 database. In th
l layers of thi
he first three 
nected layer i
the output laye
 rescaled to 2
osed network 
m value of 0.9
arameters duri
 10 whenever 
 fully connect
while the biase
t of each hidde
f the network 
t decent metho
tmax-log-loss 
ot changed an
 age of subjec
 face recogniti
led to 256x25
 of the origina
of the origina
to the model 
r of class scor
duces the imp
n/a 8
1 1
0 0
l case of the c
 input feature
 rate of p=0.5.
odel represen
is work, we a
s model uncha
fully connecte
s 4096, while
r represents th
56 x 256 pix
is carried out b
. In addition, 
ng the training
there is no im
ed layers are i
s are initialize
n layer is fed 
is calculated. 
d optimizes a
for age estima
d kept frozen
ts while not ch
on task by [25
Fig. 1. T
6 pixels. Then
l test image. T
l test image, 
to calculate th
es for the orig
act of poor qua
n/a 
1 
0 
onvolutional 
s for the first
  
ts an N-way cl
rchitect and re
nged while rep
d layers are fo
 the size of th
e number of a
el, then rando
y using stoch
the weight dec
 process. The 
provement in
nitialized by u
d to zero. The
to the next hid
nd finds the p
tion as shown
. In other w
anging the pa
].  
he proposed m
 three images 
he second and
respectively. B
e softmax prob
inal test image
lity images, su
 
 
 
layers where t
 two fully co
ass predictor, 
train the VGG
lacing the ful
llowed by dro
e second and 
ge labels whic
mly cropped 
astic gradient 
ay is set to 10
training starts
 the validation
sing a Gaussia
 RGB input im
den layer as a
arameters of t
 in Fig. 1. In 
ords, we opti
rameters of th
odel 
are extracted o
 third images 
y using the t
ability output
, the output sc
ch as low-res
 
 
 
he size of the 
nnected layer
where N repre
-Face model f
ly connected l
pout and rect
the third fully
h is 8. 
to 224 x 224
descent metho
-3. A dropout 
 by 0.1 learnin
 set accuracy
n distribution
age is fed to t
n input until t
he connected 
the meanwhil
mize the full
e convolutiona
 
f size 224x22
are extracted 
rained CCN n
 vector for ea
ore vectors of 
olutions, and o
filters and the
s is 4096. Th
sents the num
or age estimat
ayers with fou
ification layer
 connected lay
 pixel patches
d with mini-b
rate of 0.6 is u
g rate and the
 result. The w
 with zero mea
he input layer 
he probability 
layers that min
e, the convolu
y connected l
l layers which
4. The first im
from the botto
etwork, these
ch image. To 
the three imag
cclusions. 
 input 
ey are 
ber of 
ion by 
r new 
s. The 
ers is 
. The 
atches 
sed to 
n it is 
eights 
n and 
of the 
of the 
imize 
tional 
ayers’ 
 were 
age is 
m-left 
 three 
obtain 
es are 
IV. EXPERIMENTAL RESULTS AND DISCUSSION 
Different experiments have been carried out to evaluate the proposed work. The specifications of the used 
benchmark database and details about the conducted experiments are reported in this section.  
The Adience benchmark is one of the newest databases designed for age estimation from face images. In this 
work, the Adience is used to evaluate the efficiency of the proposed work. It consists of unconstrained face images 
of 2284 subjects and has 8 age labels which were uploaded to the Flicker website. Table III lists the age labels and 
the number of images per label for males and females. 
 
TABLE III  
THE ADIENCE DATABASE 
  Labels in years Total Gender 0-2 4-6 8-13 15-20 25-32 38-43 48-53 60- 
Female 682 1234 1360 919 2589 1056 433 427 9411 
Male 745 928 934 734 2308 1294 392 442 8192 
 
A performance comparison between the proposed work and the state-of-the-art methods reported in [22] and [2] is 
presented in Table IV. Furthermore, Table IV shows the 1-off accuracy that represents the accuracy when the result 
is off by 1 adjacent age label left or right. Based on our results, the proposed work significantly outperforms the 
state-of-the-art results in terms of the exact accuracy and for 1-off accuracy. These results confirm the efficiency of 
the proposed work. Table V gives the confusion matrix for the proposed model. 
 
TABLE IV 
 OVERALL ACCURACY FOR DIFFERENT CNN ARCHITECTURE 
Method Exact Accuracy 1-off Accuracy
[22] 45.1 79.5 
[2] using single crop 49.5 84.6 
[2] using over-sample 50.7 84.7 
Proposed Work 59.9 90.57 
 
TABLE V 
 CONFUSION MATRIX FOR THE PROPOSED MODEL BY USING THE VGG-FACE CNN 
Actual 
Predict 0-2 4-6 8-13 15-20 25-32 38-43 48-53 60- 
0-2 93.17 6.42 0.21 0.00 0.21 0.00 0.00 0.00 
4-6 26.84 62.11 7.37 1.93 1.58 0.00 0.18 0.00 
8-13 1.76 6.18 42.06 12.94 35.59 0.29 1.18 0.00 
15-20 1.76 0.44 4.41 24.23 64.76 0.00 4.41 0.00 
25-32 0.00 0.09 0.85 3.22 86.17 3.31 4.83 1.52 
38-43 0.39 0.20 0.39 1.78 59.76 8.88 22.88 5.72 
48-53 0.00 0.00 0.00 0.00 19.50 8.71 38.17 33.61 
60- 0.00 0.00 0.00 1.17 1.95 1.17 35.02 60.70 
 
From Table V, it is noticed that the 0-2 year old age label is estimated with the highest accuracy, 93.17%. Images 
of infants contain distinctive features that enable the classifier to distinguish this age group easily. 15-20 and 38-43 
year old age labels classified with the lowest accuracies, 24.3% and 8.88%, respectively. Both labels are adjacent to 
Label 5 that is the 25-32 year old age group. Label 5 is classified with accuracy of 86.17%. These results might be a 
result of the difference in subject numbers between labels 4 and 6 and the label 5. The number of subjects in label 5 
is very large compared to the labels 4 and 6. Therefore, label 5 is trained and classified better by affecting the results 
of its adjacent labels. Labels 4 and 6 are highly misclassified with label 5, 64.76%, 59.76%, respectively. 
 
 
To prov
classificati
proposed m
connected 
used wher
GoogLeNe
process as 
presents th
estimation
To perfo
provided r
CNN mod
trained for 
Fig.2 sh
blur, poor 
proposed w
 
V. 
In this p
VGG-Face
tuned to p
database w
GoogLeNe
estimation 
images and
training tas
OVE
e the efficienc
on on ImageN
odel. We mo
layers and cha
e the number
t is retrained 
mentioned in 
e performanc
. 
rm age estima
easonable resu
el trained for f
image classifi
ows some of 
lighting, low-r
ork. 
Fig. 2. Som
CONCLUSIO
aper, we propo
 that was train
erform age es
hich is the new
t was trained 
is not compet
 the number s
k of the emplo
RALL ACCU
Labe
0-2 
4-6 
8-13
15-20
25-32
38-43
48-53
60- 
Overall Ac
y of the propo
et ILSVRC da
dified and fine
nging the num
 of nodes for
and fine-tune
section 3. Th
e of the prop
tion, the empl
lts. However, 
ace recognitio
cation.  
the images of
esolution, mo
e of the succe
N 
sed a model t
ed for face r
timation. The
est challengin
on a very larg
itive with the 
ubjects in a tr
yed CNN dete
T
RACY OF DI
l PropVG
 
 
 
 
 
curacy 
sed work furth
tabase [29] is 
-tuned the Go
ber of nodes
 each layer i
d while preser
e modified Go
osed model b
oyment of the 
based on the r
n task are mor
 the Adience 
tion, pose, and
ssfully classifi
o perform age 
ecognition on 
 proposed mo
g age estimat
e database th
proposed mod
aining databa
rmines the net
ABLE VI 
FFERENT CN
osed model by
G-Face CNN
93.17 
62.11 
42.06 
24.23 
86.17 
8.88 
38.17 
60.70 
59.90 
ermore, the G
retrained, fine
ogLeNet CNN
. In the modif
s 1024, 2048,
ving the conv
ogLeNet CNN
y using VGG
GoogLeNet C
esults in Table
e effective tha
database. Alth
 facial expres
ed challenging
estimation bas
a large databa
del outperform
ion benchmark
at contain mil
el using the V
se effect the p
work’s perfor
N ARCHITE
 Model 
GoogLeNe
86.75
27.89
21.47
14.10
76.6
12.03
7.05
34.63
45.07
oogLeNet [28
-tuned and test
 to perform ag
ied architectur
 2048, and 8
olutional laye
 achieved 45
-Face CNN a
NN that was tr
 VI, it is clear
n the features
ough these ar
sions, they al
 images from t
ed on facial im
se. The VGG
s the previo
 that consists 
lions of trainin
GG-Face. No
erformance fo
mance for age 
CTURES (%) 
by  
t CNN
 
 
 
 
1 
 
 
 
 
] model that w
ed for age est
e prediction b
e, four fully c
 respectively. 
rs unchanged
.07% in age e
nd the GoogL
ained for imag
 that features 
 extracted by 
e very challen
l are classified
 
he Adience da
ages by using
-Face CNN is
us works by 
of unconstrain
g images, its 
t only the nu
r age estimati
estimation. 
as trained for 
imation by usi
y replacing th
onnected laye
Then the mo
 during the tr
stimation. Tab
eNet CNN fo
e classificatio
extracted by u
using a CNN 
ging images h
 correctly usi
tabase 
 a deep CNN 
 modified and
9% on the A
ed face image
performance 
mber of the tr
on, but also th
image 
ng the 
e fully 
rs are 
dified 
aining 
le VI 
r age 
n task 
sing a 
model 
aving 
ng the 
called 
 fine-
dience 
s. The 
in age 
aining 
e pre-
REFERENCES 
[1] Y. Fu, G. Guo, and T. S. Huang, "Age synthesis and estimation via faces: A survey," IEEE transactions on 
pattern analysis and machine intelligence, vol. 32, pp. 1955-1976, 2010. 
[2] G. Levi and T. Hassner, "Age and gender classification using convolutional neural networks," in Proceedings of 
the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2015, pp. 34-42. 
[3] H. Han, C. Otto, X. Liu, and A. K. Jain, "Demographic estimation from face images: Human vs. machine 
performance," IEEE transactions on pattern analysis and machine intelligence, vol. 37, pp. 1148-1161, 2015. 
[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural 
networks," in Advances in neural information processing systems, 2012, pp. 1097-1105. 
[5] N. Ramanathan and R. Chellappa, "Modeling age progression in young faces," in 2006 IEEE Computer Society 
Conference on Computer Vision and Pattern Recognition (CVPR'06), 2006, pp. 387-394. 
[6] A. Lanitis, C. J. Taylor, and T. F. Cootes, "Toward automatic simulation of aging effects on face images," IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 24, pp. 442-455, 2002. 
[7] X. Geng, Z.-H. Zhou, and K. Smith-Miles, "Automatic age estimation based on facial aging patterns," IEEE 
Transactions on pattern analysis and machine intelligence, vol. 29, pp. 2234-2240, 2007. 
[8] X. Geng, Z.-H. Zhou, Y. Zhang, G. Li, and H. Dai, "Learning from facial aging patterns for automatic age 
estimation," in Proceedings of the 14th ACM international conference on Multimedia, 2006, pp. 307-316. 
[9] Y. Fu, Y. Xu, and T. S. Huang, "Estimating human age by manifold analysis of face pictures and regression on 
aging features," in 2007 IEEE International Conference on Multimedia and Expo, 2007, pp. 1383-1386. 
[10] K. Scherbaum, M. Sunkel, H. P. Seidel, and V. Blanz, "Prediction of Individual NonLinear Aging 
Trajectories of Faces," in Computer Graphics Forum, 2007, pp. 285-294. 
[11] Y. Fu and T. S. Huang, "Human age estimation with regression on discriminative aging manifold," IEEE 
Transactions on Multimedia, vol. 10, pp. 578-584, 2008. 
[12] J. Hayashi, M. Yasumoto, H. Ito, Y. Niwa, and H. Koshimizu, "Age and gender estimation from facial image 
processing," in SICE 2002. Proceedings of the 41st SICE Annual Conference, 2002, pp. 13-18. 
[13] J. Hayashi, M. Yasumoto, H. Ito, and H. Koshimizu, "Method for estimating and modeling age and gender 
using facial image processing," in Virtual Systems and Multimedia, 2001. Proceedings. Seventh International 
Conference on, 2001, pp. 439-448. 
[14] A. Gunay and V. V. Nabiyev, "Automatic age classification with LBP," in Computer and Information Sciences, 
2008. ISCIS'08. 23rd International Symposium on, 2008, pp. 1-4. 
[15] F. Gao and H. Ai, "Face age classification on consumer images with gabor feature and fuzzy lda method," in 
International Conference on Biometrics, 2009, pp. 132-141. 
[16] S. Yan, M. Liu, and T. S. Huang, "Extracting age information from local spatially flexible patches," in 2008 
IEEE International Conference on Acoustics, Speech and Signal Processing, 2008, pp. 737-740. 
[17] S. Yan, X. Zhou, M. Liu, M. Hasegawa-Johnson, and T. S. Huang, "Regression from patch-kernel," in 
Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 2008, pp. 1-8. 
[18] J. Suo, T. Wu, S. Zhu, S. Shan, X. Chen, and W. Gao, "Design sparse features for age estimation using 
hierarchical face model," in Automatic Face & Gesture Recognition, 2008. FG'08. 8th IEEE International 
Conference on, 2008, pp. 1-6. 
[19] G. Mu, G. Guo, Y. Fu, and T. S. Huang, "Human age estimation using bio-inspired features," in Computer 
Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, 2009, pp. 112-119. 
[20] G. Guo, G. Mu, Y. Fu, C. R. Dyer, and T. S. Huang, "A study on automatic age estimation using a large 
database," in ICCV, 2009, pp. 1986-1991. 
[21] A. C. Gallagher and T. Chen, "Understanding images of groups of people," in Computer Vision and Pattern 
Recognition, 2009. CVPR 2009. IEEE Conference on, 2009, pp. 256-263. 
[22] E. Eidinger, R. Enbar, and T. Hassner, "Age and gender estimation of unfiltered faces," IEEE Transactions on 
Information Forensics and Security, vol. 9, pp. 2170-2179, 2014. 
[23] F. Alnajar, C. Shan, T. Gevers, and J.-M. Geusebroek, "Learning-based encoding with soft assignment for age 
estimation under unconstrained imaging conditions," Image and Vision Computing, vol. 30, pp. 946-953, 2012. 
[24] C. Shan, "Learning local features for age estimation on real-life faces," in Proceedings of the 1st ACM 
international workshop on Multimodal pervasive video analysis, 2010, pp. 23-28. 
[25] O. M. Parkhi, A. Vedaldi, and A. Zisserman, "Deep face recognition," in British Machine Vision Conference, 
2015, p. 6. 
[26] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, "Labeled faces in the wild: A database for studying 
face recognition in unconstrained environments," Technical Report 07-49, University of Massachusetts, 
Amherst2007. 
[27] L. Wolf, T. Hassner, and I. Maoz, "Face recognition in unconstrained videos with matched background 
similarity," in Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, 2011, pp. 529-
534. 
[28] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, et al., "Going deeper with convolutions," in 
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1-9. 
[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, et al., "Imagenet large scale visual recognition 
challenge," International Journal of Computer Vision, vol. 115, pp. 211-252, 2015.B. Smith, “An approach to 
graphs of linear forms (Unpublished work style)”, unpublished. 
 
