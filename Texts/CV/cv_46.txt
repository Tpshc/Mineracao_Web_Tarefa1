payman yadollahpour
E X P L O R I N G A N D E X P L O I T I N G D I V E R S I T Y F O R I M A G E
S E G M E N TAT I O N
ar
X
iv
:1
70
9.
01
62
5v
1 
 [
cs
.C
V
] 
 5
 S
ep
 2
01
7

E X P L O R I N G A N D E X P L O I T I N G D I V E R S I T Y F O R I M A G E S E G M E N TAT I O N
by
payman yadollahpour
A thesis submitted in partial fulfillment of the requirements for the degree of
Doctor of Philosophy in Computer Science
at the
TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO
Chicago, IL
February 2017
Thesis committee:
Dr. Gregory Shakhnarovich (Thesis Adivsor)
Dr. Dhruv Batra
Dr. Amir Globerson
Dr. David McAllester
Dr. Nathan Srebro
Payman Yadollahpour: Exploring and Exploiting Diversity for Image Segmentation, © Febru-
ary 2017
E X P L O R I N G A N D E X P L O I T I N G D I V E R S I T Y F O R I M A G E
S E G M E N TAT I O N
A thesis presented
by
payman yadollahpour
in partial fulfillment of the requirements for the degree of
Doctor of Philosophy in Computer Science
toyota technological institute at chicago
Chicago, IL
February 2017
—Thesis Committee —
Dr. Dhruv Batra Feb 06, 2017
Committee member Signature Date
Dr. Amir Globerson Feb 06, 2017
Committee member Signature Date
Dr. Nathan Srebro Feb 06, 2017
Committee member Signature Date
Dr. Gregory Shakhnarovich Feb 06, 2017
Thesis/Research Advisor Signature Date
Dr. David McAllester Feb 06, 2017
Chief Academic Officer Signature Date

Dedicated to my parents.

A B S T R A C T
Semantic image segmentation is an important computer vision task that is difficult
because it consists of both recognition and segmentation. It is important because it
subsumes important aspects of scene understanding such as image classification and
object localization. The task is often cast as a structured output problem on an ex-
ponentially large output-space, which is typically modeled by a discrete probabilistic
model. The best segmentation is found by inferring the Maximum a-Posteriori (MAP)
solution over the output distribution defined by the model. Due to limitations in op-
timization, the model cannot be arbitrarily complex. This leads to a trade-off: devise
a more accurate model that incorporates rich high-order interactions between image
elements at the cost of inaccurate and possibly intractable optimization OR leverage
a tractable model which produces less accurate MAP solutions but may contain high
quality solutions as other modes of its output distribution.
This thesis investigates the latter and presents a two stage approach to semantic seg-
mentation akin to cascade models and proposal generation works. In the first stage a
tractable segmentation model outputs a set of high probability segmentations from the
underlying distribution that are not just minor perturbations of each other. Critically
the output of this stage is a diverse set of plausible solutions and not just a single one.
The first stage reduces the exponential space of solutions to just a handful of segmen-
tations. In the second stage, a discriminatively trained re-ranking model selects the
best segmentation from this set. The re-ranking stage can use much more complex fea-
tures than what could be tractably used in the segmentation model, allowing a better
exploration of the solution space than possible by simply producing the most probable
solution from the segmentation model. The formulation of the first stage is agnostic to
the underlying segmentation model (e.g. CRF, CNN, etc.) and optimization algorithm,
which makes it applicable to a wide range of models and inference methods.
Evaluation of the approach on a number of semantic image segmentation benchmark
datasets highlight its superiority over inferring the MAP solution.
ix

P U B L I C AT I O N S
The ideas and figures in this thesis have appeared previously in the following publica-
tions:
[1] Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich.
“Diverse m-best solutions in markov random fields.” In: European Conference on
Computer Vision. Springer. 2012, pp. 1–16.
[2] Payman Yadollahpour, Dhruv Batra, and Gregory Shakhnarovich. “Discrimina-
tive re-ranking of diverse segmentations.” In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. 2013, pp. 1923–1930.
[3] Payman Yadollahpour and Gregory Shakhnarovich. “Region ranking for figure-
ground segmentation.” 2014.
xi

A C K N O W L E D G M E N T S
My deepest gratitude goes to my research advisor Dr. Gregory Shakhnarovich for con-
tinual guidance and stewardship of my graduate career. He has been a major source
of insight and has significantly contributed to my understanding of Computer Vision
and Machine Learning disciplines. I would also like to thank Dr. Dhruv Batra for his
research collaboration, and much of this thesis owes to the joint collaboration I had
with him and Greg. A big thanks to Dr. Ayan Chakrabarti for the many insightful
discussions on my thesis.
I would like to mention my appreciation to the entire faculty at Toyota Technological
Institute at Chicago for their insistence on research excellence and for making TTIC a
leading graduate research institution.
I’d like to give special thanks to my fellow students at Toyota Technological Institute at
Chicago, for making the many years enjoyable and the many fruitful discussions. I’d
like to especially thank Avleen Bijral, Andrew Cotter, Heejin Choi, Somaye Hashemi-
far, Taehwan Kim, Gustav Larsson, Mohammadreza Mostajabi, Jian Peng, Karthik
Sridharan, Siqi Sun, Hao Tang, Behnam Tavakoli, Shubhendu Trivedi, Zhiyong Wang,
and Feng Zhao. I also owe gratitude to Steven Basart, Falcon Dai, Suriya Gunasekar,
Nicholas Kolkin, and Mohammadreza Mostajabi, for giving me very useful feedback
on my thesis draft.
Lastly, I would like to thank my family for being a source of support and for their
patience these many years.
xiii

C O N T E N T S
1 introduction 1
1.1 Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.1.1 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.2 MAP Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2 divmbest 37
2.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
2.1.1 M-Best . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
2.1.2 M-Best and Max-Flow Propagation . . . . . . . . . . . . . . . . . 41
2.1.3 M-Best solutions for loopy graphs and the BMMF algorithm . . 46
2.1.4 M-Best MAP and its linear programming formulation . . . . . . 48
2.1.5 M-Best MAP LP when G is a tree . . . . . . . . . . . . . . . . . . 49
2.1.6 M-Best MAP LP when G is a general graph . . . . . . . . . . . . 49
2.2 DivMBest Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
2.2.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.2.3 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.2.4 MAP problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.2.5 MAP integer program and its LP relaxation . . . . . . . . . . . . 55
2.2.6 DivMBest: Formulation . . . . . . . . . . . . . . . . . . . . . . . . 56
2.2.7 DivMBest: Lagrangian Relaxation and the Lagrangian dual func-
tion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
2.2.8 Diversity Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
2.2.9 Supergradient Ascent on Lagrangian dual function . . . . . . . . 60
2.2.10 How tight is the Lagrange relaxation? . . . . . . . . . . . . . . . . 61
2.2.11 Computing Supergradient under different diversity functions . . 63
2.2.12 Dual-Decompostition and the approximate supergradient for higher
order potentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
2.2.13 Setting k: the amount of diversity . . . . . . . . . . . . . . . . . . 65
2.2.14 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3 divmbest+rerank 67
3.0.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.1.1 Relation to cascade approaches . . . . . . . . . . . . . . . . . . . . 70
3.1.2 Relation to proposal-generation methods . . . . . . . . . . . . . . 72
3.1.3 Discriminative re-ranking in other domains. . . . . . . . . . . . . 73
3.2 DivMBest + Re-rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3.2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3.2.2 Re-ranker model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
xv
xvi contents
3.2.3 Re-ranker loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
3.2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
4 divmbest experiments 81
4.1 Evaluating DivMBest segmentations . . . . . . . . . . . . . . . . . . . . . 81
4.1.1 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
4.1.2 Oracle Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.2 Interactive Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.2.1 CRF Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.2.2 Interactive segmentation + DivMBest . . . . . . . . . . . . . . . . 84
4.2.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
4.3 Figure-ground Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.3.1 CRF Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.3.2 CRF potentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.3.3 Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.3.4 CRF learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
4.3.5 Task loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
4.3.6 Loss-augmented inference . . . . . . . . . . . . . . . . . . . . . . . 92
4.3.7 DivMBest inference with Hamming dissimilarity . . . . . . . . . 93
4.3.8 Superpixels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
4.3.9 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
4.4 Multi-category segmentation . . . . . . . . . . . . . . . . . . . . . . . . . 96
4.4.1 Hierarchical model . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
4.4.2 Feed-forward model . . . . . . . . . . . . . . . . . . . . . . . . . . 98
4.4.3 Convolutional neural network + dense CRF model . . . . . . . . 99
4.4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5 divmbest+rerank experiments 107
5.1 Evaluating DivMBest+ReRerank pipeline . . . . . . . . . . . . . . . . . . 107
5.2 Figure-ground Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.2.1 Re-ranking segmentations . . . . . . . . . . . . . . . . . . . . . . . 107
5.2.2 Ranking features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
5.2.3 Re-ranker training . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
5.2.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
5.3 Multi-category segmentation . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.3.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.3.2 Hierarchical model . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.3.3 Feed-forward model . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.3.4 Diversity and Oracles . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.3.5 Re-ranker features . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.3.6 Re-ranker training . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.3.7 Re-ranker results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.3.8 Re-ranker Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
5.3.9 Human Ranking Experiments . . . . . . . . . . . . . . . . . . . . 118
5.3.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
contents xvii
6 conclusion 123
bibliography 125
i appendix 137
a appendix a 139
a.1 Sample of DivMBest solutions from figure-ground model . . . . . . . . 139
a.2 Sample of results when DivMBest is applied to muli-category segmen-
tation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
b appendix b 141
b.1 Example Re-ranking Results . . . . . . . . . . . . . . . . . . . . . . . . . . 141
b.2 Highest ranked vs. MAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
L I S T O F F I G U R E S
Figure 1.1 Bottom-up segmentation using normalized k-way cut algorithm
as a function of the number of segments (top eigenvectors), k.
Note that a segment can consists of multiple disconnected com-
ponents (e.g. segment on nose of horse). . . . . . . . . . . . . . . 12
Figure 1.2 SLIC superpixel results as a function of the desired number,
k, and compactness, m. As m is increased the superpixels ex-
hibit more regular appearance, aligning less with image con-
tours and more with spatial grid. . . . . . . . . . . . . . . . . . . 15
Figure 1.3 For a given superpixel (red) zoom-out features are computed
at multiple zoom-out levels (6 levels shown). The features from
each level are stacked into a column vector representation of the
superpixel, and a multi-layer perceptron is used to predict the
superpixel class probabilities. . . . . . . . . . . . . . . . . . . . . 24
Figure 1.4 Examples of zoom-out regions. We show four out of fifteen
levels: 1(cyan, nearly matching the superpixel boundaries), 6
(olive), 10 (purple) and 13 (blue). . . . . . . . . . . . . . . . . . . 26
Figure 1.5 Zoom-out network architecture using an image classification
CNN backbone, computed over superpixels. The output response
from the convolutional layers plus the softmax output (scene
level) are up-sampled to the image size and stacked into the fi-
nal feature map representation over the image. For each super-
pixel a feature vector representation is constructed by pooling
the feature map over the superpixel. . . . . . . . . . . . . . . . . 27
Figure 1.6 Example semantic segmentation on VOC2012 val images using
a 3-layer perceptron classifier used to classify zoom-out features
over superpixels across 15 zoom-out levels of a CNN originally
trained for scene classification. . . . . . . . . . . . . . . . . . . . 28
Figure 2.1 Semantic segmentations on test images from PASCAL VOC 2010. For
each image, from left: input image, MAP segmentation, best out of 10
modes obtained with DivMBest. . . . . . . . . . . . . . . . . . . . . 53
Figure 2.2 Interactive segmentations. For each image, from left: input image,
MAP solution, 2nd best MAP, and the 2nd best mode obtained with
DivMBest. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
xviii
List of Figures xix
Figure 3.1 An overview of the DivMBest+ReRank approach. In Stage 1 di-
verse segmentations are computed from a tractable probabilis-
tic model. These are fed to a large-margin re-ranker in Stage 2.
The top re-ranked segmentation is returned as the final solu-
tion. Even though the most probable segmentation from Stage
1 is incorrect, the set of segmentations does contain an accurate
solution, which the re-ranker is able to score to the top. . . . . . 68
Figure 4.1 DivMBest modes under cardinality-based HOP. From left-to-right:
image-scribble pair (X, S), MAP solution, 2nd-mode,. . . ,6th-mode.
The modes are ordered in increasing size of foreground object. 86
Figure 4.2 Examples of (left to right) input image with ground truth, MAP
from the bottom-up CRF model, oracle out of 10 diverse solu-
tions. All examples are from the test portions of Graz data sets. 102
Figure 4.3 Examples of (left to right) input image with ground truth, MAP
from the bottom-up CRF model, oracle out of 10 diverse solu-
tions. Examples are from the test portion of Weizmann horses
data set, and from one of the test folds of the Ultrasound data
set. Last row shows some failures. . . . . . . . . . . . . . . . . . 103
Figure 4.4 (a) Oracle accuracy vs. number solutions on VOC2010 val for DivMBest
(red) and confidence based perturbations (blue), along with MAP per-
formance (black dashed). (b) Mean hamming distances between each
mode (DivMBest solution) and the MAP solution (red), and average
to previous modes (blue), normalized by image size on PASCAL VOC
2010 val set. Also show, histogram of energies (as % of MAP) over
(c) 6 modes, (d) 31 modes, on validation set. The bar to the left of red
vertical lines indicate number of modes with energy less than or equal
to MAP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
Figure 4.5 Oracle performance (IoU accuracy against ground-truth) on PASCAL
VOC 2012 val, when (a) selecting best-out-of-m solutions, and (b)
composing full image labellings from connected components found
among m solutions. . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
Figure 4.6 Result of composing solutions from DivMBest segments. Second and
third row show a subset of 40 DivMBest segmentations generated
from the CNN+CRF model of § 4.4.3. First row shows in order the
image, ground-truth segmentation, and composed segmentation ora-
cle using the second approach of § 4.4.3.2. Note how the composed
segmentation oracle is a much better segmentation of the image than
the MAP solution (first segmentation in the second row). . . . . . . . 105
Figure 5.1 DivMBest+ReRank performance on PASCAL VOC 2012 val using (a)
ALE and (b) O2P models vs. the number of solutions. . . . . . . . . 112
Figure 5.2 (a) Average minimum-covering (5.3) of MAP in the first j 6 10 solu-
tions vs. j. (b) Accuracy of an oracle restricted to labels present in the
MAP, or (c) restricted to masks present in MAP. See text for details. . 115
Figure 5.3 Statistics on PASCAL VOC 2012 val with O2P model: (a),(b)
show the number of images in which the oracle / top-re-ranked
solution was originally at rank j 6 10. We can see that there is
a heavy tail in the oracle distribution, but a much lighter tail
in the re-ranker, suggesting that the re-ranker “plays it safe”
and predicts MAP very frequently; (c) shows a scatter plot of
re-ranker score vs solution accuracy. . . . . . . . . . . . . . . . . 120
Figure 5.4 Cases whereO2P-DivMBest+ReRank outperformsO2P-MAP. In each
group of images, the first column shows the original image followed
by the ground-truth, MAP, and top re-ranked solution returned by
DivMBest+ReRank. PASCAL intersection-over-union accuracy is shown
below the segmentations. . . . . . . . . . . . . . . . . . . . . . . . . 121
Figure 5.5 Example MTurk tasks along with user-provided responses which were
instructive in the creation of segmentation-specific features. . . . . . 122
L I S T O F TA B L E S
Table 4.1 Superpixel features used to learn the appearance model for the
interactive segmentation figure-ground cutout model. . . . . . . 83
Table 4.2 Interactive segmentation: pixel accuracies averaged over 50 test images. 87
Table 4.3 Segmentation performance on all data sets, in IoU values ×100.
MAP: single solution from the bottom-up CRF model. Oracle:
(hindsight) best of 10 diverse solutions from the CRF. Third col-
umn: percentage of gap (oracle-MAP) recovered by the ranking.
Last column: Figure-ground segmentation model of Kuettel et
al. [66]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Table 4.4 Pascal VOC 2010 val set accuracies for ALE model. . . . . . . . 97
Table 4.5 Pascal VOC 2010 test set accuracies for ALE model. . . . . . . 97
Table 5.1 Segmentation performance on all data sets, in IoU values ×100.
MAP: single solution from the bottom-up CRF model. Oracle:
(hindsight) best of 10 diverse solutions from the CRF. Ranking:
full ranking model (all features). last column: percentage of gap
(oracle-map) recovered by the ranking. . . . . . . . . . . . . . . 109
Table 5.2 Comparative results between methods and feature sets for re-
gion ranking. All numbers are IoU×100. Shape: only shape and
position. Textons: only textons. Color: only color histograms.
full-entropy: shape, color and textons, but not their entropies. . 109
xx
List of Tables xxi
Table 5.3 Average covering score between oracle solutions and MAP: (left) show
the category-independent measure and (right) shows the category-
specific measure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
Table 5.4 PASCAL VOC 2012 test set accuracies. . . . . . . . . . . . . . . 116
Table 5.5 (left) Human accuracy in predicting (B)est-vs-(W)orst, (M)AP-vs-(W)orst,
and (B)est-vs-(M)AP solutions. (right) Pascal VOC accuracies over 150
images for best, MAP, worst, and human response (HR) solutions. . . 118

1
I N T R O D U C T I O N
The task of automatically labeling every pixel in an image with the category label of
the object it covers is an important computer vision problem. Known as full image
labelling or semantic segmentation – because it partitions the image into semantically
coherent regions – it is one valuable proxy for measuring how well a system can rea-
son about what is being depicted in an image. It subsumes important aspects of scene
understanding such as image classification and object localization. While its impor-
tance as an end task is debatable it is a more refined proxy for measuring a system’s
discriminative capability on a finite set of object classes than image classification or
object detection. This is because the prediction must be made over local regions in the
image as opposed to a global prediction over the entire image or simple bounding
boxes over objects.
Image segmentation is typically modelled either probabilistically, via Conditional or
Markov Random Fields (CRFs/MRFs) or using discriminative feed forward approaches.
Feed forward approaches include cascade type systems that first predict region pro-
posals and then predict their most likely labels and heuristically paste the labelled
regions into the image. More recently, neural network models for segmentation have
been proposed, including Convolutional Neural Networks (CNNs) [42, 82] and Recur-
sive Neural Networks [43], which achieve state-of-the-art accuracy on many difficult
image segmentation benchmarks.
Semantic segmentation is a task that has a structured output space; the variables of
interest (namely image regions such as pixels or superpixels) are not independent of
each other, but rather must be predicted jointly. Given this fact and that the output
space of possible labellings of the variables is exponential in size introduces certain
limitations on how we can jointly model, train, and infer the variables. For instance,
in order to be able to train and run inference, CRF or MRF models often make simpli-
fying independence assumptions over the variables, either by limiting clique sizes, or
approximating the partition function. The different sources of error – approximation
error due to a poor choice of model class, optimization error due to limitations on
optimizing over the variables of interest, and estimation error due to a finite training
set – all contribute to the quality of the final predicted segmentation. Because of all
these sources of error the predicted probability distribution over the output labeling
might be significantly different from the true distribution. Thus the most probable la-
bel returned under the model distribution might not be the most probable under the
true distribution.
1
2 introduction
One way to alleviate this is to build more complex models that can capture the com-
plex interactions of the variables, at the cost of making learning and inference (i.e.
optimization) more expensive or possibly intractable. In this thesis we explore an al-
ternate approach. Instead of increasing model complexity at the cost of optimization
complexity, we propose a framework whereby we can find a small set of highly prob-
able and yet diverse segmentations (“modes”) under the model. By virtue of the fact
that this “mode” finding algorithm has exponentially reduced the space of segmen-
tations we need to consider, we can evaluate each of them using arbitrarily complex
features that can take into account dependencies between variables that would be in-
tractable to capture in the original model. Because of the exponential space of possible
segmentations, producing this “handful” of highly probable yet diverse segmentations
is going to require an approach that is more nuanced than simply enumerating all pos-
sible solutions under the model.
We show that combining this mode finding algorithm with an automatic approach
to selecting the best segmentation from this smaller set leads to a framework that
produces state-of-the-art results on challenging semantic segmentation datasets. It is
also general enough to be applicable to a wide variety of problems in vision and
elsewhere.
thesis outline This chapter presents a review of the segmentation problem, and
outlines some common approaches to it, citing related literature – specifically algo-
rithms for bottom-up and top-down segmentation. The chapter closes with presentation
of the well known MAP inference problem and its integer programming formulation
which will become relevant in the formulation of the DivMBest problem. Chapter 2
reviews a number of approaches for inferring multiple solutions from a discrete prob-
abilistic model, instead of just the MAP solution and explains why they are not ade-
quate for improving image segmentation. Chapter 2 concludes with presentation of an
alternate approach called the DivMBest problem — which leverages existing segmen-
tation models, and algorithms used to do inference over them, in order to produce sets
of high-quality segmentations that are diverse. Chapter 3 presents a discriminatively
trained re-ranking model that selects the best segmentation from this set. Evaluation
of the DivMBest and DivMBest+ReRank methods on a number of semantic segmen-
tation tasks is presented in chapter 4 and chapter 5 respectively.
1.1 segmentation
The task of partitioning all or some of the pixels in an image into coherent regions
is known as image segmentation. When the regions take on semantic labels, the par-
titioning is known as semantic segmentation — a major topic of this thesis. The non-
semenatic segmentation problem is ill-posed because what we mean by a segment is
1.1 segmentation 3
not clearly defined — for example a segment might belong to a single or multiple con-
nected components throughout the image. A primary goal of segmentation is to have
pixels within segments share a consistent property or feature. This is another reason
why segmentation is ill-posed because consistent property is problem specific. For ex-
ample, a common property we find in the output of most segmentation algorithms is
that pixels that fall within the same segment are all within a local spatial neighborhood
in the image. This is property is not necessarily required however. Other features that
do not require it such as color and texture statistics of the regions around a pixel [24,
27], pixel depth information [40], image contour strength [3], can be considered de-
pending on the segmentation task. A third reason, specific to non-semantic segmen-
tation, is that we do not explicitly associate meaning with the individual segments.
The segments could correspond to low-level image cues like regions of constant color
or texture or could be associated with semantic meanings such as physical objects or
parts of objects. Given an image if you were to ask a set of people to segment the image
we would end up getting multiple interpretations of what is a good segmentation of
that image.
Semantic segmentation, however, is a much better posed problem since the output
label space is well defined (e.g. object classes). That is to say, one common semantic
segmentation task that we care to define is labelling every image region (e.g. pixel or
superpixel) with the approriate object class that it is a part of in the image.
As the above examples of segmentation features illustrate, one axis along which we
can define different segmentation algorithms is based on the features used to capture
local information relative to pixels in the image. If the segmentation task is to parti-
tion the full image into spatially coherent segments where pixels within a segment
have similar color, spatial, depth, or boundary statistics such as curvature — this is
known as low-level, or bottom-up, image segmentation. On the other hand, in the case
of semantic segmentation (also known as semantic image parsing), the pixels corre-
sponding to a segment share similar semantic properties (such as a pixel part of sky in
image). Additionally, if the semantic categories are limited to foreground objects and
background clutter the partitioning is referred to as figure-ground, or simply, fore-
ground segmentation. Segmentation can also be with respect to 3D cues of the objects,
such as surface orientation or material properties [44].
Typically bottom-up image segmentation algorithms partition the image into disjoint
segments. The union of segments is equal to the entire image; in other words the seg-
mentation covers the entire image. In semantic segmentation whether the partitioning
covers the entire image depends on the semantic categories considered and how the
algorithm partitions the image. For instance, the algorithm could assign the area in
the image not covered by the segments explicitly labeled with semantic categories to a
catch-all category such as background, don’t care, or unknown label. On the other hand
the algorithm might explicitly try to predict ambiguous segments in the image as a
4 introduction
specific category onto itself such as stuff, in which case the partitioning might not cover
the entire image.
The size and shape statistics of the segment that we get as the output from segmen-
tation algorithms also differs depending on the image information used for segmen-
tation, as well as the algorithm details itself. For example in semantic segmentation
the desired segment shapes and sizes are governed by shapes and sizes of the objects
depicted in the images. On the other hand the output of low-level segmentation al-
gorithms such as SLIC [1] produce over-segmentations of the image, where segments
exhibit nearly uniform shape and size with small spatial support. Usually, low-level
segmentations that over-segment an image are a first step towards some other more
complex downstream task, such as semantic segmentation. These segments provide
convenient and predictable objects for downstream processing due to there consistent
shape and size. That is not to say that the output of all low-level algorithms exhibit
this regularity in size and shape. For example hierarchical image segmentation ap-
proaches [100], which do a bottom-up grouping of image regions, and segmentation
based image contour detection [3] produce low level image segmentation results where
segments can have a variety of shapes and sizes.
It is common to refer to segments that are the result of low-level image segmentation
algorithms as superpixels. Analogously for 3-dimensional segmentation the 3D regions
are referred to as supervoxels. Generally what is refered to as a superpixel is the re-
sult of an over segmentation of an image, and initially there is no semantic meaning
associated with the superpixel. Most superpixel algorithms rely on low-level image
evidence such as color, intensity, contour, and texture information. Many but not all
of these algorithms produce superpixels with regular shape and size, that adhere to
image contours and some additionally, roughly, snap to a regular grid pattern over
the image. Image contour can be further separated into internal and external edges.
By internal edges we mean contours that appear due to a marked difference in inten-
sity, color, or texture between pixels that fall on the same object surface in the image,
whereas external edges are those delineating locations where one object occludes an-
other or of self occlusions. Many of the low-level algorithms produce superpixels that
align to both types of contours. Indeed a single superpixel boundary can align with
one or more internal and external edges. This is in contrast to the desired output
from semantic segmentation algorithms where the segment boundaries should align
to object-to-object or object-to-background boundaries.
As mentioned earlier it is common for semantic segmentation approaches to rely on
superpixels, generated using low-level segmentation algorithms, as the basic primi-
tives over which to construct larger segments. This isn’t always the case however – in
fact there are semantic segmentation methods [16, 17] that use complete or partially
complete object proposals (i.e. segments) as their basic primitives, and these segments
do tend to align better to external edges in the image. The figure-ground models used
to generate the proposals are typically learned by maximizing an objectness score,
1.1 segmentation 5
thereby generating segments that better correspond to objects of interest (i.e. figure)
than to everything else (i.e. background).
As previously mentioned the segments that semantic segmentation methods generate
can also span multiple connected components (in graph parlance). For example if ob-
ject A is partially occluded by object B visually splitting A into two parts in the image,
and the two objects are of different categories, then the correct segmentation compo-
nent associated with object A is composed of two separate connected components. On
the other hand if both A and B have the same object category then the correct semantic
segmentation output would be a single component tightly covering A and B. Further-
more, if the task is instance level semantic segmentation, and A and B appear adjacent
to each other in the image but are of the same category label, then the correct output
should be two separate connected components each tightly covering one of the objects
and each assigned a unique instance label.
For downstream computer vision tasks low-level image segmentation, and requisite su-
perpixel output, provides a nice way to improve computational efficiency. Compared
to working with pixels which number from tens of thousands to millions in typical
images, superpixels tend to number in the dozens or hundreds. That’s a few orders
of magnitude reduction in the number of variables that need to be considered by a
semantic segmentation algorithm. Since superpixels are the results of algorithms de-
signed to align closely with significant image contours, they provide the added benefit
of combining to produce segments that also align well to significant contours along
their boundary. A third reason for using superpixels instead of pixels is that, in con-
trast, superpixels provide a boundary aligned spatial support on which to compute
image features. The segment on which we should compute features for a pixel is less
well defined, and usually local features [24, 76, 86] are computed on a spatial neigh-
borhood around the pixel that is grid aligned. In conjunction, the fact that superpixels
can span many pixels and cover a large pixel neighborhood, including neighbors that
are more than one pixel apart, provides useful long-range dependencies between areas
in the image. As we will elaborate on later in this chapter, these long-range dependen-
cies allow short-range dependency (i.e. dependency between adjacent elements in a
neighborhood) graph based semantic segmentation algorithms to incorporate implicit
long-range information for local prediction of superpixel labels — producing segmen-
tations that are more consistent with the image — at the same time bypassing the
complexities involved with incorporating explicit long-range edges in the graph.
So far we have talked about a few different segmentation tasks: low-level, semantic,
instance level, and figure-ground segmentation. This is by no means an exhaustive
list of segmentation tasks. Some other common segmentation tasks that we’ll mention
here include interactive segmentation, cosegmentation, object-proposals, and holistic
scene segmentation.
6 introduction
Interactive segmentation is an approach where the user is in the loop. In this task the
goal is to have a system that, given an image, asks the user to input exemplars for
the types of regions that the user would like the system to segment. The exemplars
could be pixels, superpixels, or other regions, in the image and the user interacts
with the system via scribbles [12], bounding boxes [93], or polygons, etc on the image
indicating the regions corresponding to different categories they’d like to segment.
Given the user annotation an initial segmentation of the image (be it a multi-category,
figure-ground, or low-level segmentation) is produced by the system and offered to the
user. Depending on the quality of the segmentation the user has the option to refine
or provide more annotations as before and have the system refine the segmentation.
This iterative process continues until the user is happy with the segmentation at which
point the process terminates.
In cosegmentation [6, 94, 102] the task, usually, is to jointly segment different instances
of the same object category that appear in a set of images. Alternately the images
could contain the same object instance under different views or deformations. If the
task is to segment frames in video sequences this approach to segmentation has clear
advantages because it leverages more information in learning the segmentation model
for object categories and instances.
A major advance in semantic segmentation came with the use of object proposals [16,
27]. The idea here is to produce multiple object proposals for the image. Each object
proposal is either a figurie-ground segment or bounding box in the image, and the
proposals are allowed to overlap. The semantic segmentation task shifts from labeling
pixels/superpixels in the image to selecting a subset of the top ranking object pro-
posals and assigning them semantic labels. Using object proposals makes the problem
much simpler because the set of object proposals is much smaller than the number
of pixels/superpixels in the image. Object proposals also provide much larger spatial
support for computing features useful in determining objectness likelihood of the un-
derlying image region. It’s also more likely that one of the object proposals is a good
candidate segment for an object. The object proposals are usually generated using
class-independent methods. For example the object proposals can be bottom-up seg-
mentations computed over the image which are ranked according to an objectness score
that takes into account cues like color, texture, location, saliency, etc [27]. Alternatively,
a bottom-up approach can be taken to produce multiple figure-ground masks using a
graph-based model initialized with different random seeds [16]. The masks are then
ranked according to class specific regressors trained to maximizes the likelihood that
the mask tightly covers the underlying object of that category. One of the most suc-
cessful approaches for building object proposals that is very fast and gives high recall
on objects present in the image is Selective Search [100]. Here multiple hierarchical
segmentations over superpixels are computed. The object proposals consist of either
segments within this hierarchy or bounding boxes around them.
1.1 segmentation 7
Finally, there are approaches [128] that try to reason about multiple tasks over the im-
age in order to come up with a holistic interpretation of what is being depicted. The
task, then, becomes to jointly reason about both the segmentation of the image into
semantically meaningful segments while simultaneously predicting the scene classi-
fication and detecting what objects are in the image along with their locations and
extents. Allowing for joint prediction of multiple tasks has the added benefit of in-
corporating multiple compatibility measures. Each of these compatibility features is
an added source of rich information that the model can use in order to improve the
segmentation accuracy.
In the next section we’ll dive a little deeper into some of the most popular bottom-
up and top-down segmentation methods and explain in more detail how they work.
We’ll also describe the specific segmentation models we used in the experiments of
subsequent chapters.
1.1.1 Methods
Segmentation has a long and rich history and we will not try to enumerate all the
different segmentation methods. Instead we’ll highlight a few of the most popular
methods for both low-level and semantic segmentation.
One class of segmentation methods is based on algorithms that try to find the max-
ima, or modes, of a data distribution given a discrete set of points. They are clustering
methods because they assign all points within a basin of attraction of a mode to the
same cluster. Another nice property is that these methods are non-parametric. That
means that the space of data points can be viewed as the empirical probability density
function of the parameter the data points represent. The modes of the density func-
tion will correspond to dense regions, or clusters, in the data space. That’s why these
methods are also referred to as hill-climbing or gradient based methods because they
find the maxima of the data distribution. This is nice because we don’t need to known
apriori the number and shape of the clusters.
1.1.1.1 Mean Shift
One of the most popular such methods is based on the mean shift algorithm [23].
In the mean shift algorithm the unknown data density is estimated using the kernel
density estimator,
f?(x) =
1
nhd
n?
i=1
K
(
x? xi
h
)
, (1.1)
8 introduction
where x, xi are d-dimensional data points, and K here is assumed to be a multivariate
normal kernel with diagonal bandwidth matrix h2I for simplicity (h is the bandwidth
parameter), though any radially symmetric kernel that satisfies some mild assump-
tions would suffice. The modes of the density are locations where ??f(x) = 0. The
density gradient when assuming normal kernel K is,
??f(x) = 1
nhd
n?
i=1
K
(
x? xi
h
) (?n
i=1 xiK
(
x?xi
h
)?n
i=1 K
(
x?xi
h
) ? x) . (1.2)
The second term is the mean shift,
m(x) =
?n
i=1 xiK
(
x?xi
h
)?n
i=1 K
(
x?xi
h
) ? x, (1.3)
which is the difference between the weighted average of the points and x. It can be
shown [20, 23, 35] that the mean shift is proportional to,
m(x) ? ??f(x)
f?(x)
, (1.4)
or in other words the mean shift points along the direction of steepest ascent of the
empirical density at point x. This is a nice property because it provides a natural
algorithm for mode finding:
• Start at a data point x,
• Repeat the following steps till convergence (i.e. m(x) ? 0):
– Compute the mean shift vector m(x) (1.3),
– Update the location of the kernel window using m(x),
The fact that the mean shift is normalized by the density makes the mean shift al-
gorithm an adaptive gradient ascent algorithm that takes large steps in areas of low
density and takes increasingly smaller steps as it approaches high density areas where
the modes are.
All points that converge to the same stationary point are within the basin of attraction
of a mode. These points can all be considered as one cluster and assigned the same
cluster label.
The mean shift algorithm has been applied to the task of image segmentation [23]
where, normally, the xi — i = 1, . . . ,n, where n is the number of pixels in an image
— are d-dimensional feature vectors containing the pixel location and LAB (or LUV)
color or intensity information. Typically d is small because mean shift suffers from the
curse of dimensionality. A higher dimensional space will be sparsely populated with
data points and the density concentrated in a very small part of the space making the
1.1 segmentation 9
kernel density estimator a poor estimate of the true density. Another issue with mean
shift is that the feature space is assumed to be a Euclidean space, or some other space
where an inner product or Riemannian metric is defined, which might not generally
hold. The mean shift algorithm is also pretty slow with O(n2) running time.
1.1.1.2 Quick Shift
An alternative, simpler, strategy for mode seeking is quick shift [116]. Whereas in mean
shift we had an iterative algorithm and had to compute the gradient, in quick shift we
only need to take one step for each data point and no gradients are needed. In quick
shift each data point is moved to the location of a neighboring point that increases the
probability density. We can write the probability density estimate at xj as,
f?(xj) =
c
n
n?
l=1
k
(
d(xl, xj)
)
, (1.5)
where k is some radially symmetric kernel function, d(xi, xj) is a metric on x, and c is
a normalization constant. For data point xi we assign it the data point yi such that,
yi ? argmin
j:f?(xj)>f?(xi)
d(xi, xj), (1.6)
guaranteeing that we move up the hill toward a mode. Doing this procedure for every
point xi connects all the points into a tree, with edge weights set to d(xi, xj). Cutting
edges with weight larger than some threshold t breaks the tree into subtrees that
cluster the points with the root nodes as possible modes of the empirical distribution.
Adjusting the threshold controls how much fragmentation of the modes there is which
affects the number of segments you get. The method is still rather slow with O(dn2)
complexity, where d is a small constant.
1.1.1.3 Watershed Transform
Another approach to segmentation is based on the watershed transform. There are
a number of different watershed transforms, such as watershed by immersion or by
topographical distance [91]. The basic idea is simple though, you can view an intensity
or grey level image as a landscape with catchment basins or wells in the topography and
watersheds where multiple basins meet. The general approach is to start off by assigning
local minima in the intensity or grey level image to distinct basins. In the watershed by
immersion approach (cf. [91]) the basins are recursively grown by iteratively increasing
the level set and assigning unlabeled pixels that have intensity value no greater than
the level set value to the catchment basin that is closest. If the pixel is equidistant to two
or more catchment basins then it is not assigned to any basin and is reconsidered in
the next iteration. The process continues until all level sets (i.e. image intensity values)
10 introduction
have been considered, at which point all unlabeled pixels are assigned as watershed
(i.e. boundary). The basins are the resultant segmentation of the image.
The watershed transform by topographical distance approach (cf. [91]) assumes a cost
between neighboring pixels p and q that takes into account the slope between p and q.
The topographical distance along a path is defined as the sum of costs between neigh-
boring pixels along the path. The topographical distance between two points is then
just the minimum topographical distance of any path connecting them. A catchment
basin around a local minimum is then defined as all the pixels that are closer to that
minimum in terms of topographical distance than to any other local minimum in the
image. The watershed boundaries are the set difference of the image with the pixels in
all the catchment basins.
1.1.1.4 Graph Based - Normalized Cuts
There are also graph based approaches for low-level segmentation that let each pixel
be a node in a graph, with some edge connectivity between pixels, and partition the
graph to produce a segmentation of disjoint components. Of these there is a subtype
of algorithms that are based on spectral partitioning of the graph — normalized cuts
being one such method. In the normalized cuts algorithm [104] we assume a graph
G = (V ,E) where each vertex in V is associated with unique pixel in the image (vertices
in V cover the image) and E contains edges between all pairs of pixels. For each edge
(vi, vj) ? E we assign a weight wij capturing how likely it is that vi and vj belong
to the same object in the image. Usually wij is a similarity measure between feature
vectors computed at i and j. A cut of G given two disjoint components C1 and C2 is
defined as,
cut(C1,C2) =
?
i?C1,j?C2
wij. (1.7)
The minimum cut of G, i.e. the subset of E that minimizes the total edge weight cross-
ing the cut, is the optimal partition of the image into two components. Using the total
edge weight crossing the cut is not ideal for segmentation because it tends to favor par-
titioning small components since the cut value grows as the number of edges across
the bipartition grows. To account for this in normalized cuts they use the normalized
cut (cf.[104]),
ncut(C1,C2) =
cut(C1,C2)
assoc(C1,V)
+
cut(C1,C2)
assoc(C2,V)
, (1.8)
where assoc(Ci,V) =
?
i?Ci,j?V wij is the total weight of edges from pixels in Ci to
all pixels in the graph. If C1 is small, assoc(C1,V) will tend to also be small, increasing
the cut value, ncut(C1,C2). Consequently it prevents cuts that favor producing small
components.
1.1 segmentation 11
It turns out that computing a bipartition of the graph into C1 and C2 that minimizes 1.8
amounts to solving the following generalized eigenvalue problem
(D?W)y = ?Dy, (1.9)
where D is an n×n diagonal matrix with Dii =
?
jwij along the diagonal, and W is
an n×n symmetric matrix with Wij = wij. It can be shown [104] that the eigenvector
y corresponding to the second smallest eigenvalue of 1.9 gives the assignment that
partitions G into components C1 and C2 that minimize the normalized cut. The eigen-
vector y ideally will have two discrete points +1,?1 indicating whether or not a pixel
i is assigned to component C1. But in order to solve 1.9, y is relaxed to take on real
values. Therefore the final assignment can be done by either using 0 as the threshold
on the values of y — where all elements with value > 0 are assigned to C1 and C2
otherwise — or we can search over different thresholds and pick the partitioning the
minimizes the ncut.
This leads to a simple normalized cut algorithm for segmenting the graph called two-
way ncut [104],
• Construct a fully connected graph G = (V ,E) over pixels in the image, with edge
weights wij measuring similarity between pairs of pixels.
• Solve the generalized eigenvalue problem (D ?W)y = ?Dy for eigenvectors
with smallest eigenvalues.
• Bipartition the graph using the eigenvector corresponding to the second largest
eigenvalue. If y contains more than two discrete values, search for the splitting
point that gives the minimum ncut value.
• For each component created after the bipartition we can decide whether to re-
cursively apply the same procedure again to partition the component into two
separate components based on the ncut value.
• Stop partitioning when the ncut value is below a certain threshold.
The resulting segmentations have some nice properties. Because the cut is over a fully
connected graph the solutions take global image information in producing the segmen-
tation. The components in the segmentation also need not be connected components
in the graph — a component can consist of pixels found in multiple disjoint regions
in the image. The normalized cut algorithm is relatively slow with a running time
complexity of O(n3/2), where n is the number of pixels in the image.
Results of the normalized cuts algorithm are shown in figure 1.1. These are base on the
alternate k-way cut algorithm (cf. [104]) that produces a simultaneous segmentations
into k regions. In the k-way cut approach they associate an n-dimensional vector with
each pixel in the image by stacking the top n eigenvectors. An over-segmentation of
12 introduction
(a) k = 5 (b) k = 15 (c) k = 30 (d) k = 45
Figure 1.1: Bottom-up segmentation using normalized k-way cut algorithm as a function of
the number of segments (top eigenvectors), k. Note that a segment can consists of
multiple disconnected components (e.g. segment on nose of horse).
p > k segments is produced by clustering the n-dimensional vectors using k-means.
Next, either a greedy merging strategy can be used or a global recursive cut is per-
formed. In the greedy merging approach regions are merged iteratively until k seg-
ments are left. In each iteration the two regions that minimize the following k-way
normalized cut are merged,
Ncutk =
cut(A1,V ?A1)
assoc(A1,V)
+
cut(A2,V ?A2)
assoc(A2,V)
+ · · ·+ cut(Ak,V ?Ak)
assoc(Ak,V)
(1.10)
where the Ai’s are the segments. Alternatively, a new graph can be constructed with
the Ai’s as nodes and edge weights, wij, corresponding to assoc(Ai,Aj), capturing the
total weight between pixels in Ai and Aj. Recursively bipartitioning this new graph —
by either solving the eigensystem defined earlier for the ncut criterion or exhaustively
minimizing the ncut criterion when k is small results in the final k-way partition. For
further details of the approach, refer to Shi and Malik [104].
1.1.1.5 Graph Based - Felzenswalb & Huttenlocher
A similar approach to the normalized cuts method is that of Felzenswalb and Hut-
tenlocher [30]. Here again they assume a graph G = (V ,E) where for every pixel pi
in the image there is a corresponding node vi in V . The edge set E is assumed to be
locally connected however, where the local neighborhood could be 4 or 8-connected
neighborhood of a pixel or any other local neighborhood connectivity. For each edge
(vi, vj) there is an associated weight wij, similar to normalized cuts, that measures the
dissimilarity between pi and pj. For example for an intensity image I we could define
wij = |Ipi ? Ipj |.
1.1 segmentation 13
To determine whether two components should be connected they define a boundary
predicate,
D(C1,C2) =
{
true if Dif(C1,C2) > MInt(C1)
false otherwise
(1.11)
where Dif(C1,C2) is defined as,
Dif(C1,C2) = min
vi?C1,vj?C2
wij (1.12)
and MInt(C1,C2) as,
MInt(C1,C2) = min(Int(C1) + ?(C1), Int(C2) + ?(C2)), (1.13)
where Int(C) is the value of the maximum weight edge in the minimum spanning tree
of component C in graph G. Intuitively Dif(C1,C2) captures the difference between
two connected components. For two connected components that don’t have a connect-
ing edge the value is set to?. The value MInt(C1,C2) captures the minimum internal
difference of either C1 or C2. When the difference between two components is larger
than the minimum difference in at least one component then we want a boundary
between the two components (i.e. D(C1,C2) = true), hence the inequality in eqn. 1.11.
The threshold ?(C) controls how much larger the inter-component difference needs to
be relative to the intra-component difference in order to have a boundary, and they set
it to,
?(C) =
k
|C|
(1.14)
for some constance k, where |C| is the size of C. Increasing k results in larger compo-
nents.
The Felzenswalb and Huttenlocher algorithm [30] for partitioning the graph using the
predicate is straight forward:
1. Order the m edges in E according to decreasing edge weights wij.
2. Set each node vi ? V as a distinct component in the initial segmentation, S0.
3. Repeat the following from k = 1, . . . ,m:
• For edge ek = (vi, vj) in the ordering if vi and vj are in separate components
of Sk?1 and if wij < MInt(Ck?1i ,C
k?1
j ) then merge C
k?1
i and C
k?1
j into a
single component in Sk. Otherwise Sk = Sk?1.
This simple algorithm can be applied to different neighborhood relations between ver-
tices in the graph — for example 8-pixel neighborhoods on grid graphs over the image,
or nearest neighbor graphs over feature space. In the nearest neighbor graph edges
14 introduction
connect vertices that are neighbors in feature space, with weights equal to the distance
between features. The neighborhood can be all vertices falling within a euclidean ball
or simply a fixed number of nearest neighbors returned by an approximate nearest
neighbor method. In either the grid or nearest neighbor graphs, using approximate
nearest neighbor methods, the running time is shown to be O(n logn). The results
when partitioning in feature space using the nearest neighbor graph tend to contain
higher level information because they capture more global image information. The
resulting segmentations tend to align well with image boundaries.
Ratio cut [120] is another well known such method.
1.1.1.6 SLIC
Arguably one of the best low-level segmentation algorithms that produces compact
superpixels with very good boundary adherence is Simple Linear Iterative Clustering
(SLIC) [1]. It is remarkably simple, and one of the fastest algorithms with a linear run-
time complexity in the number of pixels, O(n). It is also the superpixel algorithm we
chose for a number of our experiments in chapters 4 and 5. SLIC relies on a k-means
approach to clustering that assigns pixels with similar color features and spatial loca-
tions to the same cluster. The standard k-means approach of considering all pixels in
the image when finding nearest neighbors of a pixel is prohibitively slow so in SLIC
a different approach is taken. Initially a set of cluster centers, ci, are assigned along
a regular spatial grid over the entire image, corresponding to the centers of the su-
perpixels. The cluster centers are usually 5-dimensional vectors containing the color
intensity and spatial location. They are initialized with the pixel color and location
falling under the grid locations. The grid spacing, S =
?
n/k is proportional to the
number of superpixels, k, that the user would like to have in the image. These initial
centers are first adjusted to the lowest gradient locations within a 3× 3 neighborhood
of their initial locations so that they don’t fall on boundaries or noisy pixel locations.
The next step in the algorithm is to assign pixels to cluster centers, which means com-
puting the feature distance between each ci and the pixels that are within a spatial
neighborhood. Limiting the search space to pixels within a local neighborhood of a
cluster center is why this algorithm is so fast. Since the desired superpixel size is S× S,
the SLIC algorithm searches within a 2S× 2S neighborhood of ci. To each pixel the
algorithm assigns the label of the cluster ci that is closest to it in feature space. There
are no more that eight possible cluster labels for a pixel to be assigned due to the lim-
ited search region during assignment. Once each pixel is assigned a cluster center the
cluster centers are updated. The process is repeated until the residual error between
the current and previous iteration cluster centers reduces below some threshold. Al-
gorithm 1 is the same as that found in the SLIC paper [1]. A critical component of
why SLIC superpixels tend to be compact is the distance function D that it employs.
1.1 segmentation 15
(a) k = 100, m = 10 (b) k = 100, m = 50
(c) k = 500, m = 10 (d) k = 500, m = 50
Figure 1.2: SLIC superpixel results as a function of the desired number, k, and compactness,
m. As m is increased the superpixels exhibit more regular appearance, aligning less
with image contours and more with spatial grid.
In order to balance between spatial compactness and color consistency of a superpixel,
in SLIC they devise the following distance,
D =
?
d2c +
(
ds
S
)2
m2, (1.15)
where dc and ds are the Euclidean color and spatial distances between two points
respectively, and constant m controls the relative importance of color similarity versus
spatial compactness of the superpixels. Normalizing ds by S above balances the spatial
distance relative to color, which is important, otherwise compact superpixels would be
favored by the distance measure. Other distance functions such as geodesic distance
can also been considered. One issue that needs to be handled at the end of running
16 introduction
Algorithm 1 SLIC superpixel segmentation [1]
1: Initialize cluster centers ci by sampling pixels at regular grid interval S.
2: Move cluster centers to lowest gradient location within 3× 3 neighborhood.
3: label l(p)? ?1 for each pixel p.
4: distance d(p)?? for each pixel p.
5: repeat . Assignement
6: for each cluster center ci do
7: for each pixel p in 2S× 2S neighborhood around ci do
8: Compute distance D between ci and p.
9: if D < d(p) then
10: l(p)? i
11: d(p)? D
12: end if
13: end for
14: end for
15: update cluster centers using assignments l(p). . Update
16: compute residual error E between current and previous cluster centers.
17: until E 6 threshold
. Post-Porcess
18: enforce connectivity of superpixels by assigning orphaned pixels to nearest super-
pixels.
algorithm 1 is that some pixels are orphaned from the superpixel they belong to. A
post-processing step is done to reassign pixels to nearby superpixels so that they have
a connected structure.
1.1.1.7 Object Proposals - CPMC
Another class of segmentation algorithms is based on producing a large set of region
proposals that would provide good overlap with foreground objects. Object proposals
have the benefit of providing a larger image support for tasks where we want to
do category detection or semantic segmentation. Compared to the space of possible
solutions over pixel/superpixels the set of proposals provides orders of magnitude
fewer candidates and more efficient search. Given that the vast majority of possible
solutions over pixels or superpixels do not conform to general appearances of objects
in images — such as spatial connectivity — considering a much smaller subset of
proposals that adhere to image cues like boundary, and color and spatial uniformity,
is appealing. One popular region proposal approach is constrained parametric min-cut
(CPMC) [16]. In CPMC a set of initial candidate figure-ground proposals are generated
very efficiently which are subsequently pruned and ranked according to how likely the
1.1 segmentation 17
proposals are to tightly cover a foreground object. The top ranking proposals can be
retained for a higher level recognition task.
The initial set of proposal in CPMC is constructed by solving multiple parametric min-
cut [16] problems on a submodular grid graph over the image, with multiple different
initializations. Given a graph G = (V ,E) over pixels where adjacent pixels in a 4-
neighborhood share edges, a figure-ground segmentation is performed by minimizing
the following objective,
E(X, ?) =
?
i?V
D(xi, ?) +
?
(i,j)?E
Vij(xi, xj), (1.16)
where the data term is defined to be,
D(xi, ?) =
???????????
0 if xi = 1, i 6? Vb? if xi = 1, i ? Vb? if xi = 0, i ? Vf
f(xi) + ? if xi = 0, i /? Vf
(1.17)
where Vf and Vb are the pixels in a seed region for the foreground and background re-
spectively. The foreground seed regions are groupings of pixels forming small squares
a few pixels wide, sampled regularly along a grid over the image, and the background
seed regions are horizontal or vertical edges of the image. The pixel labeling takes on
either foreground (xi = 1) or background (xi = 0). The cost of assigning pixels not in
the foreground seed region to background is set by f(xi) + ?. The first term is either
uniformly set to 0 or is the log ratio of probabilities of pixel i belonging to foreground
versus background, where the probability of foreground is,
pf(i) = e
??minj(||I(i)?I(j)||), (1.18)
where j is over pixels in the seed region — and similarly for the background probability.
The parameter ? is a foreground bias that can be adjusted, where for each setting a
different solution is computed.
The pairwise term in eqn. 1.16 penalizes adjacent pixels that cross an image boundary,
Vij(xi, xj) =
{
0 if xi = xj
e
?
max(gPb(i),gPb(j))
?2 if xi 6= xj
, (1.19)
where gPb(i) is the contour strength at pixel i computed using globalPb [3]. Using a
parametric min-cut solver minimization of eqn. 1.16 can be done for all setting of ? in
the same time complexity as doing a single min-cut. The complexity of computing the
initial set of regions using k different combinations of foreground/background seeds
and choice of f(xi) is O(kmn logn), where m is the number of edges and n is the
number of pixels. Successively increasing values of ? given the same seeds and f(xi)
results in nested regions that progressively get larger.
18 introduction
Given this initial bag of figure-ground proposal regions pruning is done by throwing
away very small regions and sorting the remaining segments using ratio cut [120]
value. The top sorted 2000 proposals are kept.
Ranking of the proposals is done by training a regressor (random forest) that takes
region, Gestalt, and graph partition features over the proposal (details in [16]) to
regress onto the intersection-over-union score of the proposal with the best overlap-
ping ground-truth region. The objective is to retain the minimal set of ranked region
proposals that maximize a covering of the ground-truth regions, and discard the rest.
To do this CMPC uses the following covering score,
C(G,S(r)) =
1
n
?
g?G
|g| max
s?S(r)
O(g, s), (1.20)
where G and S(r) are the ground-truth segments and region proposals with rank
higher than r respectively, |g| is the number of pixels in the ground-truth segment
and O(g, s) in the intersection-over-union score between ground-truth and region pro-
posal segments. As the authors of CPMC note, many of the segments are similar in
shape size and location. Similar segments end up having similar features which mean
the regressor ranks them to similar scores, so the sorted list of region proposals will
have many cases where sequentially ranked segments have the same quality of cov-
erage for a ground-truth segment. Using the covering measure in eqn. 1.20 to pick a
cut-off rank r would result in a bag of segments with many redundant ones. To alevi-
ate this they propose to diversify the final bag using the Maximal Marginal Relevance
(MMR) measure [14],
MMR = argmax
si?S\Sk
? · score(si) ? (1? ?) · max
sj?Sk
o(si, sj), (1.21)
where S is the set of all proposals, Sk is the set of selected proposals in round k,
score(si) is the regressor score for proposal si, and o(si, sj) is the overlap between the
two proposal. The MMR measure is applied in an iterative fashion. Starting with the
highest scoring proposal which is placed in S1, the next proposal, si, picked is one that
has the best trade-off in maximizing the regressor score at the same time minimizing
its overlap with any of the previously picked proposals. This selection procedure can
continue until the covering score using the current set of region proposals reaches
some threshold.
This method produces very good quality region proposals and, at the time of our
experiments in later chapters, was a state of the art region proposal method. It shows
up as the underlying region-proposal method for the semantic segmentation model
O2P [17] which we use in Chapters 2 & 3.
1.1 segmentation 19
1.1.1.8 Object Proposals - continued
There are a number of other noteworthy object proposal methods which we’ll briefly
outline here.
Endres and Hoiem [27, 28] introduced a region proposal method that generates an
initial bag of region proposals and ranks them to produce a set of diverse region pro-
posals that maximally cover foreground objects in the image. The initial set of region
proposals is generated using the occlusion boundary algorithm [45] which constructs
a hierarchical segmentation of the image.
This hierarchical segmentation algorithm uses cues on both regions, boundaries, and
3D surface and 3D depth to predict occlusion boundary probability (i.e. boundaries
between different objects) as well as figure-ground probability at each pixel. After pre-
dicting the occlusion boundary probabilities, agglomerative clustering by iteratively
merging regions with minimum boundary strength up to a threshold produces the
hierarchical segmentation.
Given the segmentation hierarchy, initial seeds are picked from the hierarchy as start-
ing points from which to construct object proposals. These seed regions are used to
label superpixels as either belonging to the object or background depending on their
affinity with the seed region (i.e. likelihood of belonging to the same foreground object
as the seed). This problem is formulated as a CRF over superpixels consisting of two
terms, an affinity measure between superpixels and the seed region, and an edge cost
for two adjacent superpixels to take on a different label (i.e. foreground/background),
which is proportional to the probability of the occlusion boundary between the two su-
perpixels. The region affinity term uses features such as layout prediction of the seed
and superpixel on the object (e.g. right+left of object) to capture their layout agreement,
and their layout location (e.g. center, top, bottom, etc) on the object. Maximizing the
CRF energy infers a labeling over the superpixels indicating which of them are part of
the same foreground region as the seed.
Having generated a set of candidate proposals by considering multiple seed regions
their method ranks the proposals so that higher ranked proposals will be more likely
to tightly cover a foreground region while at the same time have minimal overlap with
any higher ranked proposal. Given a ranking r over a set of proposals x they define
the following score,
S(x, r;w) =
?
i
?(ri) · (wTa?(xi) ?wTp?(ri)), (1.22)
where ?(x) are appearance features and ?(r) is an overlap penalty incurring a cost
for a proposal to overlap with the set of higher ranked proposals. The monotonical
decreasing function of rank, ?(ri), encourages high ranked proposals to have higher
score. The appearance features captures how likely the proposal is to be an object re-
20 introduction
gion. Therefore they use occluding boundary probabilities, interior and exterior bound-
ary probabilities, likelihood of region being background from a background predictor,
and statistical differences in color and texture between the region and the surrounding
area to capture the appearance. It’s not possible to maximize eqn. 1.22 with respect to
r exactly so a greedy maximization strategy that incrementally selects proposals based
on which one maximizes the marginal gain. To optimize over w a latent max-margin
structure learning approach is used that minimizes the score of the highest scoring
incorrect ranking order while simultaneously maximizing the score of correct ranking.
They use a margin that encourages the best region proposals for each object to have
the highest rank. More details can be found in [27, 28].
The resulting region proposals from this method are competitive with CPMC. The
CPMC region proposals tend to be a bit less diverse and as the overlap threshold
for removing redundant regions is lowered the recall of object regions is worse than
Endres adn Hoiem’s method [27, 28].
1.1.1.9 ALE and the Associative Hierarchical CRF
A prime example of an object segmentation method that can achieve accurate segmen-
tation results by solving an inference problem on a Conditional Random Field (CRF)
over the image is the Automatic Labelling Environment [67]. It is a segmentation system
that is the culmination of a number of papers by Ladicky et al [56, 59, 68, 69, 71, 72,
98, 110]. The underlying model is a hierarchical CRF on pixels, segments, and super
segments over the image.
A main contribution of this segmentation model is the observation that the image
quantization level that one chooses is critical in producing good segmentations. It is
a common observation that inference on CRFs defined over random variables corre-
sponding to pixels in the image often produce segmentations that do not align well
with object boundaries. Conversely, the assumption that superpixels from bottom-up
algorithms align well with object boundaries is often wrong. That is why CRFs over
superpixels often yield segmentations that also do not align well with boundaries. Su-
perpixels do offer advantages though — they provide both larger and specific spatial
support (context) to compute features and as primitives they allow for more efficient
inference over graphical models. Also, the assumption that all pixels falling within the
same superpixel should take on the same label, though often incorrect, is nevertheless
a strong prior that often holds true. So the natural question is, what is the right level of
image quantization? The answer seems to be that it depends on image and the objects
in it. The major technical contribution of ALE, the associative hierarchical CRF [68], tries
to tackle this issue by considering multiple quantizations. The structure is a three-level
hierarchical CRF (the model and algorithm have no constraint on the number of levels),
where the bottom-most level consist of random variables over pixels in the image. At
the pixel level the random field consists of a grid graph over pixels in the image with
1.1 segmentation 21
a 4-pixel neighborhood for each pixel. The next level up consists of random variables
over segments. Each segment’s node has an edge (conditional dependence) between
it and the pixels that fall under it in the image. Edges connect adjacent segments in
the image. The top-most layer consists of random variables corresponding to super-
segments that are composed of the segments. Once again, the super-segment nodes
share an edge to the segment nodes beneath if the segment is contained by the super-
segment. Super-segments share edges if they are adjacent to each other in the image.
The initial (super)segmentations used at the different levels in the CRF hierarchy are
produced by running the bottom-up mean shift segmentation algorithm with vary-
ing bandwidth parameters for the color and spatial channels to produce progressively
coarser superpixels.
To summarize the model we can write the energy of the hierarchical model as pre-
sented in [68],
E(0) =
?
i?S(0)
?i(x
(0)
i ) +
?
ij?N(0)
?ij(x
(0)
i , x
(0)
j ) + min
x(1)
E(1)(x(0), x(1)), (1.23)
where x = {x(0), . . . , x(K)} is the vector of random variables, called the labeling, taking
on values from the label set Ln. The term ?i(x
(0)
i ) is the pixel-wise unary potential,
and ?ij(x
(0)
i , x
(0)
j ) is the pixel-wise label consistency term between neighboring pixels,
S(k) is the set of pixels or segments, and N(k) is the set of neighbors of a pixel or
segment at level k. The last term in eqn. 1.23 can be recursively written as,
E(k)(x(k?1), x(k)) =
?
c?Sk
?pc (x
(k?1)
c , x
(k)
c ) +
?
cd?N(k)
?cd(x
(k)
c , x
(k)
d ) (1.24)
+ min
x(k+1)
E(k+1)(x(k), x(k+1)).
The unary pixel-wise potentials are based on classifiers trained on color, shape and tex-
ture features (i.e. textons of TextonBoost [106]), historgrams of oriented gradients (HOG [24]),
and pixel location. The classifiers are applied at each pixel to estimate the probability
of the pixel to take on a particular label. The pairwise pixel terms are contrast sensitive
potentials [12] that encourage neighboring pixels to take on the same label,
?ij(xi, xj) =
{
0 if xi = xj
g(i, j) otherwise
(1.25)
where g(i, j) = |c|??(?p + ?v exp(???||fi ? fj||2)) [12, 60], where Ii and Ij are the color
vectors at pixels i and j, and ?p, ?v, and ?B are learned parameters.
The higher order potentials ?pc (x
(k?1)
c , x
(k)
c ) are robust Pn potentials [60] that are
equivalent to minimizing a pairwise graph over xc. Here x
(k)
c is the random variable
associate with a segment, or super-segment, c at level k in the hierarchy. The variable
c also stands for the clique consisting of variables (i.e. segments or pixels) at level
22 introduction
k? 1 that fall within the (super)segment c at level k. Therefore, x(k?1)c are the random
variables associated with the clique, c, of pixels or segments at level k ? 1 that fall
under segment c at level k. The robust Pn potential [68] is thus,
?pc (x
(k?1)
c , x
(k)
c ) = ?c(x
(k)
c ) +
?
i?c
?c(x
(k)
c , x
(k?1)
i ), (1.26)
where x(k)c takes on labels from L ? {LF}, and ?c(x(k)c ) is the (super)segment unary
potential that has a cost of ?lc, if x
(k)
c = l, or ?maxc , if x
(k)
c is assigned the free label LF,
where ?lc 6 ?maxc , ?l ? L. The pairwise potential [68] is defined as,
?c(x
(k)
c , x
(k?1)
i ) =
??? 0 if x
(k)
c = LF or x
(k)
c = x
(k?1)
i
wik
x
(k?1)
i
c otherwise,
(1.27)
where wi are learned weights and klc are costs associated with labeling a variable
clique c in level k ? 1 (i.e. child node) with a label l that is different than the label
for (super)segment c in level k (i.e. parent node). Therefore, the pairwise potential in
eqn. 1.27 encourages that all child variables in the lower level take on the same label
as the parent node in the higher level. Otherwise there is a cost incurred on each
and every variable in the clique taking on a different label. Combined with the unary
potential 1.26, the robust Pn potential encourages child variables to take on the same
label as the parent variables, but allows the possibility of heterogeneous labeling of
the child nodes. More specifically, by ensuring that the constraint
?
iwik
l
c > 2?c(l),
?l ? L, is satisfied, the parent variable will take a label l ? L if and only if the
(weighted) majority of child nodes takes on the same label. Otherwise, if the parent
node takes on label LF, then the child nodes are free to take on any label that minimizes
lower level unary and pairwise costs defined by the CRF, with an added cost for a
heterogeneous labeling of ?maxc . In summary, the higher order robust Pn potentials
favor homogeneous labelings of (super)segments but allow for the possibility that
regions within the (super)segments take on different labels.
The unary potentials over (super)segments are the responses of classifiers trained on
normalized histograms of clustered dense (pixel-level) features. The dense features in-
clude color, textons, HOG, and pixel location. The classifiers are multiple week learners
trained via AdaBoost [32, 33]. The variables, as define in [68], are set to,
?lc = ?s|c|min(?Hl(c) +K,?
h), (1.28)
where the log probability of clique (aka (super)segment) c taking on label l is Hl(c)
(given by classifier), ?h is a truncation threshold, and K = log(
?
l?L e
Hl(c)). The
other variables in the robust Pn potential are set to ?maxc = |c|(?p + ?s?h), and klc =
(?maxc ? ?
l
c)/0.1|c| (up to 10% of the pixels in a segment can be assigned a different
label than the segment variable before the variable is assigned LF). The pairwise (su-
per)segment potentials ?cd(x
(k)
c , x
(k)
d ) are the Euclidean distance between normalized
color histograms over pixels within the (super)segments.
1.1 segmentation 23
What’s hidden in eqn. 1.24 for every layer are weight constants, ?(k)1 and ?
(k)
2 for the
unary and pairwise potentials. In order to learn these parameters the approach taken
is to do a layer by layer search for the optimal parameter settings, on a validation
set, that minimizes the error between the dominant ground truth label for a clique
according to the ground-truth labeling and the label x(k)c assigned by the maximum
a-posteriori estimate (MAP) over the CRF.
Inference over the CRF, by computing the most probable label assignment (otherwise
known as the MAP estimate) for the energy defined by eqn. 1.24 has been shown to
run in polynomial time [99] using graph cut move making algorithms (?-expansion,
??-swap).
The ALE system also combines object detection with semantic segmentation by includ-
ing potentials over detections into the CRF energy. Ladicky et al. argue [71] that since
object detectors are good at localizing things, which have describable size and shape
(as opposed to stuff which are shapeless), the detections can be used to improve seg-
mentation accuracy. Object detections also provide object instance level information
that semantic segmentation does not; and coupled with the bounding box size, shape,
and location information provide rich information for scene understanding.
To incorporate detections from object detectors into the CRF, an additional term is
added to the energy in eqn. 1.23. Denoting Ehier as the energy in eqn. 1.23, the new
energy is defined as [71],
E(x) = Ehier(x) +
?
d?D
?d(xd,Hd, ld), (1.29)
where D are the set of detections given by a detection algorithm such as [31]. Each
detection has associated with it the bounding box (i.e. spatial extent) surrounding
the object, the predicted object label ld, and corresponding label probability Hd. The
detector potential introduces an auxiliary variable yd ? {0, 1} that indicates whether
the detector prediction is used or not. The form of the detector potential as define by
Ladicky et al. [71] is,
?d(xd,Hd, ld) = min
yd?{0,1}
(g(Nd,Hd)yd ? f(xd,Hd)yd), (1.30)
where the first term is a cost for having pixels inside the detector bounding box take
on different values that the detector label ld, and the second term is the likelihood of
an object being present inside the detector bounding box. The likelihood term is,
f(xd,Hd) = wd|xd|max(0,Hd ?Ht), (1.31)
with Ht a threshold controlling the number of detections. The label inconsistency term,
g(Nd,Hd), is defined to be,
g(Nd,Hd) =
f(xd,Hd)
pd|xd|
Nd, (1.32)
24 introduction
where Nd is the number of pixels inside the detector bounding box having a different
label from ld, and pd is set to a threshold percentage of inconsistent pixels.
Recall that the first energy term in eqn. 1.29 can be minimized efficiently using graph
cut move making algorithms [99]. It turns out that the detector potentials can also
be minimized with respect to xd using ??-swap and ?-expansion algorithms [71], so
inference on the CRF defined by the energy in eqn. 1.29 can be computed in polynomial
time.
The addition of the detector potentials help further disambiguate and correct false
labellings.
1.1.1.10 Zoom-out convolutional neural network
1
?1 . . . ?6 . . . ?10 . . . ?13 ?window ?scene
multi-layer perceptron
“car”
Figure 1.3: For a given superpixel (red) zoom-out features are computed at multiple zoom-out
levels (6 levels shown). The features from each level are stacked into a column vector
representation of the superpixel, and a multi-layer perceptron is used to predict the
superpixel class probabilities.
Semantic segmentation is often viewed as a structured prediction task, because of the
relationship between variables in the output space — for example pixels appearing in
a similar context take the same segmentation label and the likelihood of pixels taking
a particular label are conditionally dependent on what type of scene the image depicts.
As the hierarchical CRF model in ALE demonstrated, one common way to do struc-
tured prediction is to model the variables and their conditional dependencies using a
graphical model and running inference over the graph to compute the most likely la-
beling. Here we present another approach to doing structured prediction for semantic
segmentation that side steps the issue of explicitly imposing conditional dependencies
between variables via a graph structure and higher order potentials. The advantage is a
model that avoids the hard or intractable inference and learning that often plagues con-
1.1 segmentation 25
ventional structured prediction problems, while simultaneously incorporating higher
order clique interactions between regions in the image, in an implicit way.
The idea by Mostajabi et al. [82] proposes to label each region in the image by classify-
ing it using features computed on the image. We will assume that the regions are the
result of some bottom-up image segmentation algorithm such as SLIC. The task is to
semantically label each superpixel by classifying it such that the majority ground-truth
label over pixels falling in that superpixel agree with the semantic label predicted by
the classifier. This approach has been taken before where the classification is based on
features computed over the superpixel. What is new in the approach by Mostajabi is
the spatial extent over which features are computed. Instead of computing features
limited to the spatial extent of the superpixel, multiple spatial scales of influence or
context around the superpixel are also considered. The increasing spatial scales, or
zoom-out levels, can be broadly categorized as local, proximal, distant, and scene. They
can be described as follows,
• Local — the spatial extent defined by the superpixel itself. Features computed in
this region capture local color, texture pattern, and gradient cues specific to the
superpixel. Neighboring superpixels can have very different features, for exam-
ple if they appear on different objects in the image.
• Proximal — regions centered on the superpixel extending over a one or two
superpixel neighborhood. Neighboring superpixels will have overlapping prox-
imal regions and their corresponding features will have some similarity. As the
distance between superpixels grows beyond the one or two superpixel neigh-
borhood the features computed in their respective proximal regions will capture
increasingly different image statistics. Therefore the proximal region features im-
plicitly encode local conditional dependencies between superpixels that are in an
approximate neighborhood of each other.
• Distant — further along the scale, distant regions are centered on the superpixels
and capture a much larger portion of the image than proximal regions. Super-
pixels that are adjacent have very similar distant level features due to the large
overlap in the regions. Distant level features implicitly capture long range depen-
dencies between superpixels in the image. As the superpixels drift further apart
their distant level features gradually differ. These higher-order interactions are
difficult to incorporate into standard structured prediction models.
• Scene — this is at the level of the entire image. Features computed at the scene
level capture what the scene is depicting as a whole and provide strong cues as
to what object categories might be present in the scene. As such all superpixels
share the same scene level features, which provides soft global constraints to the
classifier that impact local predictions.
26 introduction
C
B
A
A
A
A
B
B
B
C
C
C
Figure 1.4: Examples of zoom-out regions. We show four out of fifteen levels: 1(cyan, nearly
matching the superpixel boundaries), 6 (olive), 10 (purple) and 13 (blue).
The spatial categories can be further refined along a more fine-grained scale pyramid.
A depiction of the features used by the local region classifier is shown in figure 1.3. The
features across the spatial regions are concatenated and used as input to the classifier.
The classifier, in turn, is trained to predict the likelihood of the superpixel taking on
a semantic label. Examples of the different zoom-out regions for three superpixels in
various images is shown in 1.4. You will notice superpixels that are closely spaced in
the image have overlapping proximal regions but as they are spaced further apart their
proximal regions no longer overlap but their distant level regions do. Thus superpixels
that are close share much of the same image statistics whereas distant superpixels do
not. Note from the figures that if two nearby superpixels are on the same object but
with very different local image features there’s still a high likelihood that the classifier
will assign them the same semantic label, given the fact that they share proximal and
distant features. Conversely, if two nearby superpixels are on different objects their
respective features computed over local regions will hopefully be different enough to
bias the classifier to label them as different semantic classes.
In order to incorporate the concept of multi-scale feature pooling, including scene-
level features, with state-of-the-art learned features, Mostajabi et al. use a convolutional
neural network (CNN) architecture trained on scene level classification. The feature com-
putation at multiple scales centered on a specific location in the image can be mapped
directly to the filter response of different convolutional layers in the CNN, correspond-
ing to the different spatial extents. Each filter response in a layer corresponds to a
receptive field in the image centered at a particular location. The layer response is a
three dimensional feature map, that is w× h× d dimensional. Responses from differ-
ent convolutional layers have different feature dimension, d. As you move further up
the CNN the feature maps have progressively smaller spatial extents, w× h, due to
convolutional kernel stride and feature pooling layers. Inversely, feature map locations
from further layers in the CNN have larger receptive field (region of influence) in the
image. In order to replicate the zoom-out idea with a CNN the responses of the con-
volutional layers are upsampled so that their spatial extents match the image size, and
subsequently pooled over superpixels to produce scale-space features. Figure 1.5 illus-
1.1 segmentation 27
1
inp
ut i
ma
ge
superpixel s
...
con
v1 m
ap
64 filters
?1(s) ? R64
avg. pooling over s
...
poo
l1 m
ap
...
con
v2 m
ap
128 filters
...
con
v2 m
ap
- re
size
d
upsampling×2
?2(s) ? R128
avg. pooling over s
...
poo
l2 m
ap
...
con
v3 m
ap
256 filters
...
con
v2 m
ap
- re
size
d
upsampling×4
?3(s) ? R256
avg. pooling over s
Figure 1.5: Zoom-out network architecture using an image classification CNN backbone, com-
puted over superpixels. The output response from the convolutional layers plus the
softmax output (scene level) are up-sampled to the image size and stacked into
the final feature map representation over the image. For each superpixel a feature
vector representation is constructed by pooling the feature map over the superpixel.
trates how the features for a superpixel are extracted from a CNN. Scene level features
are also extracted from the CNN as the final softmax probabilities for each semantic
category from the last layer of the CNN.
The CNN can be arbitrary, though deeper networks provide more spatial scales to
consider. Mostajabi et al used the propular VGG-16 convolutional neural network [108]
as the backbone, that is initially trained on the scene classification task.
The superpixel zoom-out features are next classified. Mostajabi et al. experiment with
both linear and non-linear shallow multilayer perceptron classifiers, which are trained on
Zoom-out features extracted from the training set. The classifier loss they minimize is
the standard category classification cross-entropy loss. Note that the zoom-out feature
classifier MLP and CNN used for extracting the superpixel features can be combined
and trained in an end-to-end fashion so that the CNN feature extraction layers can
benefit from supervision on the task of semantic segmentation over superpixels (as
opposed to just scene level classifaction supervision). This segmentation task specific
supervision further improves the accuracy (cf. [82]).
Experimental analysis of the relative importance of the features extracted at different
scales shows that they all contribute a non-negligible signal toward the prediction of
semantic labels [82]. The competitive performance (relative to state-of-the-art semantic
segmentation algorithms) of the Zoom-out network coupled with its relative simplic-
ity — and computational efficiency of a feed-forward model (relative to alternative
structured prediction models) — make it an attractive semantic segmentation model.
Figure 1.6 displays some typical segmentation results from the Zoom-out network.
28 introduction
Figure 1.6: Example semantic segmentation on VOC2012 val images using a 3-layer perceptron
classifier used to classify zoom-out features over superpixels across 15 zoom-out
levels of a CNN originally trained for scene classification.
The Zoom-out network of Mostajabi et al. can also be applied densely (to every pixel)
instead of over superpixels. This is done by first converting the CNN backbone into a
fully-convolutional CNN where the final fully-connected classification layers are con-
verted to 1 × 1-convolutional layers. Additional skip-connections are introduced that
take the output response of the intermediate convolutional layers, and pass them
through a concatenation layer that stacks the intermediate feature maps (after up-
sampling to be the same size as the image using bilinear interpolation) into a hyper-
column [42] representation for every pixel in the output. This feature map is then fed
into the final fully-convolutional classification layers of the network followed by soft-
max activation to make predictions at every pixel. Further dense refinement of the
label predictions can then be made by applying efficient approximate inference on
a special dense (i.e. fully-connected) CRF [2, 65] over the pixels. This CRF uses the
pixel label probabilities as unary potentials. The fully-convolutional CNN backbone
(minus the CRF) can then be learned in an end-to-end fashion. Typically the CNN is
initially trained on a complimentary task (and dataset) such as image classification
and then converted into a fully-convolutional network with skip-connections which
is then finetuned on the semantic segmentation task (and corresponding dataset). A
similar approach is taken by contemporary works such as [18, 19, 42, 75].
1.2 map problem
A Markov network — also known as a Markov random field — is defined an undirected
graph, G = (V ,E), with a set of vertices, V , associated with n = |V | random variables
X = {Xv|v ? V}, and a set of edges, E, between variables associated with the prob-
abilistic relationships between those variables. A Markov network structure encodes
1.2 map problem 29
the dependence assumptions associated with the random variables. The variables in
the Markov network have the following Markov properties:
• any two non-neighboring variables are conditionally independent given all other
variables in G,
Xs ? Xt | XV\{s,t}, ?(s, t) 6? E (1.33)
• any variable is conditionally independent of all other random variables in G
given the random variables that are its immediate neighbors (i.e. those it shares
an edge with),
Xs ? XV\{s?ne(s)} | Xne(s) (1.34)
This neighborhood of a node is referred to as its Markov blanket.
• any two subsets of variables, S1 and S2, in G are conditionally independent given
the subset of variable that connect them,
XS1 ? XS2 | XC, (1.35)
where every path from XS1 to XS2 in G pass through some variable(s) in XC.
We will restrict ourselves to discrete Markov random fields where each random vari-
able Xs takes values from the finite label set Xs := {0, . . . , r? 1}, so that the random
vector X ? Xn := X1 × · · · ×Xn, encodes the joint configuration over all the variables.
According to the Hammersley-Clifford theorem any probability distribution that is strictly
positive satisfies the above Markov properties if and only if it can be factorized accord-
ing to cliques of the graph. Therefore a Markov random field where p(x) > 0 for all
x ? Xn can be written as,
p(x) =
1
Z
?
c?C
?c(xc), (1.36)
where C is the set of cliques in G, xc = {xs | s ? c}, and ?c(xc) are functions from
Xc ? R called factors or clique potentials. A clique is a maximal subgraph of G. A clique,
Xc, is maximal if any superset containing Xc is not a clique. Equation 1.36 is referred
to as either the Gibbs distribution, Gibb random field, or Markov random field. The
normalizing constant Z is called the partition function,
Z =
?
X?Xn
p?(X), (1.37)
with unnormalized value,
p?(X) =
?
c?C
?c(xc) (1.38)
30 introduction
Equivalently, the set C in the factorization of eqn. 1.36 can be restricted to only contain
maximal cliques. This is because any factorization over complete subgraphs can be
equivalently written as a factorization over maximal cliques with corresponding clique
potentials that are products over all factors who’s scope is covered by the maximal
clique.
The structure of the Markov network generally does not capture the factorization ac-
cording to the Gibbs distribution, because the factorization could be over maximal or
non-maximal cliques and the Markov network does not show this explicitly. An alter-
nate parameterization of the Markov network is via a factor graph. In a factor graph
additional factor nodes are introduced. Edges are only between variable nodes and
factor nodes. Each factor, ?, in the factorization is associated with a corresponding
factor node in the graph. Variables are connected with a factor node with an edge if
the variables are found in the scope of the factor corresponding to the factor node.
An even more explicit parameterization of a Markov network is via log-linear models.
In this case each factor is written in an equivalent energy function form,
?(x) = exp(??(x)), (1.39)
with energy function ?(x) = ? ln?(x). The joint probability can then be written as,
p(x) =
1
Z
exp
(
?
?
c?C
?c(xc)
)
. (1.40)
To get the log-linear representation we can associate one or more potential functions
with each clique C. More specifically assume a set of potential functions {??|? ? I(C)}.
A potential function, ? : Xn ? R, associated with a clique C, has xC as its scope, and
I(C) is some index set over C. Let I = ?CI(C) be the union over the index sets of all
cliques. The log linear model can be written as,
p(x; ?) =
1
Z
exp
(?
??I
????(x)
)
, (1.41)
with parameters ??. Notice that any energy function over discrete variables, ?C(xC),
can be written as a weighted sum,
?
??I(C) ????(x), with appropriate choice of poten-
tial functions and ?. The above definitions and characterizations of Markov networks
can be found in more detail in [61].
In what follows we will review a re-characterization of the MAP inference problem as
an integer programming problem attributed to Wainwright et al. [118, 119].
If we let ? = [?1, · · · , ?d], where d = |I|, for the collection {?? | ? ? I} and define the
mapping ? : Xn ? Rd, for the collection {?? | ? ? I}, we can write eqn. 1.41 more
compactly as,
p(x;?) =
1
Z
exp??,??. (1.42)
1.2 map problem 31
Equation 1.42 defines a linear exponential family of distributions — therefore discrete
MRFs are linear exponential families. Each ? defines a different MRF.
The class of random fields that we will focus on are metric MRFs with discrete random
variables and at most pairwise factors. In fact any Markov random field with discrete
random variables and higher order factors can be turned into an equivalent MRF with
only pairwise factors (see [119]). For the pairwise MRF the index set is,
I := {(s; j) | s ? V , j ? Xs} ? {(st; jk) | (s, t) ? V , (j,k) ? Xs ×Xt} (1.43)
The metric pairwise MRF potential functions, ?, take specific form of indicator func-
tions. Specifically, the node and parwise interaction potentials are,
Is;j(xs) :=
{
1 if xs = j,
0 otherwise
?s ? V , j ? Xs, (1.44)
Ist;jk(xs, xt) :=
{
1 if xs = j and xt = k,
0 otherwise
?(s, t) ? E, (j,k) ? Xs ×Xt, (1.45)
and are referred to as the canonical overcomplete representation, with the corresponding
? called the canonical parameters [118]. The representation is overcomplete because they
satisfy certain linear constraints, namely,?
j?Xs
Is;j(xs) = 1 ?s ? V , (1.46a)?
(j,k)?Xs×Xt
Ist;jk(xs, xt) = 1 ?(s, t) ? E, (1.46b)?
j?Xs
Ist;jk(xs, xt) = It;k(xt) ?(s, t) ? E ?k ? Xt, (1.46c)
Plugging eqns. 2.34 and 2.35 into eqn. 1.42 and using the index set in eqn. 1.43 the
joint probability distribution for the pairwise MRF can be written as,
p(x;?) = exp
???
s?V
?
j?Xs
?s;jIs;j(xs) +
?
(s,t)?E
?
(j,k)?Xs×Xt
?st;jkIst;jk(xs, xt) ?A(?)
?? ,
(1.47)
where A(?) := lnZ(?). A more compact representation can be written if the following
substitutions are made,
?s(xs) :=
?
j?Xs
?s;jIs;j(xs), (1.48)
32 introduction
?st(xs, xt) :=
?
(j,k)?Xs×Xt
?st;jkIst;jk(xst, xt), (1.49)
into eqn. 1.47 resulting in the pairwise MRF joint probability distribution,
p(x;?) = exp
???
s?V
?s(xs) +
?
(s,t)?E
?st(xs, xt) ?A(?)
?? . (1.50)
In this thesis we will only be concerned with a specific type of inference problem —
namely finding the joint configuration x that maximizes the distribution p(x;?) spec-
ified by a particular ?. This is known as the maximum a posterior or MAP assignment
problem. Note that there can be multiple maximizing assignments. Formally we want
to find an assignment x? such that,
x? ? {x ? Xn | p(x;?) > p(y;?), ?y ? Xn} (1.51)
Notice that in eqn. 1.50, A(?) is independent of x, so the value of the assignment that
maximizes the joint probability is equivalent to,
max
x?Xn
??,?(x)? := max
x?Xn
?
s?V
?s(xs) +
?
(s,t)?E
?st(xs, xt). (1.52)
A maximizing assignment is thus,
x? := argmax
x?Xn
??,?(x)?. (1.53)
Equation 1.52 is an linear integer program (IP) because x ? Xn take on integer values
and both the constraint set and the objective function are linear. This integer program
is a discrete combinatorial optimization problem that is known to be NP-hard to solve
for general graphs [119]. To overcome this problem the integer program can be relaxed
into a continuous linear program.
Take the set P := {p(x) | p(x) > 0,
?
x p(x) = 1} of all probability distributions on x. It
is easy to see that the following equality holds,
max
x?Xn
??,?(x)? = max
p?P
?
x?Xn
p(x)??,?(x)?, (1.54)
because for any x that satisfies the LHS there exists a probability distribution p(·) that
puts all probability mass on x so the value of the RHS is at least as large as the LHS.
The RHS is also a convex combination of ??,?(x)? terms so cannot be any larger than
max
x?Xn
??,?(x)?.
1.2 map problem 33
Expanding the RHS term of eqn. 1.54 we get,
max
x?Xn
??,?(x)? = max
p?P
?
x?Xn
p(x)
???
s?V
?s(xs) +
?
(s,t)?E
?(s,t)(xs, xt)
?? (1.55)
= max
p?P
?
s?V
?
j?Xs
?s;j
?
x?Xn
p(x)Is;j(xs)+ (1.56)
+
?
(s,t)?E
?
(j,k)?Xs×Xt
?st;jk
?
x?Xn
p(x)Ist;jk(xs, xt),
We can define the following quantities,
µs;j :=
?
x?Xn
p(x)Is;j(xs) = Ep[Is;j(xs)] = P[Xs = j], (1.57)
µst;jk :=
?
x?Xn
p(x)Ist;jk(xs, xt) = Ep[Ist;jk(xs, xt)] = P[Xs = j?Xt = k], (1.58)
called mean parameters which have intuitive meaning — namely µs;j is the node
marginal probability that random variable xs takes label j, and µst;jk is the edge
marginal probability of the joint assignment (xs = j, xt = k). Plugging definitions 1.57
and 1.58 into eqn. 1.56 we get,
max
x?Xn
??,?(x)? = max
p?P
?
s?V
?
j?Xs
?s;jµs;j +
?
(s,t)?E
?
(j,k)?Xs×Xt
?st;jkµst;jk. (1.59)
Similar to ? we let µ = {µ? | ? ? I}. We can define the set of all possible marginal
probabilities on graph G as,
M(G) := {µ ? Rd | ?p(·) ? P s.t. µs;j = Ep[Is;j(xs)], µst;jk = Ep[Ist;jk(xs, xt)]}.
(1.60)
M(G) is called a marginal polytope [118]. From M(G) we see that searching over proba-
bility distributions P maps to searching over µ, which means that the RHS of eqn. 1.59
can be written as,
max
x?Xn
??,?(x)? = max
µ?M(G)
?
s?V
?
j?Xs
?s;jµs;j +
?
(s,t)?E
?
(j,k)?Xs×Xt
?st;jkµst;jk (1.61)
= max
µ?M(G)
??,µ?. (1.62)
Equation 1.62 is the linear programming (LP) relaxation of the original MAP integer
program.
Note that M(G) is the convex hull of the overcomplete representation defined in
eqn. 2.34 and eqn. 2.35 over the finite index set in eqn. 1.43. These indicator functions
define the extreme points of M(G). Since the extreme points are indicator functions
34 introduction
they take {0, 1} values, which means they are all integral. From standard linear pro-
gramming optimization theory the optimal solution of an LP always lies at an extreme
point of the feasible set (i.e. one of the vertices of M(G)).
For each x ? Xn the canonical overcomplete representation ?(x) corresponds to an
extreme point µx of M(G), thus the optimal solution is integral and in one-to-one
correspondence with assignments x. Moreover this means that M(G) has |Xn| ex-
treme points, which is exponential in n. Optimization over an exponential number
of constraints is not feasible so a simpler (read fewer constraints) outer bound on the
marginal polytope is desired.
According to the Minkowski-Weyl theorem any convex hull over a finite set of vectors can
be represented equivalently by the intersection of a finite number of linear half-spaces
(i.e. of the form {µ : aTµ 6 b} for some a ? Rd and b ? R). Included in the half-space
representation of M(G) the linear inequality (half-space) constraints also include the
equality constraints that are a consequence of the overcomplete representation. They
are analogous to the consistency constraints in eqns. 1.46a–1.46c, namely,?
j?Xs
µs;j(xs) = 1 ?s ? V , (1.63a)?
(j,k)?Xs×Xt
µst;jk(xs, xt) = 1 ?(s, t) ? E, (1.63b)?
j?Xs
µst;jk(xs, xt) = µt;k(xt) ?(s, t) ? E ?k ? Xt, (1.63c)
as well as non-negativity constraints on the marginal probabilities (i.e. µ? > 0 for all
? ? I). Note that each of the above equality constraints can be written as two inequality
constraints — i.e. the constraint aTµ = b is equivalent to maintaining the following
two constraints: aTµ 6 b and ?aTµ 6 ?b.
It turns out that for general graphs with cycles representing the marginal polytope
M(G) as an intersection of half-spaces, or facets, becomes difficult because the number
of half-spaces becomes exponential. Other than for tree structured graphs the number
of facets of M(G) in a general graph are not known. Instead a simpler outer bound on
M(G) can be constructed by simply considering the intersection of a subset of the half-
space constraints, namely those in eqns. 1.63a– 1.63c, along with the non-negativity
constraint on µ. This gives the local polytope which is set of locally consistent marginal
distributionsi [118, 119],
L(G) := {µ ? Rd+ | eqns. 1.63a– 1.63c hold} (1.64)
L(G) is the intersection of a subset of the half-space constraints required to represent
M(G), thus M(G) is a subset of L(G). It can be shown [118] that for trees (i.e. any
acyclic connected graph) L(G) = M(G). For general graphs with cycles though M(G)
will be a strict subset of L(G). The number of facets of L(G) is polynomial in graph
1.2 map problem 35
size. The number of extreme points of L(G) is larger than M(G) for general graphs,
which include the integral extreme points, {µx | x ? Xn}, plus a set of fractional
extreme points that lie outside of M(G), the total number of which is unknown for
general graphs. But that fact that L(G) can be represented as polynomial number of
facets (i.e. inequality constraints) means that the following alternate problem,
max
µ?L(G)
??,µ? (1.65)
can be efficiently solved. From eqns. 1.61–1.62 and the definition of the local polytope
in eqn. 1.64 we have the following relations,
max
x?Xn
??,?(x)? = max
µ?M(G)
??,µ? 6 max
µ?L(G)
??,µ? (1.66)
The relaxation on the RHS of eqn. 1.66 is tight for tree structured graphs, but is not
guaranteed to be tight for general graphs with cycles. Solutions to the RHS are optimal
(i.e. the relaxation is tight) if they lie at one of the integral vertices but on general
graphs with cycles the solutions are often at one of the fractional vertices. Much work
has been done to develop algorithms that give the tightest upper bound on the solution,
and the relationship between the the RHS relaxation (and its dual) and various efficient
approximate MAP inference algorithms such as tree-reweighted max-product message-
passing [118] and dual decomposition [64] have been established.
The MAP integer program and its linear relaxation formulation described above can
be credited to Wainwright et al. and a more detailed exposition can be found in the
respective material [118, 119].

2
D I V M B E S T
The primary objective of the research efforts described in this thesis is to improve se-
mantic segmentation. Typically, improving on an existing segmentation model means
devising a new model that produces more accurate segmentations. There are a number
of sources of error in any model that need to be addressed in order to improve upon
it. Approximation error — the error due to limitations imposed by the choice of model
class — is addressed by devising more accurate, and often more complex, models for
semantic segmentation. Unfortunately, as is often the case, more complex models ex-
hibit more optimization and estimation error. When models become too complex for
exact inference the approximate inference surrogate methods introduce optimization
error. More complex models often incorporate higher order interactions between vari-
ables and typically have more free variables — all of which require more examples to
train. The limitation of a finite training set to train the more complex model leads to
larger estimation error. Worse yet, it may not even be clear how to incorporate certain
higher-order information into a model for segmentation. Even if we are able to, we
may end up with models that are intractable to train or do inference on.
For all of the above reasons coming up with a new model can be difficult. Suppose
we would like to improve upon an existing discrete semantic segmentation model
which can be trained efficiently on a finite training set and on which inference is
tractable. Without loss of generality, given an image, the model is trained to minimize
the average error over the training set between the segmentation it produces and the
ground-truth for the image. At test time, given an image, we use the trained model
to infer the most likely (read probable) semantic segmentation of the image. When we
are working with a probabilistic model this segmentation is the maximum a-posteriori
(MAP) solution (or MAP assignment). Without loss of generality we’ll use MAP so-
lution to mean the most likely segmentation returned by the model irrespective of it
being a probabilistic model or not. We can assume that the model assigns a score to
every possible labelling (i.e. assignment to all the variables over the image), indicating
how likely the labeling is a correct segmentation of the image. Alternatively, we can as-
sociate a probability with the likelihood of the image returning a certain segmentation
for an image.
When we devise a more accurate segmentation model in effect what we want to achieve
is a model that produces a MAP solution that is at least as close to the ground-truth
segmentation for the image as the MAP solution produced by a less accurate model.
37
38 divmbest
However, the more accurate model might be intractable or at best comes at a cost of
higher estimation and approximation error.
Instead we can consider using the less accurate model to output multiple highly prob-
able solutions, not just the MAP. One of these other solutions might be a more accurate
segmentation of the image. We can then consider returning a set of segmentation for
the image or pick a single one from the set. If we consider that we are reasonably
confident in our sub-optimal segmentation model to return to give high probablity to
solutions that "do the right thing" in many areas of a typical image, then by producing
multiple high probability segmentations from the model we are considering alternate
explanations that the model exhibits for the same image. This approach is analogous to
cascade models [101, 117, 122] where successive stages of the cascade refine the output
of previous stages. Since the space of segmentations (i.e. labelings over (super)pixels) is
exponential, by producing an initial set of segmentations, instead of just the MAP, and
then refining it simplifies the inference problem by reducing it from a 1-out-of-|L|n to
a 1-out-of-M inference task (where L is the cardinality of the label set on each of the
n variables, and M is the number of high probability segmentations in the set, where
M |L|n). By producing not just one but multiple high probability segmentations we
are simultaneously providing an explicit way to manage the uncertainty in the model.
That is to say, compared to a single MAP solution, we are better summarizing the
uncertanties that the model has about the output space of labellings.
The problem of producing the M most probable solutions, which are different from
MAP and each other, is called the M-Best MAP problem. We will see that in fact this
idea has been studied in the context of problems outside segmentation and vision. We
will review the most well known approaches.
We will show that for the semantic segmentation task the M-Best segmentations are
not an ideal set of segmentations of an image. This is because the M-Best formulation
only enforces the segmentations not to be the same — there is no explicit control on
how diverse the segmentations in the set are. Contrary to problems in other domains,
generating a set of segmentations that are simultaneously highly probable and diverse is a
better way of managing uncertainty in the segmentation task. To this end, this chapter
presents the DivMBest problem that produces a set of highly probable segmentations
that are different from the MAP solution (and each other), with explicit control over the
amount of diversity between solutions. Intuitively the goal of the DivMBest approach
is to construct a set of segmentations that correspond to the modes of the output dis-
tribution of the underlying segmentation model. We will show that DivMBest is a
very general framework that can be applied to virtually any tractable segmentation
model, and it can be particularly efficient when we consider special forms of dissimi-
larity between solutions. In fact DivMBest, like the M-Best algorithm, can be applied
to any problem that can benefit from inferring more than just the MAP solution. We
also show that DivMBest generalized the M-Best method and more generally contains
other related formulations as special cases.
2.1 related work 39
For the case of a probabilistic segmentation model, a simple alternate way to gener-
ate multiple segmentations is to sample from it, such as with Markov chain Monte
Carlo (MCMC) sampling. There have been a number of works [5, 88, 89] that take this
strategy. It could, however, be a prohibitively time consuming approach, because of
the time required to return samples from modes with small support. Additionaly, in
contrast to the DivMBest approach, there isn’t any explicit control over diversity in
the set of sampled solutions, which would necessitate sampling a larger set to cover
the space of alternate explanations of the image. Related to DivMBest, Papandreou
and Yuille [87] present an approach (perturb-and-MAP) to sample from a discrete
probabilistic model (e.g. random field) by perturbing the model parameters with ran-
dom noise and solving for the MAP solution using existing discrete optimization algo-
rithms. This extends deterministic MAP inference to non-deterministic iid sampling of
the model distribution. In contrast to perturb-and-MAP the DivMBest approach mod-
ifies the model parameters in a deterministic way resulting in a set of highly probable
diverse solutions.
In this chapter we review the related M-Best algorithms and present the DivMBest for-
mulation, limiting the discussion to the case of discrete probabilistic models for ease
of exposition. We formulate the DivMBest problem as an integer program (see § 1.2)
minimizing a discrete energy (probability distribution) model over a set of random
variables subject to diversity constraints on the solutions and consider a linear pro-
gramming relaxation of it. We present a greedy iterative algorithm to efficiently com-
pute the DivMBest solutions, as well as a gradient ascent method to set search over
the diversity parameters. We show that for certain measures of diversity the LP dual
of the DivMBest problem enjoys some nice theoretical guarantees. In subsequent chap-
ters we show the superiority of DivMBest over MAP, and M-Best MAP inference, for
various segmentation tasks. In order to handle the 1-out-of-M inference task we also
introduce an approach to rank the DivMBest segmentation sets in order to return a
single segmentation.
2.1 related work
In the next section we begin with a review of the M-Best MAP problem and related
literature.
2.1.1 M-Best
One of the earliest methods to address the M-Best problem was by Lawler [74]. It
is a simple and general method for computing the M optimal solutions of discrete
optimization problems, and is agnostic to the optimization algorithm used to compute
40 divmbest
the solution of any specific problem. It is a divide-and-conquer method that solves
multiple independent discrete optimization problems that are created by iteratively
partitioning the assignment space. We’ll outline the basic method here (cf. [74]).
Without loss of generality the method assumes a set of binary variables, x1, . . . , xn ?
{0, 1}. Note that in problems where the discrete variables take on more than two values
one can make a straightforward transformation to {0, 1}-valued variables — e.g. ?xi ?
{0, . . . , ` ? 1} replace with ` variables: xi;j ? {0, 1}, where xi;j = [[xi == j]], with an
implicit constraint that
?
j xi;j = 1.
The task is to return the top-M solutions to a discrete optimization problem (w.l.o.g.
assume a minimization) over the xi’s – that is the M solutions that best minimize
the problem. The method starts by computing the optimal solution to the original
problem. It then iteratively partitions the assignment space into disjoint sets in a way
that removes the previous top m ? 1 solutions from consideration, and for each set
solves a new optimization problem returning a candidate solution, whereby the next
best solution is the one in the set of candidates with lowest value.
The algorithm is reproduced in alg. 2. More specifically, in each iteration m, of alg. 2,
n? s new problems are created, where s is the number of variables that were fixed in
the optimization problem that produced the previous solution x(m?1). The key, in line
9 of alg. 2, is to partition the assignment space into n? s disjoint sets, P1, . . . ,Pn?s.
Each problem Pj has the first s variables plus an additional j variables fixed. The j
additional variables are fixed in such a way to remove x(m?1) from the set of feasi-
ble solutions to Pj. Note that if for any j, xs+j is set to 1? x
(m?1)
s+j , then x
(m?1) has
been removed from the set of feasable solutions for Pj. Additionally the set of feasi-
ble solutions for Pj is disjoint from the rest of the problems {Pi | i 6= j}. Moreover,
n?s?
j=1
Pj = X? {x
(m?1)}.
Since in each iteration the s variables from the problem used to produce solution
x(m?1) remain fixed all the previous optimal solutions are also removed from consid-
eration in each problem, i.e. in iteration m we have
n?s?
j=1
Pj = X? {x
1, . . . , x(m?1)}.
Effectively alg. 2 recursively partitions the assignment space. Each new partitioning
occurs on the set of feasible solutions used to constraint the problem that produced
the optimal solution in the previous round. For example in the first iteration the par-
titioning is over the entire assignment space because no variables were fixed in the
optimization problem that compute the initial best assignment x(1).
The computational complexity of computing the top-M solutions using alg. 2 is O(Mn?(n)),
where ?(n) is the cost of solving a single optimization problem over n variables. Since
it is a general M-Best algorithm it is not tailored to any specific discrete optimization
problem so it cannot simultaneously solve for the top M solutions. This makes the
2.1 related work 41
Algorithm 2 M-Best-Lawler [74]
1: OPTS = ?, TOP-M = ?.
2: m ? 1: compute optimal solution x(1) ? argmaxx f(x), without fixing any vari-
ables in x
3: OPTS? OPTS? (x(1), ?).
4: repeat
5: (x(m),P(m))? bestsol(OPTS), TOP-M? x(m) . bestsol returns the minimum value solution
along with the set of fixed variables in the corresponding problem P(m) .
6: if m =M then
7: break;
8: end if
9: Let x1, . . . , xs be the variables that were fixed in the problem solved to get x(m),
i.e. P(m). Construct (n? s) new problems by fixing additional variables:
(P1): x1 = x(m)1 , . . . ,xs = x
(m)
s , xs+1 = 1?x
(m)
s+1
(P2): x1 = x(m)1 , . . . ,xs = x
(m)
s , xs+1 = x
(m)
s+1, xs+2 = 1?x
(m)
s+2
(P3): x1 = x(m)1 , . . . ,xs = x
(m)
s , xs+1 = x
(m)
s+1, xs+2 = x
(m)
s+2, xs+3 = 1?x
(m)
s+3
...
...
(Pn?s): x1 = x(m)1 , . . . ,xs = x
(m)
s , xs+1 = x
(m)
s+1, xs+2 = x
(m)
s+2, . . . ,xn?1 = x
(m)
n?1,xn = 1?x
(m)
n
10: for j = 1, . . . ,n? s do
11: Solve xPj ? argmin
x s.t. Pj satisfied
f(x)
12: OPTS? OPTS? (xPj ,Pj).
13: end for
14: m? m+ 1
15: until m =M
algorithm less efficient than specialized inference methods and in each iteration of the
algorithm n? s separate optimization problems need to be solved.
The space required for the algorithm is M(n? 1) because at most that many items are
in OPTS.
2.1.2 M-Best and Max-Flow Propagation
Dawid [25] and later Nilsson [84] extended the M-Best task to the problem of comput-
ing optimal configurations over directed and undirected graphical models with cycles.
Their approach relies on the ability to exactly and efficiently compute the maximizing
assignment to a joint distribution, over a set of random variables, which factorizes
according to cliques in the graphical model. In order to achieve this, the approach is
42 divmbest
based on constructing a higher order structure, called a junction tree, from the graph.
We’ll denote the junction tree with T. We won’t explain the junction tree construction
here (details can be found in [47, 61, 73]) but instead mention important properties. In a
junction tree nodes correspond to cliques of random variables from the set {Ci : i ? C},
where C is an index set over cliques. Edges in the set {Sij : (i, j) ? S}, connect adja-
cent nodes Ci and Cj, and are associated with the variables shared between the two
cliques, i.e. Sij = Ci ?Cj. The edge sets Sij are called separators. For any variable xu, if
it appears in any two cliques Ci and Cj of the junction tree, then it must also appear
in all the cliques in the unique path from Ci to Cj. This is known as the junction tree
property [61, 84].
The joint probability function, f, over random variables X, taking values x ? X, factor-
izes according to the cliques and edges in the tree as follows,
f(x) =
?
i?C fCi(xCi)?
(i,j)?S fSij(xSij)
, (2.1)
where fC and fS are non-negative real functions on cliques and edges respectively. As
before these functions are referred to as potentials. Since computing the probabilities
of f over the space of configurations X can often be exponential in the number of
variables, a factorization of f over cliques can be computationally advantageous if the
clique sizes are limited. Using a message-passing algorithm that limits computation
of (max) probabilities over just the cliques allows for efficient inference, as long as the
tree width (i.e. the maximum size of any clique in the tree) is small.
To compute the assignment of variables x ? X that maximizes f a max-flow message
passing algorithm over the junction tree is used.
2.1.2.1 Max-flows over Junction Tree
Assume that we are given an initial factorization of f over a set of clique and separator
potentials,
({fCi : i ? C}, {fSij : (i, j) ? S}), (2.2)
A message from node Ci to an adjacent node Cj is defined as the normalized max-flow
from Ci to Cj:
?i?j =
f ?Sij
fSij
, (2.3)
where,
f ?Sij = maxCi\Sij
fCi . (2.4)
2.1 related work 43
Given two sets B ? A, and function g on XB, the above max notation means,
max
A\B
g(xB) = max
z?XA
{g(z) : zB = xB}, (2.5)
where xB ? XB .= ×u?BXu. The update to clique potential fCj is then,
f ?Cj = fCj · ?i?j. (2.6)
Message-passing proceeds with the following update schedule: pick any node in T,
say C1, as the root node. Starting from the leaves of T pass max-flow messages up to
C1 and back down to the leaves. A clique sends a message to its neighbor Cj once
it has received all messages from its neighbors with possible exception of Cj, such a
message is called an active max-flow. After this two-phase propagation of messages
the potentials are guaranteed to have reached equilibrium resulting in max-marginal
potentials [25],
({f?Ci : i ? C}, {f?Sij : (i, j) ? S}), (2.7)
where the max-marginal potential over set A is defined to be,
f?A(xA) = max
z?X
{f(z) : zA = xA}. (2.8)
An important property of the update rule is that f in eqn. 2.1 is invariant to max flow
updates. To see this consider adjacent cliques Ci, Cj and the separator, Sij between
them. A max-flow update gives,
f ?Sij = maxCi\Sij
fCi , ?i?j =
f ?Sij
fSij
f ?Cj = fCj · ?i?j = fCj ·
f ?Sij
fSij
f ?Cj
f ?Sij
=
fCj ·f
?
Sij
f ?Sij
·fSij
=
fCj
fSij
,
(2.9)
where the RHS of the bottom row shows the invariance in max-flow update to f in the
contribution by Cj and Sij.
Also note the following property [84],
max
Ci\Sij
f?Ci = f?Sij = max
Cj\Sij
f?Cj , (2.10)
known as the max-consistency property, which holds after computing the max-marginal
potentials. Also note the following theorem,
Theorem 2.1.1 (Max-marginal theorem [84]). The joint distribution f and the marginals
agree on the maximimum value,
max
x?X
f(x) = max
xCk?XCk
f?Ck(xCk). ?k ? C (2.11)
This is a direct result of the definition of f?Ck ,
max
xCk?XCk
f?Ck(xCk) = max
xCk?XCk
{
max
z?X
{f(z) : zCk = xCk}
}
= max
x?X
f. (2.12)
44 divmbest
2.1.2.2 Maximizing assignment and traceback
To compute the maximizing assignment, x?, given the max-marginal potentials (eqn. 2.7)
the algorithm starts at the root of T, say Ci, and picks the assignment x?Ci that max-
imizes f?Ci(xCi). It then propagates simple max-flows [84]. From thm. 2.1.1 we have
that f?Ci(x
?
Ci
) = maxx?X f(x). Next the algorithms takes an incident separator Sij
and assigns the variables XSij the corresponding values in x
?
Ci
to get x?Sij . Because
of max-consistency (eqn. 2.10) we have that f?Sij(x
?
Sij
) = maxx?X f(x). The algorithm
now moves to Cj and assigns values from x?Sij to variables in XCj that coincide with
XSij . Then the algorithm finds the maximizing assignment, x
?
Cj
, to f?Cj over the remain-
ing variables in Cj – such that f?Cj(x
?
Cj
) = maxx?X f(x) due to max-consistency. The
algorithm proceeds until it’s processed the leaves of T.
In a tree with m nodes there’s m? 1 edges. Thus, given that f?Ci = max f, for all i ? C,
and f?Sij = max f, for all (i, j) ? S, then we are assured [84],
f(x?) =
?
i?Cf?Ci(x?Ci)?
(i,j)?S f?Sij(x
?
Sij
)
=
(max
x?X
f(x))|C|
(max
x?X
f(x))|S|
= max
x?X
f(x). (2.13)
2.1.2.3 Simplified max-flow propagation algorithm [84]
A rather simple but inefficient approach of producing M-Best solutions uses Lawler’s
M-Best algorithm of § 2.1.1. This algorithm is referred to as the simplified max-flow
propogation algorithm or SMFP [84]. Assume the vector of random variable assign-
ments x(1) = (x(1)1 , . . . , x
(1)
n ), represents the maximizing assignment to the joint proba-
bility distribution f(x) on T that we get after running the max-flow propagation algo-
rithm of the previous section. In order to compute the next highest assignment SMFP
partitions the space into n subsets that cover X \ {x(1)},
(P1): {x ? X : x1 6= x(1)1 }
... . . .
(Pi): {x ? X : x1 = x(1)1 , . . . , xi?1 = x
(1)
i?1, xi 6= x
(1)
i }
(Pn): {x ? X : x1 = x(1)1 , . . . , xn?1 = x
(1)
n?1, xn 6= x
(1)
n }
Note that each assignment space Pi constrains one of the variables to take on a differ-
ent value than it did in x(1), thereby removing x(1) from the space of assigments. In
2.1 related work 45
order to encode the constraints for an assignment space Pi, SMFP defines a series of
functions as follows,
f?i(x) =
{
f(x) if x1 = x
(1)
1 , . . . , xi?1 = x
(1)
i?1, xi 6= x
(1)
i ,
0 otherwise.
(2.14)
Then it’s easy to see that,
max
i?[n]
max
x?X
f?i(x) = max
x?X\{x(1)}
f(x). (2.15)
To compute maxx?X f?i(x) (i.e. the maximum assignment to f(x) constrained to Pi) a
subset of variables are fixed,
X1 = x
(1)
1 , . . . , Xi?1 = x
(1)
i?1, Xi 6= x
(1)
i }, (2.16)
by introducing the following representation that modifies the potential functions: given,
?1(x1;C) =
{
1 if (X1 ? C? x1 = x(1)1 ) or X1 6? C,
0 otherwise
(2.17)
...
?i?1(xi?1;C) =
{
1 if (Xi?1 ? C? xi?1 = x(1)i?1) or Xi?1 6? C,
0 otherwise
(2.18)
?i(xi;C) =
{
1 if (Xi ? C? xi 6= x(1)i ) or Xi 6? C,
0 otherwise
, (2.19)
SMFP modifies the clique and separator potentials as follows,
fCk(xCk) = fCk(xCk)
i?
q=1
?q(xq;Ck) (2.20)
Max-flows are then propagated in T until equilibrium. Given the max-consistency
property (eqn. 2.10) and thm. 2.1.1 the maximum of f?i(x) can be computed, and the
corresponding maximizing assignment x(2).
To find the third highest assignment to f(x), the partitioning is as follows. If x(2)
belongs to subset Pi, then it is refined by partitioning it into the following subsets,
(Pn+1): {x ? X : x1 = x(2)1 , . . . , xi?1 = x
(2)
i?1, xi 6= {x
(1)
i , x
(2)
i }}
(Pn+2): {x ? X : x1 = x(2)1 , . . . , xi = x
(2)
i , xi+1 6= x
(2)
i+1}
... . . .
(P2n?i+1): {x ? X : x1 = x(2)1 , . . . , xn?1 = x
(2)
n?1, xn 6= x
(2)
n }.
46 divmbest
Together with P0 = {P1, . . . ,Pn} \ Pi the new partitioning P1 = {Pn+1, . . . ,P2n+1?i}
covers X \ {x(1), x(2)}. Repeating the above procedure for computing eqn. 2.15, x(3)
can be found. Continuing in this way SMFP finds the M-Best solutions [84].
The down side of the approach is that many max-flow operations over T have to be
done in order to compute max
i?[n]
max
x?X
f?i(x) each round – a two-pass max flow propaga-
tion thru the entire tree for each assignment set.
Nilsson [84] also presents an alternate partitioning strategy that’s much more efficient
which relies on the running intersection property of junction trees. This improved parti-
tioning strategy allows for the max of f over partitions to be found with a single root
to leaf propogation of max flows.
2.1.3 M-Best solutions for loopy graphs and the BMMF algorithm
So far we have discussed M-Best algorithms when exact inference is tractable. This
included inference over general graphs with small tree-width that could be converted
to junction trees in order to carry out exact inference. When the tree-width of the graph
becomes large however inference over the junction tree becomes infeasible because the
clique sizes are too large, so alternate M-Best algorithms are needed.
Recall that when the graph is a tree (e.g. junction tree) exact inference can be carried
out using the max-product message-passing algorithm we reviewed earlier. Also, recall
from theorem 2.1.1 that the max-marginals and joint posterior distribution over the
variables agree on the maximizing value. When the graph is a tree the max-marginals
can be computed exactly and a traceback operation can be subsequently carried out to
find the most probable variable assignments.
If we have a loopy graph, using the junction tree representation for inference becomes
inefficient and approximate inference methods are needed for computing the approxi-
mate max-marginals over the graph. Moreover using traceback operation over the max-
marginals on a loopy graph isn’t guaranteed to return the maximizing assignment (see
[127] for an example). Independently picking the variable assignments that maximize
each individual max-marginal will not work either because ties can exist in the max-
marginal tables (i.e. max-marginal has more than one maximizing label for a variable)
which means that theorem 2.1.1 will not hold (cf. [127]). Since ties can exist and trace-
back over a loopy graph will not work the alternative is to have multiple rounds of
computing the max-marginals, where in each round additional tied variables are con-
strained to take on a single maximizing label. This process is continued until no more
ties exist and we can get the maximizing assignment by independently maximizing
over individual max-marginals.
2.1 related work 47
Nilsson’s SMFP algorithm that we discussed in 2.1.2.3 is an example of an M-Best al-
gorithm that computes the max-marginals in a junction tree by using max-product mes-
sage passing algorithm and subsequently uses the max-consistency property (eqn. 2.10)
and thm. 2.1.1 to find the maximizing assignment. It needs O(Mn) computations of the
max-marginals which is very expensive, where M is the number of M-Best solutions
and n is the number of variables in the graph.
The following algorithm by Yanover and Weiss [127] can find the M-Best solutions
in loopy-graphs with only 2M computations of max-marginals (M is the number of
M-Best solutions), and no trace-back operations (relying only on thm. 2.1.1).
Algorithm 3 Best Max-Marginal First (BMMF) algorithm for M-Best solutions [127]
1: SCORE1(i, j)? max
x : xi=j
f(x)
2: x
(1)
i ? argmax
j
SCORE1(i, j)
3: CONSTRS1 ? ?
4: USED2 ? ?
5: for m = 2, . . . ,M do
6: SEARCHm ? (i, j, k < m : x(k)i 6= j, (i, j,k) 6? USEDm)
7: (im , jm ,km )? argmax
(i,j,k)?SEARCHm
SCOREk(i, j)
8: CONSTRSm ? CONSTRSkm ? {xim = jm}
9: SCOREm(i, j)? max
x | xi=j, CONSTRSm
f(x)
10: x
(m)
i ? argmaxj SCOREm(i, j)
11: USEDm+1 ? USEDm ? {(im, jm,km)}
12: CONSTRSkm ? CONSTRSkm ? {xim 6= jm}
13: SCOREkm(i, j)? max
x | xi=j,CONSTRSkm
f(x)
14: end for
15: return {x(m)}Mm=1
In alg. 3 the joint probability over all variables of interest (e.g. posterior probability) is
represented as f(x). The algorithm start by inferring the max-marginals in line 1. In
line 2 the maximizing MAP assignment is found using the max-marginal theorem. To
compute the remainingM?1 solutions the algorithm repeats the following operations:
the max-marginal tables are searched to find the variable with next best max-marginal
value, (cf. lines 6-7). The variable is fixed to the label corresponding to this value (i.e.
xit = jt) and added as a constraint for the next round of max-marginal computations
(see cf. lines 8-9). Using th max-marginal theorem the next best solution is computed
(cf. line 10). The complementary constraint (i.e. xit 6= jt) is added to the constraint set
used to produce the max-marginals that gave the highest value earlier and the max-
marginals are recomputed with this augmented set of constraints (cf. lines 12-13). In
each iteration, t, a new set of max-marginals is added (i.e. SCOREt(i, j)) that is the
result of a running inference on the graph with some of the variables fixed. This fixing
48 divmbest
of variables successively refines the partitioning of the assignment space in such a way
that the previous best solutions are removed from consideration.
It turns out that for exact max-marginal computation, the assignment x(m) produced
by the BMMF algorithm 3 is the m-th most probable assignment under f(x) (cf. [127]).
For loopy graphs where approximate inference algorithms have to be used for com-
puting the max-marginals (e.g. loopy max-product belief propagation), the solutions
produced by BMMF (so called loopy-BMMF) are not guaranteed to correspond to the
M-Best solutions but tend to be quite good in practice, compared to the top M assign-
ments produced by Gibbs sampling.
2.1.4 M-Best MAP and its linear programming formulation
We’ve seen that when computing MAP assignments is not tractable approximate meth-
ods can be used to compute the approximate M-Best solutions. Yanover and Weiss’ M-
Best MAP method (loopy-BMMF) [127], that we reviewed earlier, is one such method.
The downside of approaches such as loopy max-product is that they do not provide
bounds on the optimal values of the solutions. However, LP approximations to MAP
do provide bounds on the optimal value and Fromer and Globerson [34] provide an
extension of the LP MAP formulation to the M-Best setting. This section provides an
overview of their approach.
To start, recall from the review in chapter 1 that the MAP problem can be formulated
as the following LP,
max
x
f(x) = max
µ?M(G)
µ ·? (2.21)
and that the maximizing µ(?) is integral and found at a vertex of M(G) — where
M(G) is the marginal polytope defined in eqn. 1.60. Moreover, µ(?)) corresponds to the
MAP assignment x(1). For general graphs representing M(G) requires an exponential
number of inequalities so recall that the LP is relaxed by using an outer bound on
M(G), called the local polytope (cf. eqn. 1.64), L(G) which can be represented by far
fewer inequality constraints over the variables (i.e. half-spaces). As we mentioned in
§ 1.2, it has been shown that M(G) = L(G) for tree structured graphs, so solving the
LP-relaxation yields the exact MAP assignment.
2.1 related work 49
2.1.5 M-Best MAP LP when G is a tree
First consider tree-structured graphs. In order to extend the MAP LP formulation in
eqn. 2.21 to the 2nd best MAP problem Fromer and Globerson [34] propose to swap
M(G) for the following assignment-excluding marginal polytope,
M?(G, x(1)) = {µ | ?p(x) ? P s.t. p(x(1)) = 0, p(xs, xt) = µst(xs, xt), p(xs) = µs(xs)},
(2.22)
where M?(G, x(1)) is the convex hull of a set of integral vectors corresponding to the
different assignments, excluding only x(1). They show that,
max
x 6=x(1)
f(x) = max
µ?M?(G,x(1))
µ ·?. (2.23)
In order to represent M?(G, x(1)) as inequalities in the MAP LP, Fromer and Globerson
propose the following: when G is a tree they show that adding the single inequality
I(µ, x(1)) 6 0 to M(G) will result in M?(G, x(1)), i.e.
M?(G, x(1)) = {µ | µ ?M(G), I(µ, x(1)) 6 0} (2.24)
where,
I(µ, x(1)) =
?
s?V
(1? ds)µs(x
(1)
s ) +
?
(s,t)?E
µst(x
(1)
s , x
(1)
t ), (2.25)
and ds is the degree of the nodes s in the tree (cf. [34]). They show that when G is a
tree the polytope M?(G, x(1)) will remove only the integral solutions x(1) and will not
introduce fractional solutions. They point out that for general graphs however, adding
I(µ, x(1)) 6 0 to G removes some other integral vertices and may introduce fractional
vertices.
2.1.6 M-Best MAP LP when G is a general graph
Recall from chapter 1 that when G is a general graph the polytope of feasible solutions,
M(G), for the MAP LP needs an exponential number of constraints, so the simpler
outer-bound approximation, L(G) is used. Analogously, for the M-Best MAP problem
Fromer and Globerson [34] propose an outer-bound approximation to M?(G, x(1)). The
approach they takes is to add inequalities to L(G) to separate x(1) from the other
integral vertices. Each new constraint also removes some fractional vertices. If enough
such constraints are added then maybe only an intergral solution is left. The type of
constraints they add are inequalities over spanning trees on the graph,
IT (µ, x(1)) =
?
s?V
(1? dTs )µs(x
(1)
s ) +
?
(s,t)?E
µst(x
(1)
s , x
(1)
t ), (2.26)
50 divmbest
where dTs is the degree of node s in spanning-tree T . Analogous to when G is a tree
(eqn. 2.24), for general graphs they propose an assignment-excluding marginal poly-
tope that incopropates all spanning-tree inequalities of the graph,
L?ST (G, x(1)) = {µ | µ ? L(G),? tree T ? E IT (µ, x(1)) 6 0}. (2.27)
The 2nd best MAP LP for general graphs is thus,
max
µ?L?ST (G,x(1))
µ ·?, (2.28)
which is an approximation to solving over the feasible set M?(G, x(1)). They note that
maximizing over M?(G, x(1)) is guaranteed to give an integral solution whereas maxi-
mizing over L?ST (G, x(1)) does not.
The number of spanning trees over G is exponential in n but Fromer and Globerson
use an efficient approach to consider all spanning trees. They first note that given µ
and spanning tree T , the quantity IT (µ, x(1)) can be decompose over edges,
IT (µ, x(1)) =
?
(s,t)?E
(µst(x
(1)
s , x
(1)
t ) ? µs(x
(1)
s ) ? µt(x
(1)
t )) +
?
s?V
µs(x
(1)
s ), (2.29)
therefore to find the tree that maximizes IT (µ, x(1)) is equivalent to computing the
max-weight spanning-tree over G where the edge weights are set to,
wst = µst(x
(1)
s , x
(1)
t ) ? µs(x
(1)
s ) ? µt(x
(1)
t ). (2.30)
They then rely on existing efficient algorithms for computing the max-weight spanning-
tree of a graph.
To solve the LP-relaxation in eqn. 2.28 they use a cutting-plane algorithm that adds the
most violated constraint IT (µ, x(1)) > 0 to the LP. Starting with any spanning-tree of
G the most violated spanning-tree inequality for the current setting of µ is found and
added to the LP. This inequality removes µ from the polytope of feasible solutions. The
LP is solved again for a new setting of µ. The process continues until a non-fractional µ
is found or all the constraints are satisfied. If there are no violated constraints and µ is
still fractional Fromer and Globerson propose additional constraints that can be added
but they note that typically only a few iterations of the cuntting-plane algorithm are
required to give integral solutions.
To extend the 2nd best MAP problem to the M-Best MAP problem they propose an al-
gorithm that recursively partitions the assignment space, similar to that of Nilsson [84]
and Weiss [127] which we reviewed earlier. Their Partioning for Enumerated Solutions
(PES) algorithm is shown in alg. 4.
The most computationally expensive part of the algorithm is the inference on line 17.
The LP’s are solved using general LP solvers such as CPLEX [46]. When the M-Best
inference in line 17 is the LP-relaxation of eqn. 2.28, Fromer and Globerson refer to the
algorithm as Spanning Tree Inequalities and Partitioning for Enumerated Solutions
(STRIPES) [34].
2.2 divmbest algorithm 51
Algorithm 4 PES Algorithm (cf. [34])
1: for m = 1, . . . ,M do
2: if m = 1 then
3: x(1) ? arg maxx f(x) . MAP assignment
4: CONSTRS1 ? ?
5: else
6: k? argmax
k?{1,...,m?1}
f(y(k)) . find assignment space containing highest valued assignment
7: x(m) ? y(k) . next best assignment
8: (v,a)? any member of the set {(s, x(m)s ) | x(m)s 6= x(k)s }
9: CONSTRSm ? CONSTRSk ? {xv = a} . remove x(k) from assignment space m
10: CONSTRSk ? CONSTRSk ? {xv 6= a} . remove x(m) from assignment space k
11: y(k) ? NextBestSolution(CONSTRSk, x(k))
12: end if
13: y(m) ? NextBestSolution(CONSTRSm, x(m))
14: end for
15: return {x(m)}Mm=1
16: procedure NextBestSolution(CONSTRS, x(?))
17: return y? argmax
x 6=x(?), CONTRS
f(x)
18: end procedure
2.2 divmbest algorithm
1 M-Best algorithms only constrain the m-th solution to be different than the previ-
ous m? 1 high probability solutions. For each of the previous solutions the current
one needs to have a different value for at least one variable. While the set of M-Best
solutions is a more diverse set to pick from than the MAP assignment, the amount of
diversity in the M-Best set is not a parameter that can be adjusted and the minimum
amount of diversity between solutions is not a-priori quantifiable. This is why apply-
ing M-Best methods to discrete probabilistic models for image segmentation tend to
produce M-Best segmentations that are very similar to the MAP solution and each
other. The number of possible segmentations for a typical image is |L|n, where n is the
number of pixels ( between tens of thousands to millions), and the number of labels
per pixel, L (two or more). The number of segmentations is exponential in n. If the dis-
crete distributions over the space of assignments, that our probabilistic models learn,
had spiky modes around very different solutions with nearly equal probability then
the exact M-Best solutions would indeed be diverse. Generally though the learned
distributions contain modes that are smooth around neighborhoods of very similar
1 The contributions to the thesis presented in this section are found in [7], and are in collaboration with
Gregory Shakhnarovich and Dhruv Batra.
52 divmbest
solutions giving them nearly equal probability, which results in M-Best solutions that
are very similar. Given that the space of segmentations is large, these neighborhoods
around modes can contain a large number of very similar segmentations, each with
nearly the same high probability. Having a set of segmentations that are very simi-
lar to one another and the MAP segmentation, both qualitatively and quantitatively,
doesn’t provide an advantage over choosing the MAP segmentation. Instead we want
to produce a set of segmentations that meet certain criteria.
The key criteria of the set of segmentations produced with an M-Best-like method
include,
1. the set contains highly probable segmentations,
2. the segmentations are sufficiently different from one another and the MAP seg-
mentation,
3. the set is as small as possible
The last property is important because we would like to reduce the assignment space
to a set small enough on which more complex inference methods can be applied to
pick a single high probability segmentation. This includes having a user in the loop
to pick from the set. Clearly the first two properties are opposing — the more diverse
the segmentations are the more likely that the set contains low probability ones, and
inversely, higher probability segmentations tend to come from the same mode, hence
are very similar.
The ideal set containing segmentations corresponding to the M-Best-modes of the dis-
tribution learned by the probabilistic model satisfy the three properties above.
In this section we introduce an M-Best-like approach that tries to ensure the above
properties, called DivMBest— in contrast to M-Best MAP, the DivMBest approach
emphasizes diversity between solutions. We will show that the M-Best MAP problem
is a special case of the DivMBest formulation.
To ensure that the set of segmentations contains sufficiently diverse segmentations the
DivMBest formulation incorporates a measure on dissimilarity between two segmen-
tations. The formulation maximizes a linear combination of the probability of solution
and dissimilarity to previous solutions. In fig. 2.1 and fig. 2.2 we illustrate, qualitatively,
the differences between the MAP segmentation, and various alternate segmentations
returned by M-Best and DivMBest methods, for two segmentation tasks.
Figure 2.1 provides evidence that a diverse set of highly probably segmentations under
the learned model can contain significantly more accurate explanations of the image
compared to the MAP assignment. Figure 2.2 compares segmentations corresponding
to the MAP, 2nd best MAP, and 2nd best mode (second assignment of DivMBest
2.2 divmbest algorithm 53
input MAP mode input MAP mode
Figure 2.1: Semantic segmentations on test images from PASCAL VOC 2010. For each image,
from left: input image, MAP segmentation, best out of 10 modes obtained with
DivMBest.
Input MAP 2nd MAP 2nd Mode
Figure 2.2: Interactive segmentations. For each image, from left: input image, MAP solution,
2nd best MAP, and the 2nd best mode obtained with DivMBest.
algorithm) for an interactive segmentation task. The discrete probabilistic model is
learned over bottom-up superpixels computed over the image. The 2nd best MAP
segmentation is almost identical to the MAP estimate, and one must look closely to
see the minor difference. In contrast, the 2nd best mode of the DivMBest approach
recovers a large portion of the object in one case and gives a drastically different
explanation of the image in the other image. The advantage of using a DivMBest
approach to segmentation over M-Best MAP is apparent from these examples.
2.2.1 Overview
This section presents the DivMBest approach which is a generalization of the M-Best
MAP problem. It borrows a similar formulation as that of the M-Best MAP integer
54 divmbest
programming problem [34]. The DivMBest problem has access to, and so assumes is
available, a dissimilarity function, ?(·, ·), measuring the difference between two solu-
tions. The Lagrangian relaxation of the DivMBest integer program yields a problem
that minimizes a linear combination of the energy and similarity to previous solutions.
We conclude the section by presenting some nice properties of this linear programming
relaxation of the original DivMBest problem.
2.2.2 Contributions
The main contributions of the thesis in this section include,
• the first principled formulation for extracting a set of diverse highly probable
solutions in discrete MRFs. The M-Best MAP problem is a special case of this.
• For certain families of diversity functions between solutions, we show that the
Lagrangian relaxation to the DivMBest integer program is no more difficult to
solve than the MAP problem. This makes it an attractive approach for inference
because the same exact or provable approximate algorithms used to compute the
MAP solution can be used to compute the DivMBest solutions.
2.2.3 Notation
To refresh the notation we gave in Chapter 1, recall that we are given a set of discrete
random variables X = {Xs | s ? [n]} (where [n] .= 1, 2, . . . ,n), each takes a value from
a finite set, xs ? Xs. Give a clique C ? [n], from a set of cliques C ? C, let xC denote
{xs | s ? C}, and the label space XC be the cartesian product of the label spaces in the
clique, ×s?CXs.
2.2.4 MAP problem
Let G = (V ,E) be a graph defined over the random variables X, and ?C : XC ? R
be functions defining the energy over cliques in the graph. Let I(C) be some index set
over C and let I .= ?CI(C) be the union over index sets of all cliques in the graph. The
maximum a-postiriori (MAP) problem is to find the assignment x ? Xn that minimizes
the following energy function:
min
x?Xn
?
??I
??(x?) = min
x?Xn
?
s?V
?s(xs) +
?
(s,t)?E
?st(xs, xt), (2.31)
2.2 divmbest algorithm 55
where we’ve restricted the cliques to be over nodes and edges of the graph for ease of
exposition, but the method developed here apply to higher-order MRFs as well.
2.2.5 MAP integer program and its LP relaxation
Representing the energy in exponential form, the MAP problem can be written us-
ing the canonical overcomplete representation [118] where the node and edge energy
functions can be defined as,
?s(xs)
.
=
?
j?Xs
?s;jIs;j(xs), (2.32)
?st(xs, xt)
.
=
?
(j,k)?Xs×Xt
?st;jkIst;jk(xs, xt), (2.33)
using the following node and edge potential functions,
Is;j(xs)
.
=
{
1 if xs = j,
0 otherwise
?s ? V , j ? Xs, (2.34)
Ist;jk(xs, xt)
.
=
{
1 if xs = j and xt = k,
0 otherwise
?(s, t) ? E, (j,k) ? Xs ×Xt, (2.35)
yielding the re-written MAP inference problem,
min
x?Xn
?
s?V
?s · Is +
?
(s,t)?E
?st · Ist (2.36)
where for each clique Cwe have the set of energies for all possible configurations of xC,
?C
.
= {?C;? | ? ? XC}, and corresponding potential functions IC .= {IC;?(xC) | ? ? XC}.
Instead of minimizing over x we could alternatively assume that there are unknown
variables µ such that the above MAP inference problem is equivalent to,
min
µs,µst
?
s?V
?s · µs +
?
(s,t)?E
?st · µst (2.37a)
s.t.
?
j?Xs
µs;j(xs) = 1 ?s ? V , (2.37b)?
(j,k)?Xs×Xt
µst;jk(xs, xt) = 1 ?(s, t) ? E, (2.37c)?
j?Xs
µst;jk(xs, xt) = µt;k(xt) ?(s, t) ? E ?k ? Xt, (2.37d)?
k?Xt
µst;jk(xs, xt) = µt;k(xs) ?(s, t) ? E ?j ? Xs, (2.37e)
µs;j(xs), µst;jk(xs, xt) ? {0, 1} (2.37f)
56 divmbest
where the constraints enforce that each variable is assigned a single label and the
assignments agree across edges. Note that when indicator variable µA(v) is set to 1,
this corresponds to xA taking label v. If we let L(G) denote the set of constraints in
( 2.37a)-( 2.37e) then we can write eqn. 2.37 more concisely as,
min
µ?L(G), µA(xA)?{0,1}
?
A?V?E
?A · µA. (2.38)
The above integer program is equivalent to the MAP problem in eqn. 2.36, and is
known to be NP-hard in general. We describe a Linear Programming relaxation of this
problem in Chapter 1. A good review is also found in [123].
2.2.6 DivMBest: Formulation
This section presents the DivMBest formulation. The goal is to generate a diverse set of
low-energy (high-probability) solutions. The approach is an iterative greedy algorithm
— in each iteration we find the lowest energy solution that is at least some minimum
dissimilarity from the previously generated solutions. To measure the dissimilarity
between solutions the algorithm has access to a dissimilarity function ?(µ(1),µ(2))
between two solutions. Suppose that we have already computed the MAP solution,
which we denote as µ(1). In order to compute the second best mode (we use the term
mode loosely to mean low-energy diverse solutions) we propose to solve the following
general problem,
µ(2) = argmin
µ?L(G), µA(xA)?{0,1}
?
A?V?U
?A · µA (2.39a)
s.t. ?(µ,µ(1)) > k, (2.39b)
which we call 2Modes(?,k). The constraint in (2.39b) ensures that the next solution is
at least k-units away from µ(1) according to ?(·, ·). The choice of ? and k are design
choices which we’ll describe in greater detail later.
Since DivMBest is an iterative greedy approach we can extend the 2Modes(?,k) prob-
lem to the MModes(?,k) problem (k = {ki | i ? [m? 1]}) in a straightforward manner
by searching for the lowest energy solution that is km-units away from each of the
previous m? 1 solutions,
µ(m) = argmin
µ?L(G), µA(xA)?{0,1}
?
A?V?U
?A · µA (2.40a)
s.t. ?(µ,µ(1)) > k1, (2.40b)
?(µ,µ(2)) > k2, (2.40c)
... (2.40d)
?(µ,µ(m?1)) > km?1 (2.40e)
2.2 divmbest algorithm 57
2.2.7 DivMBest: Lagrangian Relaxation and the Lagrangian dual function
Given the extra inequality constraints of (2.40b)-(2.40e) it’s not clear how this problem
relates to common MAP inference approximations to the MAP IP in eqn. 2.38. The
Lagrangian relaxation of MModes(?,k) that we get by dualizing the constraints can be
written as,
f(?) = min
µ?L(G), µA(xA)?{0,1}
?
A?V?E
?A · µA ?
m?1?
i=1
?i(?(µ,µ(i)) ? ki), (2.41)
where ? are the dual variables, also referred to as Lagrangian multipliers. The La-
grange dual, f(?), minimizes a linear combination of the solution energy and similarity
to the previous solutions. The Lagrange multipliers, ?, are non-negative and control
the amount of penalty incurred for violating the minimum dissimilarity constraints.
The following theorem holds for the Lagrange dual,
Proposition 2.2.1. The Lagrangian dual function, f(?), is a piece-wise linear function that
is concave in ? and is a lower-bound on the optimal value µ(m)
?
of the primal problem,
MModes(?,k), for all values of ? > 0.
Proof. The proof can be found in [11], and is reproduced here for completeness.
(i) The Lagrange dual can be written in the following form, f(?) = minµ aµ · ?+ bµ,
which shows that f(?) is a piece-wise linear function. Assume only two different
values of µ, namely µ1 and µ2, and corresponding linear functions, fµ1 and fµ2 ,
where fµj = aµj · ?+ bµj . We can then write the pointwise minimum f as,
f(?) = min{fµ1(?), fµ2(?)} (2.42)
Let 0 6 ? 6 1 and consider ?1, ?2 ? dom f. Then we have,
f(??1 + (1? ?)?2) =min {fµ1(??1 + (1? ?)?2), fµ2(??1 + (1? ?)?2)}
=min {?fµ1(?1) + (1? ?)fµ2(?2), ?fµ2(?1) + (1? ?)fµ2(?2)}
>?min {fµ1(?1), fµ2(?1)}+ (1? ?)min {fµ1(?2), fµ2(?2)}
=?f(?1) + (1? ?)f(?2), (2.43)
which shows that f(?) is concave in ?. We can easily extend this to k functions
fµ1 , . . . , fµk , for k finite to show the general result.
(ii) To see that f(?) is a lower-bound on the optimal value µ(m)
?
of the primal prob-
lem, MModes(?,k), consider a feasible point µ? of the primal problem. Then
?(µ?,µ(i)) ? km > 0 for all i ? [m? 1]. This gives,
L(µ?,?) 6
?
A?V?E
?A · µ?A, (2.44)
58 divmbest
where L(µ,?) is the Lagrangian,
L(µ,?) =
?
A?V?E
?A · µA ?
m?1?
i=1
?i(?(µ,µ(i)) ? ki). (2.45)
Therefore we have,
f(?) = min
µ?L(G), µA(xa)?{0,1}
L(µ,?) 6 L(µ?,?) 6
?
A?V?E
?A · µ?A. (2.46)
Since f(?) 6
?
A?V?E
?A · µ?A holds for all feasible points µ? it also holds for µ(m)
?
.
2.2.8 Diversity Functions
The DivMBest formulation in eqn. 2.40 relies on defining a dissimilarity function ?(·, ·)
between two solutions. It turns out that the DivMBest formulation is general enough
to include other methods as special cases, through the right choice of dissimilarity
function. Below are two such cases.
2.2.8.1 0-1 dissimilarity and M-Best MAP
If we let ?(µ,µ(i)) = [[µ 6= µ(i)]], where [[·]] is an indicator function which is 1 if the
predicate is true and 0 otherwise, and set ki = 1, such that the constraints in (2.40b)-
(2.40e) are of the form ?(µ,µ(i)) > 1 for i ? [m? 1], then we recover the M-Best MAP
problem. These constraints force the m-th solution to be different from each of the
previous (m? 1) solutions in at least one location.
2.2.8.2 Local dissimilarity and N-Best Maximal Decoding of Park and Ramanan
If we let ?(µ,µ(i)) = maxs?V ?n(µs,µ
(i)
s ), where there’s potentially a different dissim-
ilarity function, ?n, defined for each node in V , and set ki = 1, then we recover the
N-Best maximal decoding of Park and Ramanan [88]. In terms of the local measure of
dissimlarity, ?n, setting ki = 1 forces the m-th solution to be different from each of
the previous (m? 1) solutions at least one node.
Some dissimilarity functions can be decomposed according to the structure of the
graph which provide some nice properties. A specific class of decomposable dissimilar-
2.2 divmbest algorithm 59
ity functions that are used extensively in the experiments are dot-product dissimilarity
functions.
2.2.8.3 Dot-product dissimilarity
If we let ?(µ,µ(i)) = ?
?
s?V µ
T
sWµ
(i)
s and if the solution vectors, µ and µ(i) are
discrete, then ?(µ,µ(i)) encodes the weighted Hamming distance between the two
solutions, where the weights W capture the importance of various pairwise labellings
across the two solutions. If W = I (i.e. W is set to the identity matrix) then ? is the
straight-forward Hamming distance between the two solutions. It’s interesting to see
the form of the Lagrangian when ? is the general dot-product dissimilarity between
two solutions,
?
A?V?E
?A · µA ?
m?1?
i=1
?i(?(µ,µ(i)) ? ki) =
?
s?V
(
?s +
m?1?
i=1
?iWµ
(i)
s
)
· µs +
?
(s,t)?E
?st · µst +
m?1?
i=1
?iki. (2.47)
Note that f(?) is now comprised of the three terms in eqn. 2.47 where the first two
terms are simply the MAP problem of eqn. 2.38, with modified unary energies, and
the last term is independent of µ. When W = I there is a cost paid, proportional to ?i,
for setting local parts of the current solution, µs, equal to µ
(i)
s of each of the previous
i ? [m? 1] solutions. When W is non-identity the cost is spread over larger parts of
the assignment.
Thus in the case where the ?-function is a Hamming distance, since the first two terms
of f(?) are the same as for theMAP problem, and the last term is independent of the
minimization variables, we can compute f(?) using any MAP inference machinery
(exact or approximate) that was used to compute the first solution µ(1). Moreover, the
edge energies are left unchanged which means that certain classes of efficient MAP
inference — such graph-cut algorithms that require submodular edge potentials —
remain viable options for computing subsequent solutions.
2.2.8.4 Higher-order dissimilarity
Consider higher order dissimilarity functions of the form ?(µ,µ(i)) =
?
C?I?C(µC,µ
(i)
C ),
where C are subsets of variables, I is an index set on subsets of variables, and ?C(·, ·)
has some structure allowing for efficient message passing. Unlike dot-product dissim-
ilarity the higher-order dissimilarity does not decompose over nodes in the graph.
Examples include cardinality potentials [111], pattern-based sparse higher order po-
tentials [63, 95], and lower linear-envelope potentials [54]. We’ll describe how efficient
60 divmbest
inference on the ?-augmented energy of the Lagrangian dual function can be per-
formed in the next section.
2.2.9 Supergradient Ascent on Lagrangian dual function
As previously mentioned the Lagrangian relaxation, f(?), is a lower-bound on the
value of the primal DivMBest problem. We would like to find the ?? such that f(??)
provides the tightest lower bound on the value of the primal problem. If there is no
duality gap between the primal and dual problem values then strong duality holds and
solving the primal problem is equivalent to solving the Lagrangian dual relaxation. To
find the tightest lower bound on the primal problem we need to solve the following
Lagrange dual problem,
max
?
f(?) (2.48a)
s.t. ? > 0 (2.48b)
Recall that f(?) is a piece-wise linear function that is concave in ? (prop. 2.2.1). We can
solve problem 2.48 using a projected supergradient ascent algorithm (alg. 5) on ? [105].
Algorithm 5 Projected Supergradient Ascent (cf. [105])
1: t? 1
2: {?t | ?t > 0, limt?? ?t = 0, ??t=0 ?t =?}. . define sequence of step-sizes
3: Initialize ?(0)
4: f
(0)
best ? f(?(0))
5: repeat
6: ?(t) ? ?(t?1) +?t?f(?(t?1))
7: ?(t) ? [?(t)]+ . project onto positive orthant
8: f
(t)
best ? min{f
(t?1)
best , f(?
(t))} . keep track of best point found thus far
9: t? t+ 1
10: until lim
t?? |f(t)best ? f?| 6  . stopping criteria
In alg. 5 the supergradient of f at ?(t) is denoted by ?f(?(t)). In order to guarantee
convergence of the algorithm a convergent sequence of non-negative step-sizes, {?t},
has to be chosen such that limt?? ?t = 0, and ??t=0 ?t = ? (e.g. ?t = ?/?t, where
? > 0). In practice the stopping criteria on the last line of alg. 5 is such that if the value
of f(t)best does not improve the algorithm terminates.
Recall that f(?) is a point-wise minimum of a set of linear functions,
f(?) = min
µ
aµ · ?+ bµ, (2.49)
where the supergradient of f is ?f(?) = aµ?(?), with µ?(?)
.
= argminµ aµ · ?+ bµ.
2.2 divmbest algorithm 61
Proof. To see this consider the definition of the supergradient, namely g is a supergra-
dient of a concave function f at x ? dom f if
f(y) 6 f(x) + gT (y? x), ?y ? dom f. (2.50)
Consider f(? ?), for some ? ? ? dom f, which equals aµ?(? ?) · ? ?+ bµ?(? ?), where µ?(? ?)
.
=
argminµ aµ · ? ? + bµ, by definition. Clearly the following inequality holds for all ?,
f(? ?) = aµ?(? ?) · ? ? + bµ?(? ?) 6 aµ?(?) · ? ? + bµ?(?). (2.51)
We can add and subtract the quantity aµ?(?) · ? to the RHS to get,
f(? ?) = aµ?(? ?) · ? ? + bµ?(? ?) 6 aµ?(?) · ? ? + bµ?(?) +aµ?(?) · ??aµ?(?) · ? (2.52)
= f(?) +aµ?(?)(?
? ? ?), (2.53)
thus ?f(?) .= aµ?(?) is a supergradient of f at ?.
The supergradient of the Lagrangian dual function at ? is thus,
?f(?) = ?
????
?(µ?(?),µ(1)) ? k1
...
?(µ?(?),µ(m?1)) ? km?1
???? , (2.54)
where µ?(?) is optimal solution to problem 2.41 for the current setting of ?. The super-
gradient has an intuitive meaning in relation to the projected supergradient descent
algorithm presented in alg. 5. If at time t, a constraint is violated, say, ?(µ?(?(t)),µ(i))?
ki < 0, for some i — then the supergradient vector with respect to ?(t) will be positive
at index i and the update in alg. 5-line 6 will increase the cost, ?(t+1)i , for violating
the i-th constraint. Conversely, ?f(?) is negative for constraints that are satisfied, thus
reducing the corresponding costs, ?(t+1)j , for violating those constraints (because the
constraints are probably not active) and thus allowing for lower energy solutions.
One nice property is that in each iteration of alg. 5, the inference problem that is
needed to be solved for ?(t) is very similar to that used to solve ?(t?1). Thus warm-
starting the solver for ?(t) with the solutions ?(t?1) can be beneficial (e.g. re-using
search trees in graph-cuts [57], or reusing messages in dual-decomposition).
2.2.10 How tight is the Lagrange relaxation?
2 As mentioned earlier we’d like to find the tightest lower bound on the primal prob-
lem, MModes. We gave a Lagrange relaxation of MModes(?,k), termed f(?) which
2 Results in this section are due to Dhruv Batra [7]
62 divmbest
we showed to be a lower bound on the value of MModes(?,k) for all feasible u,
and all ? > 0. We know state the following result on the Lagrangian dual problem,
max?>0 f(?), which is the tightest lower bound on MModes(?,k).
Theorem 2.2.1. (i) The Langrangian dual problem, max?>0 f(?) is equivalent to solving
the following relaxation of MModes(?,k),
min
µ
?
A?V?E
?A · µA (2.55a)
s.t. µ ? co{µA(xA) ? {0, 1} | µ ? L(G)} (2.55b)
?(µ,µ(i)) > ki ?i ? [m? 1] (2.55c)
where co{·} is the convex hull of a set of discrete solutions.
(ii) Generally the Lagrangian relaxation is not guaranteed to be tight, but, for some dis-
similarity functions ?(·, ·), the convex hull cat be replaced with the discrete solutions
µA(xA) ? {0, 1}, µ ? L(G) themselves resulting in a tight Lagrangian relaxation.
Proof. (i) The result follows directly from the following equivalent LP-dual problems
shown by Geoffrion [36],
(dual) max
?>0
min
x>0
cTx? ?T (Ax?b) (2.56a)
s.t. Bx > d (2.56b)
xj ? I, j ? I, (2.56c)
and,
(primal) min
x
cTx (2.57a)
s.t. Ax > b (2.57b)
x ? co{x > 0, Bx > d, xj ? I, j ? I}, (2.57c)
where I is an index set over variables. Making appropriate substitutions gives the
desired result.
(ii) Recall from Chapter 1 that M(G) is the set of realizable marginal distributions
over graph G. Moreover in § 1.2 we mentioned the result that M(G) is the convex
hull of the overcomplete representation defined in eqn. 2.34 and eqn. 2.35 over the
finite index set in eqn. 1.43, where the indicator functions take on {0, 1} values and
are the extreme points of the polytope. Therefore M(G) is exactly co{µA(xA ?
{0, 1} | µ ? L(G)}.
It’s also a well known fact that minimizing a linear objective over a convex hull
has the optimal solution at some extreme point of the convex hull, therefore
it’s equivalent to minimizing over the discrete solutions (which are the extreme
2.2 divmbest algorithm 63
points). However, we also have the diversity constraints. The set of feasible solu-
tions for problem 2.55 are those in the set,
P
.
=
{
µ | ?(µ,µ(i)) > ki ?i ? co{µA(xA) ? {0, 1} | µ ? L(G)}
}
. (2.58)
Therefore when P is a polytope with integral vertices we can remove co{.} from
the constraints. Since co. has integral vertices we need to check whether ?(µ,µ(i)) >
ki introduce fractional vertices. When we have the M-Best MAP dissimilarity
function, Fromer and Globerson [34] presented spanning-tree inequalities that
are guaranteed not to introduce fractional vertices when G is a tree.
In general, though, when no assumption on ?(·, ·) are made, the Lagrangian relax-
ation is not guaranteed to be tight. Consider the dot-product (Hamming distance)
dissimlarity, ?(µ,µ(i)) = ?
?
s?V µ
T
sµ
(i)
s . This dissimilarity introduces fractional
vertices which was described as a counter-example by Fromer and Globerson [34],
Counter-example: Suppose we have a graph consisting of two nodes with an edge
between them, and each node takes on two labels. Let the node energies be
?1 = ?2 = (0, 0), and the edge energy be ?12 = (0, 10, 10, 10). The MAP solution
(minimizing solution over this graph) is (0, 0). To find the second best solution —
which is constrained to be different from the MAP solution with k = 1 — we in-
troduce the constraint, ?µ1(0) ? µ2(0) > ?1 =? µ1(0) + µ2(0) 6 1. The solution
minimizing the energy over the graph with this new constraint is (0.5, 0.5) with
energy value 5, wheres the other non-MAP solutions have energy value 10. Since
the solution is fractional the Lagrangian relaxation is not tight.
2.2.11 Computing Supergradient under different diversity functions
Recall that for certain diversity functions such as the Hamming distance dissimilar-
ity functions we can compute f(?) using the same MAP inference machinery (exact
or provably approximate) that was used to compute the first solution µ(1) because
imposing the dissimilarity function between solutions amounts to only modifying the
node energies but leaving the edge energies unaffected.
Not all dissimilarity functions share this decomposability property, especially when
dissimlarity is measured between subsets of variables, which we term higher-order
dissimilarity functions. However, there are some higher-order dissimilarity functions
where the individual terms over subset of variables, ?C(·, ·) (where C is a subset of
variables), have some structure that can be exploited in order to carry on efficient in-
ference over the ?-augmented energy. Here we mention how for such ?-augmented
energies, where the higher-order dissimilarities contain specific structure, efficient en-
64 divmbest
ergy minimization can be performed via dual-decomposition based message-passing
algorithms.
For simplicity of exposition, consider the Lagrangian relaxation to the 2MModes prob-
lem,
min
µ?L(G), µA(xA)?{0,1}
?
A?V?E
?A · µA ? ?1?(µ,µ(1)) ? ?1k1. (2.59)
and suppose ?(µ,µ(1)) is a higher-order dissimilarity function that does not decom-
pose according to nodes in the graph. Assume ?1 to be a fixed variable and let ?
(1)
hop(µ)
.
=
??1?(µ,µ(1)). Since ?1k1 is independent of µ the problem is reduced to,
min
µ?L(G), µA(xA)?{0,1}
?
A?V?E
?A · µA + ?(1)hop(µ). (2.60)
In contrast to dissimilarity functions that do decompose over nodes, even if the MAP
problem (eqn. 2.38) could be solved efficiently, this ?-augmented energy function is
difficult to solve because of the higher order potential, ?(1)hop. However, for certain
higher-order potentials with structure where messages can be efficiently computed
dual-decomposition based message-passing algorithms can be used to approximate
the supergradient.
2.2.12 Dual-Decompostition and the approximate supergradient for higher order potentials
In order to minimize energy in problem 2.60 we apply the dual-decomposition ap-
proach [8, 38, 64]. We introduce auxiliary variables for each of the optimization vari-
ables, µs in problem 2.60 and write the following equivalent problem,
min
µ, µhop
?
A?V?E
?A · µA + ?(1)hop(µhop) (2.61a)
s.t. µ ? L(G) (2.61b)
µhops = µs ?s ? V , (2.61c)
µA, µ
hop
A ? {0, 1} (2.61d)
where we are now minimizing over two sets of variables µ, and µhop which are con-
strained to agree. Introducing Lagrange multiplies ?s for each constraint in line 2.61c,
we can write the Lagrangian relaxation of problem 2.61 as,
g(?) = min
µ, µhop
?
A?V?E
?A · µA + ?(1)hop(µhop) ?
?
s?V
?s(µ
hop
s ? µs) (2.62a)
s.t. µ ? L(G) (2.62b)
µA, µ
hop
A ? {0, 1} (2.62c)
2.2 divmbest algorithm 65
We can rewrite the above objective as a sum of two separate minimizations, one over
the variables µ and the other over µhop,
g(?) = min
µ?L(G), µs,µst?{0,1}
?
s?V
(?s + ?s)µs +
?
(s,t)?E
?stµst (2.63a)
+ min
µ
hop
A ?{0,1}
?
(1)
hop(µ
hop) ?
?
s?V
?sµ
hop
s (2.63b)
To find the tightest Lagrangian relaxation we want to maximize g(?),
max
??Rn
g(?) (2.64)
which we can do using the supergradient method of alg. 5. The term in line 2.63a is
the original MAP problem with perturbed unary potentials, so the minimization over
µ can be carried out using the same efficient inference machinery used to compute
the MAP solution. The term in line 2.63b is a minimization over µhopA which is effi-
ciently computable for higher order potentials that have structure such as cardinality
potentials [39, 111], lower linear-envelope potentials [54] or sparse higher-order poten-
tials [63, 95]. For example, in the case of cardinality potentials Gupta et al. [39] and
Tarlow et al. [111] message-passing algorithms to compute them.
2.2.13 Setting k: the amount of diversity
The Lagrangian relaxation to the MModes(?,k) problem provides a trade-off between
minimizing the energy and the amount of diversity between solutions. Choosing the
value k relates to the minimum amount of diversity we want between solutions. Choos-
ing the right value for k is important because if the value of k is too small then the
next solution might not be outside the energy valley of one of the previous solutions.
On the other hand, too large a value for k could mean than several valid modes would
be ignored. Also note that for each value of k there is a different value of ? that min-
imizes the Lagrangian relaxation, ??(k) = argmin?(k)>0 f(?(k)). This means that we
would have to search for the optimal value of k, where for each value we’d have to
run the supergradient ascent algorithm, which is expensive. Alternatively we can di-
rectly do grid search over values of ?. This is analogous to tuning the regularization
parameter in learning. Since directly tuning ? is more efficient in practice, the amount
of diversity is tuned in the experiments found in later chapters using cross-validation
on ?, instead of directly searching over k.
2.2.14 Summary
To summarize, this chapter has presented the DivMBest problem which finds a diverse
set of highly probable solutions under a discrete probabilistic model. The DivMBest
66 divmbest
problem is a generalization of the M-best MAP problem. The DivMBest problem is for-
mulated as a Lagrangian relaxation of an integer linear program that involves solving
the ?-augmented energy minimization problem which minimizes a linear combination
of the energy and similarity to previous solutions. For certain classes of ?-function, the
modes of the underlying distribution can be computed using the same inference algo-
rithms that are used to compute the MAP solution.
The DivMBest method provides an alternative approach to image segmentation — in-
stead of devising complex models with higher-order terms that are hard to optimize
over one can use simpler models in which exact or approximate MAP inference is
tractable. With proper choice of ?-function the same inference machinery can be used
by the DivMBest algorithm to obtain a set of diverse solutions. This small set of seg-
mentations can then simply be evaluated by a more complex model in order to rank
them. We introduce the ranking mechanism in chapter 3.
The DivMBest approach is a greedy approximate strategy to finding a set of highly
probable and yet diverse solutions under the model. In contrast Kirillov et al. [50]
present the joint DivMBest problem which simultaneously finds all M segmentations
using an approximate solver that minimizes a single joint energy. In contrast to the
DivMBest formulation in this chapter their approach gives better quality results at the
cost of significantly slower run time. For submodular energies Kirillov et al. [51] later
propose an exact solver which is efficient albeit slower than the sequential approach.
This is extended in [52], specific to binary submodular energies, to give a solver that
is faster than the sequential approach presented in this chapter.
3
D I V M B E S T + R E R A N K
There are many confounding factors that make semantic segmentation an inherently
difficult task — from inter and intra object occlusion to lighting and varying appear-
ance and pose. A segmentation algorithm will confront all these sources of uncertainty.
However, devising fully probabilistic models that can incorporate all confounding fac-
tors in order to reason about the distribution over all possible segmentations jointly
is usually intractable. This leads to two separate approaches to devising segmentation
models. We can either build
1. Restrictive Probabilistic Models that can make efficient joint predictions over a pos-
terior distribution of all variables of interest at the cost of limited prediction
capacity due to simplifying independence assumptions, or
2. Expressive Feed-Forward Models that can incorporate more complex interaction of
variables by using simple feed-forward predictions but propagate uncertainty by
not modelling all the variables in a probabilistic joint-prediction framework.
Semantic segmentation models that fall into the first approach include Conditional
Random Field (CRF) models such as [9, 60, 68]. To make joint prediction on all vari-
ables in a CRF tractable simplifying independence assumptions are usually made such
as only local variable interactions that are associative or attractive [68]. The second ap-
proach includes feed-forward pipelines like [4, 15, 37] that find regions that are scored
and then combined into a segmentation. The feed-forward approach can incorporate
rich dependencies between regions that are difficult to capture in a tractable CRF, but
errors propagate and accumulate in the pipeline.
This chapter introduces a two-stage hybrid approach called DivMBest+ReRank that
leverages both approaches. The first stage consists of a tractable probabilistic model
that reasons about an exponentially large output state-space and makes joint predic-
tions — but crucially outputs a diverse set of plausible segmentations not just a single
one. The second stage of the approach is a discriminative re-ranker that is free to use
arbitrarily complex features, and attempts to pick out the best segmentation from this
set. Figure 3.1 gives an illustration of this approach.
DivMBest+ReRank approach to semantic segmentation has several key advantages:
• Global optimization over a simple model. The first stage of this approach is able
to perform global optimization over all the variables of interest, in a tractable
67
68 divmbest+rerank
Figure 3.1: An overview of the
DivMBest+ReRank approach.
In Stage 1 diverse segmentations
are computed from a tractable
probabilistic model. These are fed
to a large-margin re-ranker in Stage
2. The top re-ranked segmentation
is returned as the final solution.
Even though the most probable
segmentation from Stage 1 is incor-
rect, the set of segmentations does
contain an accurate solution, which
the re-ranker is able to score to the
top.
albeit imperfect model to find a small set (? 10 ? 30) of plausible hypotheses.
Experimentally we find that typically at least one of these solutions is highly
accurate.
• Rich (higher-order) features in re-ranker. Since the number of segmentations that the
re-ranker needs to consider is small we do not have to worry about tractability
issues when designing re-ranker features. The re-ranker is free to use arbitrarily
complex features that would be intractable to add to the probabilistic model
in the first stage. This is because the re-ranker does not need to optimize over
all possible segmentations but merely evaluate these features on a small set of
solutions.
• Discrimination only within the set. The re-ranker features need not be globally dis-
criminative over all possible segmentations, rather only locally discriminative
within the set returned by the first stage. Specifically, for the re-ranker the goal
is not to identify generic good segmentations but use features that can help it
discriminate good solutions from bad ones within a small set.
3.0.1 Contributions
The main contribution presented in this chapter is a discriminative re-ranking for-
mulation for semantic segmentation. Our algorithm takes as input a set of labellings
{y(1), . . . ,y(m)} for an image and predicts the most accurate labelling from this list.note the change of
notation from x(i)
to y(i).
3.1 related work 69
The learning task is formulated as a Structured SVM (SSVM) [114], where the task loss
penalizes the re-ranker for deviating form the most accurate solution in this set.
3.1 related work
The DivMBest+ReRank approach to segmentation that is presented in this chapter is
similar in spirit to the Constrained Parametric Min-Cuts (CPMC) approach of Carreira
et al. [16], that was reviewed in § 1.1.1. CPMC produces a small set of high quality
object segmentation proposals over an image that are scored according to how likely
they are to be of an object. In the first stage of CPMC an large set of overlapping
candidate figure-ground segmentation proposals are generated (using a bottom-up
segmentation method) that are further pruned to remove redundant segmentations
and finally ranked and culled in the second stage. The ranking is done by a regressor
that is trained to map mid-level features, computed over the image and the figure-
ground segment proposal, to the largest overlap the segment has with a object in the
image (measured against ground-truth), quantifying the "objectness" of the proposal.
Key to the approach is the concept of reducing the solution space (space of generated
figure-ground regions) in the first stage. As opposed to building a model of object
segmentation that includes both local and global interaction terms to capture the large
scope of dependencies between regions in the image, the approach uses simple mod-
els in the first stage that incorporate only local interactions. This avoids the intractable
nature of complex models with high-order terms and serves as a filter that reduces
the state space to a much smaller set. The elements of this set have good alignment
with image contours — a feature that can be captured using local interactions alone.
In this restricted solution space the ranker in stage two can use features that capture
more global properties that would potentially be intractable or at best inefficient to
compute over the original exponential state space — global "objectness" features such
as convexity and smoothness of region boundaries, eccentricity, and other gestalt fea-
tures. An important property of the filtering stage is that it maintains high-recall, pre-
serving high-quality segmentations, but reduces the state-space enough (i.e. reduces
false-positive rates) so that higher-order processing is tractable on the remaining solu-
tions.
Given that adjacent ranked segments tend to be very similar to each other a diversi-
fying model is also incorporated in the second stage of [16] using Maximal Marginal
Relevance (MMR) [14]. MMR is a sequential procedure — starting with the top-scoring
segment the next segment is chosen by MMR that maximizes the original ranking score
minus a score for having a redundant segment that is based on amount of overlap with
the previously selected segments.
70 divmbest+rerank
3.1.1 Relation to cascade approaches
Related to the DivMBest+ReRank approach are cascade models that consist of mul-
tiple stages of successively more complex inference models. Starting with a relatively
simple model the stages progressively prune the output state-space in order to speed
up inference and increase prediction accuracy. Cascade models accomplish this by
leveraging the computational efficiency of simple models in the initial stages to filter
out the majority of examples that are easy to discriminate. In further stages the cas-
cades benefit from the predictive power of much more complex models, that though
expensive to compute on the original state space, become tractable for a small set of
solutions. The learnable model in each cascade stage is trained on the filtered output
from the previous stage. Therefore negative examples that reach later stages (i.e. exam-
ples that pass the filtering of all the earlier stages) tend to be harder, and training the
complex models in further stages to focus on discriminating these examples can lead
to lower false-positive rates and improved performance. Key to any cascade approach
is the balance between the following two criteria,
1. Accuracy: Minimize the number of errors made by each stage of the cascade
to ensure accurate inference in subsequent models. This is so that the errors
propogated to later stages is minimized.
2. Efficiency: Reduce the output state space of each stage so that inference in subse-
quent models can be done more efficiently.
This section reviews a number of relevant cascade models for vision tasks that are
learned to balance these two criteria.
3.1.1.1 Face detection cascade
One of the early works to show effective use of cascade models was Viola and Jones’
face detector [117]. There a classifier cascade consisting of multiple consecutive stages
of simple to more complex classifiers is used to increase the efficiency of the detector
(compared to using a single complex classifier) while simultaneously increasing the
performance. Each classifier in the cascade takes as input features computed within
a sub-window of the image, and predicts whether or not the sub-window contains a
face. The dictionary of features can be very large and so too can the number of features
computed within any sub-window. In each stage the classifier is a combination of a
set of weak classifiers that has been trained using AdaBoost [32]. In [117] they restrict
the weak learners to be classifiers that each depend on a single feature. Thus boosting
amounts to a selection mechanism over the dictionary of features, retaining only the
most informative features for classification.
3.1 related work 71
Although using boosted classifiers itself reduces classification time compared to classi-
fiers that depend on many more features the detection performance tends not to be as
good. To improve on this Viola and Jones introduce the face detection cascade (i.e. clas-
sifier cascade) which is a simple approach to improve detection performance while
giving low run-time complexity. The idea is to construct a cascade of progressively
more complex classifiers, where complexity is in terms of number of features used by
the classifier. The simpler classifiers at the early stages of the cascade are trained to
detect all positive sub-windows while rejecting as many negative sub-windows as pos-
sible. In this way the early stage classifiers, that are more efficient to run, filter out the
majority of the state space (the state space is all possible sub-windows in the image)
so that the more complex classifiers in later stages, which are more computationally
expensive, can focus on the task of classifying a much smaller set of sub-windows.
The classifier in each stage of the cascade is trained with Adaboost, on training data
constructed from sub-windows that have passed the previous stages. Since the sub-
windows that pass earlier stages are harder to classify than the typical instance the
classifiers further along the cascade have a more difficult task. As a result they give
higher false-positive rate for a fixed value of recall (i.e. true-positive rate). To train the
detector each stage in the cascade is trained by progressively increasing the number
of features the classifier in a stage uses until the desired minimum false-positive rate
and maximum true-positive rate are achieved on a held out set. To illustrate, Viola
and Jones train a cascade with an initial stage consisting of a two feature classifier that
remove 50% of the non-face sub-windows while retaining 99% of the sub-windows
containing faces, at a 50% false-positive rate. The next stage classifier uses ten features
and filters 80% of the non-face sub-windows while retaining 100% of the faces. Further
stages use more and more features until the false-positive rate is virtually zero while
maintaining a high true-positive rate. The result is a high performance face detector
that is efficient to run at multiple-scales over the image.
3.1.1.2 Structured prediction cascades
The Structured Prediction Cascades of Weiss et al. [121, 122] provide a general frame-
work for tractable and efficient MAP inference in structured output models (e.g. graph-
ical models) with either high tree-width or large output state space. The idea is to
reduce the state space by removing clique assignments that do not correspond to
the MAP assignment. The Structured Prediction Cascade consists of sequential stages
that take as input a set of possible assignments to cliques in the model and prune
some of the clique assignments before passing the remaining to the next stage. Subse-
quent stages consist of increasingly more complex models. Model complexity can be
achieved either by considering higher-order cliques, using more complex features, or
starting with a coarsened state space and successively refining it in subsequent stages.
In each stage pruning is done by running inference on the stage’s model and identi-
fying states that will be pruned. Pruning occurs based on the max-marginals of the
72 divmbest+rerank
model. Recall that the max-marginal on a clique c of a model capturing some joint
distribution f(·) (alternatively an energy or score) is defined as,
f?(xc)
.
= max
x ??Xn
{f(x ?) : x ?c = xc} (3.1)
where xc is the portion of complete assignment x that is associated with variables in
clique c. The max-marginal is the maximum probability/score of any assignment that
agrees on the clique assignment xc. The cascade stages filter out any clique assignment
xc for which f?(xc) 6 t? for some tuned stage specific threshold t?. Consider the score,
f(x), of a joint assignment x. Note that if f(x) > f?(x ?c) then x 6= x ?c, which follows
from the definition of f?(x ?c). This implies a safe filtering property [122]: if f(x) > t?
for some t?, then for all xc, f?(xc) > t? (since f?(xc) > f(x) > t?). Therefore as
long as f(x) > t? pruning clique assignments for which f?(xc) < t? will not remove
the optimal clique assignments. Threshold t? is defined as a convex combination of
the MAP assignment score and the mean of the max-marginal score with combination
weight ? (0 6 ? 6 1), set to minimize the filtering error (number of correct clique states
pruned by a stage). The threshold is similar to a quantile of the max-marginal values.
Adjusting ? is a trade-off between efficiency (i.e. aggressive pruning) and accuracy.
Each stage is learned independently and sequentially using stochastic sub-gradient
descent on the model parameters ? (recall f(x) = ??,?(x)?) in order to minimize the
filtering error. The threshold for each stage is set via cross-validation on ?.
If inference over the models in each stage is intractable an ensemble method [122] is
proposed that breaks the model into a collection of sub-models (e.g. graph reduced
to sub-graphs collectively covering all nodes and edges) for which exact inference
is tractable. A similar analysis to the case of a single model leads to an equivalent
approach for filtering by thresholding the sum of max-marginals computed over the
indivual sub-models (cf. [122]), and analogous joint safe filtering property. Significant
performance improvement on a number of vision tasks where the structured output
has very high state-space, such as articulated pose estimation [101], show the perfor-
mance benefits of the cascade approach.
3.1.2 Relation to proposal-generation methods
Similar to the DivMBest+ReRank pipeline are a number of segmentation methods
which produce an initial pool of segmentations of the image that are subsequently
ranked according to how well they segment the objects in the scene. We mention a few
noteworthy methods.
The category-independent object proposal and diverse ranking method of Endres et
al. [27] that was review in § 1.1.1 produces a diverse set of object segmentations which
align well with object boundaries. The final region proposal can be used to automati-
cally localize object in the image for further processing for recognition, or can alterna-
3.1 related work 73
tively be used to provide improved spatial support, compared to bounding boxes, for
detection tasks.
Russell et al. [97] use a pool of global segmentations of images to learn object categories
and their segmentations. Their approach can be viewed as a two-stage strategy of pro-
ducing a diverse set of segments in the first stage followed by ranking the segments
according to how well they match each of the discovered categories. More specifically,
the approach (cf. [97]) uses the normalized-cut segmentation algorithm (cf. § 1.1.1) to
produce a pool of candidate global segmentations of the image. For each segment in
the pool of segmentations a histogram of visual words is computed. To concisely repre-
sent the segments a dictionary of visual words is constructed by computing SIFT [76]
descriptors over the images of a held-out dataset which are clustered using k-means
clustering. Each cluster center represent a visual word. Visual words can be used to
represent an image or a region in an image. By using the dictionary to quantize the
SIFT descriptors over an image into visual words a representation for a segment is
built by computing the histogram of visual words contained in the segment. Given
this bag of visual words representation for each segment statistical text analysis models
are used to learn topics (i.e. object categories) from the pool of segments. A topic is thus
a visual word histogram as well. Thus a segment can then be represented by a mixture
of the discovered topics. For each of the discovered object categories the segments are
subsequently ranked by how well a segment matches the visual word (KL-divergence
between corresponding visual word histograms).
Another related approach is used to perform object segmentation with category-independent
shape priors [49]. Multiple segmentations of an image are considered since an image
might contain multiple objects, coupled with the fact that shape priors are imperfect
so multiple competing hypotheses might exist. The shape priors are constructed by
aggregating multiple learned category-independent shape priors. Each segmentation
problem is initialized with one of the aggregated shape priors and a binary-labeling
of the image is inferred (via graph-cut inference). Producing a pool of object segmen-
tations increases the chance that at least one of the shape priors is useful in producing
a high-quality object segmentation. Similar to CPMC (cf. § 1.1.1) classifiers can be
trained to rank the object proposals.
3.1.3 Discriminative re-ranking in other domains.
Discriminative re-ranking of multiple solutions is also a common approach found in
domains such as speech [22, 26] and natural language processing [21, 90, 103].
74 divmbest+rerank
3.2 divmbest + re-rank
1 In § 2.2.6 we presented the DivMBest algorithm for producing a diverse set of m
highly probable segmentations from a discrete probabilistic graphical model, such
as a CRF. Often we would like to return the single best segmentation of the image
from this diverse set — that is we’d like an algorithm that can perform a 1-out-of-m
inference task. This section presents a novel two-stage approach to ranking the diverse
segmentations produced by the DivMBest algorithm presented in chapter 1, called
DivMBest+ReRank. In the first stage a probabilistic model generates a set of diverse
plausible segmentations. In the second stage, a discriminatively trained re-ranking
model selects the best segmentation from this set. The re-ranking stage can use much
more complex features than what could be tractably used in the probabilistic model,
allowing a better exploration of the solution space than possible by simply producing
the most probable solution from the probabilistic model.
3.2.1 Notation
In chapter 2 we denoted a segmentation (equivalently an assignment of labels to n
(super)pixels or regions) by a vector x where x = {x1, . . . , xn} ? Xn, Xn .= ×s?[n]Xs —
where Xs is the set of labels for region s.
In this chapter we will make a change of variable for the assignment vectors. For the
first stage (which produces DivMBest candidate segmentations) let y = {y1, . . . ,yn} ?
Yn be a segmentation of the image, where the space of labellings is Yn .= ×s?[n]Ys,
and Ys is the set of labels for region s. Recall the DivMBest formulation for finding
the m-th best diverse segmentation (i.e. MModes problem),
µ(m) = argmin
µ?L(G), µA(yA)?{0,1}
?
A?V?E
?A · µA (3.2a)
s.t. ?(µ,µ(i)) > ki ?i ? [m? 1]. (3.2b)
Note that the indicator vectors, µ, encode the label assignment to vectors y, i.e. (µj(`) =
1) =? (yj = `). Therefore we can define a mapping v : {0, 1}d ? Yn from d-
dimensional indicator vector µ to labelling y: y = v(µ). Given µ(m) the corresponding
segmentation is y(m) = v(µ(m)). Let Yi = {y
(1)
i , . . . ,y
(m)
i } denote the set of m diverse
segmentations of the i-th image. At training time, the input to the second stage is a set
of (image, ground-truth, segmentation-set) triples — {xi,y
gt
i ,Yi | i ? [N]}, where xi is
the i-th image and ygti is the corresponding ground-truth segmentation. The quality
of a segmentation is measured by a loss function, `(ygti , y?), that returns the cost of
predicting y? when the ground-truth is ygti .
1 The contributions to the thesis presented in this section are found in [125], and are in collaboration with
Gregory Shakhnarovich and Dhruv Batra.
3.2 divmbest + re-rank 75
Let y(?)i denote the most accurate segmentation in the set Yi — that is,
y
(?)
i = argmin
y?Yi
`(ygti ,y). (3.3)
The re-ranker uses features ?(x,y) : R3×w×h × Yn ? Rp that are computed on the
image x, and corresponding segmentation y. The score of the re-ranker on segmenta-
tion yi of image xi is denoted by,
Sr(yi) = ?
T?(xi,yi), (3.4)
where ? are the p-dimensional re-ranker parameters.
3.2.2 Re-ranker model
As mentioned above the re-ranker is modelled as a linear combination of features,
?(x,y) computed on the image x and corresponding segmentation y, which assigns a
score to each segmentation: Sr(y) = ?T?(x,y). Inferring the best segmentation under
the re-ranker corresponds to computing the highest score,
y?i = argmax
y?Yi
Sr(y). (3.5)
Re-scoring the segmentations using a ranker has a couple of benefits,
1. Can use more complex features than segmentation model: The re-ranker features ?
can be different from the features used in the model that generated the segmen-
tations. In fact they can be quite complex and expensive to compute. The reason
for this is that the re-ranker only needs to compute features on a relatively small
set of candidate segmentations in contrast to the exponential number of segmen-
tations that have been pruned by the first stage. Inference in the second stage is
simply taking a dot product of the features with the re-ranker parameters ? and
sorting the resulting scores. Hence we can afford to compute computationally
expensive re-ranker features.
2. Can incorporate features that are intractable to include in the segmentation model: In the
first stage the segmentation model can only compute features on the image or
segmentation that are tractable. Incorporating higher-order interactions between
regions into the model would result in potentials that could make inference over
the model intractable. Hence incorporating performance limiting dependencies
between variables are typically avoided in segmentation models. In contrast,the
re-ranker features are a function of both the image xi and segmentation yi. That
means we can compute features like size of various categories, connectivity of
the label masks, relative location of the label masks, and other such quantities
that are functions of global statistics of the segmentation and thus intractable to
include in the first stage.
76 divmbest+rerank
3.2.3 Re-ranker loss
To train the re-ranker we need a measure of performance. Let L(ygti ,y) be the re-
ranker loss. Earlier we mentioned that the quality of a segmentation y? predicted by the
re-ranker as being captured by the task loss `(ygti , y?). Thus we could use the task loss
as the re-ranker loss, i.e. L(ygti , y?)
.
= `(ygti , y?i). However , using `(y
gt
i , y?) has a draw-
back. Consider the following case: we are given two images i, jwith two segmentations
each, and corresponding accuracies Acc(Yi) = {95%, 75%} and Acc(Yj) = {40%, 35%}.
When the re-ranker loss is set to the task loss, for Yi we have that the loss on the two
segmentations are {100? 95%, 100? 75%} = {5%, 25%} whereas for Yj the re-ranker
incurs much higher losses {100? 40%, 100? 35%} = {60%, 65%}. This means that the
re-ranker will focus on picking the best segmentation in set j and ignore how well
it does on set i. This is undesirable because set j segmentations are all of relatively
the same (albeit poor) quality. Given that we are committed to the set, if the re-ranker
makes a poor selection for the best segmentation in the set the cost incurred is only
5% compared to if the re-ranker had made the correct choice. On the other hand set i
contains segmentations that are of very different qualities — the re-ranker will incur a
20% cost if it makes the wrong choice. Clearly it would be better for the re-ranker to
focus attention on making the correct choice on set i instead of set j.
In order to shift the re-ranker to focus its effort on training instances where it is under
performing relative to the set the following relative re-ranker loss is proposed,
L(ygti , y?i) = `(y
gt
i , y?i) ? `(y
gt
i ,y
(?)
i ). (3.6)
Using the relative loss in eqn. 3.6 gives losses: {5? 5%, 25? 5%} = {0%, 20%} for set i
and {60? 60%, 65? 60%} = {0%, 5%} for set j — this shifts the focus to set i because
an incorrect choice in that set is much costlier (difference of 20%) than an incorrect
choice in set j (difference of 5%). Using the relative loss compared to the task loss was
found empirically to play an important role in the performance of the re-ranker.
3.2.3.1 Re-ranker Training
Note that it is not necessary for the re-ranker to produce a scoring that induces a total
ordering of the segmentations in set Yi. We only desire the re-ranker to assign the best
segmentation in Yi a higher score than the other segmentations in the set, i.e. we desire
Acc(y?) > Acc(y) for all y ? Yi \ y?, where y? is defined in eqn. 3.5. Thus we want to
learn parsimonious re-ranker parameters ? such that for image i,
?T?(xi,y
(?)
i ) ??
T?(xi,y) > ? ?y ? Yi \y(?)i , (3.7)
where y(?)i is the best segmentation in the set Yi, and ? > 0 is some margin.
3.2 divmbest + re-rank 77
Then object in eqn. 3.7 coincides with the following quadratic program (QP),
max
??R
C ·
?
i?[N]
?
y?Yi\y(?)i
[
?T (?(xi,y
(?)
i ) ??(xi,y)) ? ?
]
? ||?||1, (3.8)
where the first term encourages the best segmentations to be scored higher than the
other segmentations for each image, and the `1-penalty term is a regularization on ?
in order to reduce over-fitting on the training set by producing a parsimonious (i.e.
sparse) representation of the features. The scalar value C balances the importance of
the two terms. Introducing scalar slack variables for each image in the first term in the
objective of problem 3.8 we can write it as,
max
?, ?i
C ·
?
i?[N]
?i ? ||?||1 (3.9a)
s.t. ?T (?(xi,y
(?)
i ) ??(xi,y)) > ?+ ?i ?i ? [N], ?y ? Yi \y
(?)
i , (3.9b)
?i > 0 ?i ? [N]. (3.9c)
Re-writing problem 3.9 as a minimization and replacing the `1-loss (because it’s not
differentiable) with the `2-loss we get,
min
?, ?i
1
2
||?||22 +C ·
?
i?[N]
?i (3.10a)
s.t. ?T (?(xi,y
(?)
i ) ??(xi,y)) > ?? ?i ?i ? [N], ?y ? Yi \y
(?)
i , (3.10b)
?i > 0 ?i ? [N]. (3.10c)
If we let ? = 1 (any choice of ? 6= 0 can be incorporated by the magnitude of ?) and
rescaling the slack variables by L(ygti ,y) gives the familiar Structured SVM QP [48],
min
?, ?i
1
2
||?||22 +C ·
?
i?[N]
?i (3.11a)
s.t. ?T (?(xi,y
(?)
i ) ??(xi,y)) > 1?
?i
L(ygti ,y)
?i ? [N], ?y ? Yi \y(?)i , (3.11b)
?i > 0 ?i ? [N]. (3.11c)
Intuitively we can see that constraint 3.11b tries to maximize the (soft)margin between
the score of the oracle solution and all other solutions in the set. Importantly, the slack
(or violation in the margin) is scaled by the loss of the solution. Thus if in addition to
y
(?)
i there are other good solutions in the set, the margin for such solutions will not be
tightly enforced. On the other hand, the margin between y(?)i and bad solutions will be
very strictly enforced. We solve problem 3.11 via the 1-slack cutting-plane algorithm of
Joachims [48] which we re-produce in alg 6 for reference. In each iteration the cutting-
plane algorithm finds the segmentation that most violates the margin constraint on
each image, i.e. lines 5- 7 of alg. 6 and add it to the working set (line 8). In the 1-slack
78 divmbest+rerank
Algorithm 6 1-slack cutting-plane algorithm for training Structural SVM with slack-
rescaling [48]
1: Input: D = {(xi,y
gt
i ,Yi) | i ? [N]}, C, 
2: W? ? . initialize working set of constraints
3: repeat
4: (?, ?)? argmin
?,?>0
1
2
?T?+C? (3.12a)
s.t. ?(y?1, . . . , y?N) ?W : (3.12b)
1
N
?T
?
i?[N]
L(ygti , y?)(?(xi,y
(?)
i ) ??(xi, y?i)) >
1
N
?
i?[N]
L(ygti , y?i) ? ?
(3.12c)
5: for i = 1, . . . ,N do . find most violated constraints
6: y?i ? argmax
y??Yi
{
L(ygti , y?i)
(
1??T
[
?(xi,y
(?)
i ) ??(xi, y?i)
])}
7: end for
8: W?W? {y?1, . . . , y?N} . add corresponding segmentations to constraint set
9: until 1N?
T
?
i?[N]
L(ygti , y?i)(?(xi,y
(?)
i ) ??(xi, y?i)) ?
1
N
?
i?[N]
L(ygti , y?i) 6 ?+ 
10: return (?, ?)
formulation we add a single constraint in each iteration (constraint 3.12c) consisting of
the average loss re-weighted margin constraints. Notice that in problem 3.12 of line 4
in alg.6 there is a single slack variable ? instead of an ?i for each image as in the
original SSVM QP (i.e. problem 3.11).
At test time we compute stage-1 features ? on an image and segmentation model
potential functions ?A, which we use to run the DivMBest algorithm to produce a set
of diverse segmentations Y . We compute re-ranker features ? on this set and score
each segmentation using 3.4, returning the highest scoring solution (i.e. perform re-
ranker inference in eqn. 3.5).
3.2.4 Summary
This chapter has presented a two-stage approach to segmentation: produce a set of
diverse segmentations from a discrete probabilistic model, then re-rank them using
a discriminative re-ranker formulated as a structural SVM. The re-ranking stage can
use arbitrarily complex features, such as global features that are computed over the
entire image or solution, in order to evaluate the best segmentation in the set. The first-
stage filters the exponential space of possible segmentations to a small set of highly
plausible solutions that are not merely minor perturbations of each other. The second-
3.2 divmbest + re-rank 79
stage can focus on the best-out-of-m inference task on a much reduced space, and
thus only needs to compute features that are relevant in discrimination within the set.
In chapter 5 we evaluate the performance of this approach on a number of semantic
segmentation tasks.

4
D I V M B E S T E X P E R I M E N T S
4.1 evaluating divmbest segmentations
We look at a number segmentation tasks and investigate the quality of the DivMBest
segmentations against the MAP solution produced by the respective underlying seg-
mentation models.
4.1.1 Baselines
We evaluate the DivMBest segmentations against a number of baselines:
• M-Best MAP — The method of Yanover and Weiss [127] is used to produce a
set of low energy (i.e. high probability) solutions, where the solutions are only
constrained to be different on at least a single label assignment. There is no addi-
tional characterization of diversity between solutions.
• Random — Multiple solutions can be generated without any optimization as
well. A new solution is created by taking a subset of the nodes in the MAP
solution at random and changing their label assignment to the next best label
according to the node min-marginals. Repeating this process produces a set of
segmentations.
• Confidence — Similar to how the random segmentations were produced except
the nodes are selected based on a confidence measure. A subset of the nodes with
highest entropy (according to their min-marginals) are selected and their label
set to the next best value.
For each solution µ of the DivMBest algorithm, let d(µ,µ(1)) denote the number of
places that µ differs from the MAP solution µ(1). In order to have a fair comparison
between the perturbation based baselines and the DivMBest solutions, for each solu-
tion µ produced by DivMBest we generate a perturbation based solution that differs
from the MAP solution in exactly d(µ,µ(1)) locations. This ensures that the solutions
generated using random or confidence perturbations have an equal measure of diversity
compared to the DivMBest solutions.
81
82 divmbest experiments
4.1.2 Oracle Solution
In order to evaluate the upper-bound on the quality of the segmentations in the
DivMBest set we can compute the oracle solution. Given a set of segmentations for
the ith image, {x(1)i , x
(2)
i , . . . , x
(m)
i }, and corresponding ground-truth segmentation x
gt
i ,
let the segmentation accuracy w.r.t ground-truth be denoted as Acc(x(k)i , x
gt
i )
1. The1 Acc(·, ·) can be
intersection-over-
union score between
two segmentations
or any relevant
measure on accuracy
of a predicted
segmentation with
respect to
ground-truth.
oracle segmentation, x(?)i , is defined to be the segmentation within the set that achieves
maximum segmentation accuracy, i.e.,
x
(?)
i = argmin
xi?{x(1)i ,...,x
(m)
i }
Acc(xi,yi). (4.1)
4.2 interactive segmentation
1 Recall from § 1.1 that in interactive segmentation the user is interested in cutting out
the foreground object from the rest of the image via annotations like scribbles [12] or
bounding boxes [93]. The problem is typically formulated as a figure-ground segmen-
tation task where some model variables are fixed according to the user annotations.
In each round the MAP solution is computed and presented to the user, at which
point the user provides additional supervision and the MAP solution is updated. This
process is repeated until the MAP solution is acceptable. Instead of showing a single
cutout each round the number of user interactions could be minimized by having the
interface show a set of possible cutouts for the user to pick from. Ideally we’d like an
algorithm that can efficiently produce a small set of diverse solutions.
4.2.1 CRF Model
Consider the image-scribble pair (X, S), where each image is a collection of n super-
pixels X .= {Xs | s ? [n]}. Let x = {xs | s ? [n]} be a corresponding label assignment to
all the superpixels in the image, where each superpixel takes on either foreground or
background label, i.e. xs ? {fg, bg}. A subset S ? [n] of the superpixels have known
label according to the scribbles, i.e. the superpixel labels {xs | s ? S} are assigned ac-
cording to the manually provided scribbles. Alternatively pixels could have been used
but superpixels were preferred for computational efficiency reasons, and better align-
ment of segmentations to internal image boundaries. We chose the SLIC algorithm [1]
to generate superpixels, with the desired number of superpixels in an image set to
1 The contributions to the thesis presented in this section are found in [7], and are in collaboration with
Gregory Shakhnarovich and Dhruv Batra.
4.2 interactive segmentation 83
Features
Color [44]
C1: RGB mean values
C2: C1 in HSV colorspace
C3: Hue histogram and entropy
C4: Saturation histogram and entropy
Texture
T1: Histogram of gradients (HOG) [24]
Local
L1: Histogram of SIFT [76] codewords
Table 4.1: Superpixel features used to learn the appearance model for the interactive segmen-
tation figure-ground cutout model.
3000. The average image in our dataset contains ' 150K? 200K pixels. To model the
figure-ground segmentation problem we build a graph G = (V ,E) over the superpixels
and define a pairwise CRF with the following energy,
E(x; A) .=
?
s?V
???s(xs; A) + ? · ?
t?N(s)
?st(xs, xt)
?? , (4.2)
where N(s) are the superpixels adjacent to superpixel s in the image. The data term is
the cost of assigning a superpixel to foreground or background, and it depends on the
appearance model A that’s learned from the user scribbles S. The pairwise smoothness
term penalizes neighboring superpixels being assigned different labels.
4.2.1.1 Data term
The data term depends on an appearance model A that is based on the output of
a Transductive SVM (TSVM). The appearance model is learned by extracting features
?(Xs)from labelled and unlabelled superpixels and training a TSVM [109] to predict if
a superpixel belongs to foreground or background. The features include the low level
color, texture, and local cues listed in table 4.1. Let the superpixel score for belonging
to foreground be score(Xs) = wT?(Xs), where w is the learnt weight vector of the
TSVM. We define the foreground data term energy as,
?(xs = fg)
.
=
{
? if score(Xs) > 0,
?? 1 otherwise
(4.3)
84 divmbest experiments
where ? = 0.5e
?|score(Xs)|
2
??2 , ?2 = Var({score(Xs) | s ? S}), and ? is a constant that is
set via cross-validation on a held out set that is kept the same for all images. The
background energy is then simply,
?(xs = bg)
.
= 1? ?s(xs = fg). (4.4)
4.2.1.2 Smoothness term
The smoothness term is a contrast sensitive Potts energy [12] that penalizes adjacent
labels taking different labels. The penalty is proportional to how similar the two su-
perpixels are in feature space. The more similar the features are the more penalty is
paid,
?st(xs, xt)
.
= ?xs 6=xt ·?1 · e??2dst (4.5)
where dst is the distance between the feature vectors of superpixels s and t, and the
scale parameters are set to, ?1 = 2, ?2 =
?
max{dst}
20 .
4.2.1.3 Inference
The figure-ground interactive segmentation problem amounts to finding the assign-
ment to unlabelled superpixels that minimizes the energy in eqn. 4.2. Note that this
is a binary (i.e. two-label) contrast sensitive Potts model with submodular pairwise
energy terms, for which efficient graph-cut algorithms exist to compute the exact MAP
solution in polynomial time [13, 57].
4.2.1.4 Data + training
To evaluate the interactive segmentation model, and its DivMBest extension described
next, evaluation was performed on 100 images from Pascal VOC2010. For each im-
age scribbles marking foreground objects and background regions were manually pro-
vided. Fifty of the images were used for tuning the parameters and the rest were used
for reporting test accuracy. The weight on the smoothness term (?) was tuned by doing
grid search in the range [0, 1]. The best setting on training images was achieved with
? = .18, and used on the test set experiments.
4.2.2 Interactive segmentation + DivMBest
Using the DivMBest framework the underlying interactive segmentation model can
be extended to generate a set of plausible segmentations each round, instead of just a
4.2 interactive segmentation 85
single MAP solution. The DivMBest formulation will encourage high quality segmen-
tations under the model that are diverse. Experimental results are provided for the
dissimilarity functions described below.
4.2.2.1 Hamming dissimilarity
The negative dot-product distance function ?(µ,µ(i)) .= ?
?
s?V
µTsµ
(i)
s captures the
Hamming dissimilarity between two solutions (see § 2.2.7). Recall that since the dot-
product function decomposes over nodes in the graph, the DivMBest formulation
under Hamming dissimilarity is equivalent to a ?-augmented energy minimization
problem were the unary terms in the energy have been perturbed in a certain way
(cf. § 2.2.7). The pairwise interaction terms remain unaffected, so if the smoothness
term is submodular in the original problem then it remains submodular. This means
that the m-modes can be computed using the same efficient inference algorithm used
to compute the MAP solution. In the case of our binary pairwise energy in eqn. 4.2,
we can compute the m-modes using the same efficient graph-cuts algorithm that was
used to optimally compute the MAP solution.
4.2.2.2 Higher-order potential (HOP) dissimilarity
Let #µ .=
?
s?V
µs(1) denote the number of nodes in the solution that are set to the
foreground label. The value #µ represents the size of the foreground region. The HOP
dissimilarity is defined as,
?(µ,µ(1)) =
{
(#µ? #µ(1))2 if #µ > #µ(1),
0 otherwise
(4.6)
Intuitively the HOP dissimilarity is zero if the foreground size of the current seg-
mentation is smaller than the foreground size of the MAP solution. Otherwise the
dissimilarity grows quadratically. Since the 2Modes constraint on the MAP solution is
?(µ,µ(1)) > k, for some k > 0, the HOP dissimilarity encourages foreground size of
the current solution to be larger than the MAP. By changing the sign on ? we can alter-
natively encourage smaller solutions. Note that #µ is a global measure of the solution
so it cannot be decomposed over subsets of variables. However, the ?-augmented en-
ergy minimization problem that we get from the Lagrange relaxation of the DivMBest
problem, under HOP dissimilarity, results in a cardinality potential [39, 111] for which
efficient approximate solutions exist. To solve this ?-augmented energy minimization
problem we can use the HOPMAP algorithm of Tarlow et al. [111].
Figure 4.1 shows a few DivMBest modes using HOP dissimilarity on the interactive
segmentation problem.
86 divmbest experiments
Figure 4.1: DivMBest modes under cardinality-based HOP. From left-to-right: image-scribble pair
(X, S), MAP solution, 2nd-mode,. . . ,6th-mode. The modes are ordered in increasing
size of foreground object.
4.2.3 Experiments
In this section we evaluate the quality of the object cutouts generated in the first round
of interactive segmentation. Specifically we compare the MAP segmentation against
the alternative approach of generating a set of DivMBest solutions. We also evaluate
some other baseline approaches, which we describe next.
4.2.3.1 Baselines
Since eqn. 4.2 is a binary contrast sensitive Potts energy [12] that is submodular its
exact MAP solution can be computed using the graph-cut implementation of [62]. The
DivMBest solutions under the Hamming dissimilarity can also be computed using the
same graph-cut implementation. We also evaluate the DivMBest solutions under HOP
dissimilarity generated using HOPMAP [111].
The first baseline is the M-best MAP algorithm [127] which we reviewed in § 2.1.3. We
also compare against the random and confidence baselines (see § 4.1.1) where the exact
min-marginals are computed using dynamic graph-cuts [58].
4.2.3.2 Results
For each of the 50 test images in PascalVOC10 the MAP segmentation cutout plus
five additional modes was generated using the approaches described above. Table 4.2
shows the best-out-of-6 cutout accuracies averaged over 50 images for the different
methods. Note that the DivMBest cutouts (column two) achieves the best results. Some
4.3 figure-ground segmentation 87
MAP MModes-dot prod. MModes-HOP M-Best Random Confidence
Acc.(%) 91.542 95.16 93.82 91.59 91.68 93.17
Table 4.2: Interactive segmentation: pixel accuracies averaged over 50 test images.
example cutouts are shown in figure 2.2. Notice how Yanover and Weiss’ [127] 2nd-
best MAP cutout is almost identical to the MAP cutout. In contrast the 2nd-mode of
DivMBest is qualitatively different and more likely the cutout the user intended. The
2nd-mode corrects where the MAP solution has likely made a mistake, for example
the arm of the person in the first image is completed and a second instance of an ob-
ject category is found in other image. The MAP solution suggests that the interactive
segmentation model (TSVM + contrast sensitive Potts) does not perfectly capture the
most probable cutout the user intended and yet we see that other modes of the under-
lying model distribution correspond to good segmentations. With DivMBest we have
a framework that provides a principled way of extracting these other segmentations.
4.3 figure-ground segmentation
2 This section presents the application of the DivMBest framework to the figure-
ground image segmentation task. Instead of relying on a complex model for fore-
ground and background, the approach uses a simple binary pairwise CRF which re-
lies on features computed over superpixels. The CRF can be learned efficiently using
Structured SVM formulation.
Figure-ground segmentation can be used as input to multi-category segmentation
models, or used in feed-forward approaches as a way to generate a set of candi-
date masks that are processed further [16, 92]. As such generating high-quality figure-
ground segmentations of an image is an important task.
A common approach to category-level image segmentation relies on building struc-
tured probabilistic models with low-order interactions (such as pairwise CRFs). Such
models are appealing because inference over them tends to be tractable and is often
guaranteed to be optimal. On the other hand the simplifying independence assump-
tions of these models lead to exact MAP assignments that are highly inaccurate. In
contrast, more complex models have been introduced that incorporate different types
of higher-order interactions over the image such as cardinality and co-occurrence po-
tentials [60, 70, 111] and hierarchical CRFs [68]. However, even though these models
may better capture complex statistics of natural scenes they can be inefficient and
2 The contributions to the thesis presented in this section are found in [126], and are in collaboration with
Gregory Shakhnarovich.
88 divmbest experiments
slow to train. Therefore approximate inference algorithms are often needed to make
inference tractable.
In contrast, this section presents a fairly simple probabilistic model for figure-ground
segmentation where inference is efficient. While the inferred MAP solution is often
not good enough, the set of solutions generated from this model using the DivMBest
method tend to contain highly accurate segmentations. This suggests that even though
the model does not accurately model the most likely figure-ground segmentation, the
underlying CRF distribution tends to have high quality solutions as one of its modes.
The next section introduces our simple figure-ground CRF model.
4.3.1 CRF Model
We represent the image, I, as as set of n disjoint superpixels, I = {X1, . . . ,Xn}, and
V = [n]. We define a graph G = (V ,E) where the vertices correspond to superpixels
and edges connect adjacent superpixels in the image. A segmentation corresponds to
an assignment x = {x1, . . . , xn} ? {0, 1}n, where xs = 1 indicates assignment of Xs to
foreground. The binary CRF energy is define to be,
E(x; u) = uT1
?
s?V
?1(xs) +u
T
2
?
(s,t)?E
?2(xs, xt)+
uc
?
s?V
?c(xs) +
?
(s,t)?E
?e(xs, xt). (4.7)
The unary and pairwise potentials are ?1(·) and ?2(·) respectively. Potential functions
?c(·) and ?e(·) are cardinality potentials on the nodes and edges respectively.
Intuitively, the unary ?1 captures characteristic properties of superpixels in figure
vs. background classes, while the pairwise ?2 captures the likelihood of neighboring
regions to be assigned the same class.
4.3.2 CRF potentials
The details of each type of potential are described below.
unary : The unary potential consists of p-channels, ?1 = (?11, . . . , ?1p). The jth
channel is defined to be,
?1j(Xs = xs)
.
= P(Xs = xs | ?1j(Xs); w1j). (4.8)
The channel captures the likelihood that superpixel s belongs to a foreground/back-
ground object, given the jth unary features computed on the superpixel, ?1j(Xs), and
4.3 figure-ground segmentation 89
learned parameters w1j. The background score is simply ?1j(Xs = 0) = 1? ?1j(Xs =
1). The foreground probability of a superpixel is modelled as a logistic regression
classifier,
P(Xs = 1 | ?1j(Xs); w1j)
.
= ?
(
?w1j, ?1j(Xs)?
)
, (4.9)
where ?(z) = 1/(1+ e?z). Since foreground is less common that background, we use
asymmetric logistic loss, tuned to provide 90% recall for foreground on training data.
pairwise : The pairwise potential consist of q-channels, ?2 = (?21, . . . , ?2q). The
kth pairwise channel is defined as,
?2k(Xs = xs,Xt = xt)
.
= P(xs 6= xt | ?2k(Xs,Xt); w2k), (4.10)
and captures the likelihood that adjacent superpixels Xs and Xt should have consistent
labels, according to the kth pairwise features?2k(Xs,Xt) and learned parametersw2k.
The pairwise potentials are modelled as a logistic regression classifiers similar to the
unary case, and trained with asymmetric logistic loss tuned to achieve 90% recall for
neighboring regions with different labels.
higher order : The third and fourth terms in the CRF energy of eqn. 4.7 are unary
and pairwise cardinality potentials [113] respectively. They are a function of the global
solution and not local image evidence. The unary potentials are defined as,
?c(xs)
.
= [[xs = 1]], (4.11)
and count the number of superpixels labelled foreground, capturing what portion of
the image is assigned to foreground.
The pairwise cardinality potentials are,
?e(xs)
.
= [[xs = xt]], (4.12)
and measure the length of the boundary between foreground and background regions
in the solution.
4.3.3 Features
4.3.3.1 Superpixel (unary) features
The segmentation model relies on basic appearance features described below.
90 divmbest experiments
intensity histograms We bin the intensity into nc equally spaced bins. Since
color/intensity distribution within an image may be skewed, this may be an inefficient
binning scheme, and so we also use adaptive binning according to 1/nc quantiles of
the intensity in the given image. For graylevel images this produces four histograms:
fixed and adaptive binning schemes, each with nc = 8 and with nc = 32 bins. For
color images there are twelve histograms, four per each dimension in the L*a*b space.
texton histogram We compute a dictionary of 32 textons [78] on all training
images, using a bank of 12 filters. Histogram of texton assignments within a region
forms a single 32-dimensional histogram.
gradient features We compute the histogram of oriented gradient [24] within
the region, binned into four directional bins. Furthermore, we compute the statistics
of the gradient magnitude: sum of L2 and of L1 norms of the gradients within region,
as well as the ratio of the two sums, known as blur index [53].
entropy features Finally, we set up one-dimensional features computed as mea-
sures of entropy of histogram-based features. This is intended to capture how homo-
geneous a region is. There is one entropy value for each intensity, texton and gradient
histogram; total of 6 for graylevel and 14 for color images.
Note that in contrast to many other models, we do not employ HoG/SIFT descriptors
or shape features in this model. This is because at the level of small superpixels we do
not expect such features to be informative.
4.3.3.2 Boundary (pairwise) features
histogram differences For each of the histogram-based unary features we com-
pute the ?2 difference between the two regions. We also compute the earth mover’s
distance (EMD) between the histograms. High values of these features here indicate
different color/intensity content between the two regions.
entropy differences For each entropy feature, we compute the absolute value of
the difference in entropies. High value here indicates one region is more homogeneous
than the other in the respective feature.
boundary strength We compute the integral of the boundary probability ac-
cording to gPb [77] along the boundary between the two regions. High value corre-
sponds to pronounced boundary evidence according to gPb.
mser correlation We extract a set of maximally stable extremal regions (MSERs, [80]),
and for each MSER compute the percentage of the superpixel covered by that MSER.
With M MSERs, this produces an M-dimensional vector for each superpixels. The cor-
4.3 figure-ground segmentation 91
relation coefficient of these vectors is a pairwise feature; high value indicates that the
two superpixels tend to belong to the same MSERs. This and the next feature were
inspired by ideas in [81].
mser overlap Another pairwise feature is the largest overlap of any of the MSERs
and the union of the two superpixels. Higher value of this feature indicates that “merg-
ing” the two superpixels in the same mask is better supported by MSERs.
All of these features could be used directly in the model (eqn. 4.7). However, this
would lead to a fairly high-dimensional parameterization making learning more chal-
lenging. Instead we proceed in two stages. First we train for each unary feature
group ?1j(Xs) (each histogram, each entropy value, etc.) a logistic regression clas-
sifier ?(?w1j, ?1j(Xs)?) predicting FG/BG label. Similarly, for each pairwise feature
group ?2k(Xs,Xt) we train a classifier predicting whether the two superpixels are in
the same class or not.
4.3.4 CRF learning
Instead of using max-likelihood training to learn the CRF weights u = (u1,u2,uc,ue)
in eqn. 4.7 we optimize the following Structured SVM [114] objective,
u = argmin
u,?>0
1
2
||u||22 +C · ? (4.13a)
s.t.
1
N
?
i?[N]
max
x?i?X
[L(xi, x?i) ? ?u,?(Ii, x?i)?+ ?u,?(Ii, x?i )?] 6 ?, (4.13b)
where Ii represents the ith image and x?i is the best segmentation achievable given a
particular superpixel partitioning of image Ii. For an image with n superpixels the
potential values are pooled across all nodes and edges,
?(Ii, x?i) =
????????????
?
s?[n]
?1(xis)?
(s,t)?E
?2(xis, xit)?
s?[n]
?c(xis)?
(s,t)?E
?e(xis, xit)
????????????
, (4.14)
and the task loss L(xi, x?i) is discussed in the next section. The quadratic program in
eqn. 4.13 is a one-slack, margin-rescaled, structural SVM [48].
92 divmbest experiments
4.3.5 Task loss
The task loss we use for the binary segmentation problem is the intersection-over-
union score (IoU) for predicting segmentation x?i with respect to the ground-truth for
image Ii. It can be written as,
IoU(x?) =
1
2
?
`?{0,1}
?
j[[p?j = `? p
?
j = `]]?
j[[p?j = `? p
?
j = `]]
, (4.15)
where p?j
.
= g(x?), g(·) is a function mapping a figure-ground segmentation over super-
pixels to the label assignment of the underlying pixels in the image, g : X? {0, 1}w×h,
and j ? {1, . . . ,wh}. The ground-truth label pixel label assignment is p?j .
Since superpixels are the underlying image elements used in the model it is unlikely
that a perfect image segmentation can be achieved, i.e. IoU(x?) < 1. This is because
the superpixels may not have perfect alignment with foreground objects. Instead, we
use a task loss that measures performance relative to the best achievable segmentation
when committed to a specific set of superpixels,
L(x?, x?) .= IoU(x?) ? IoU(x?). (4.16)
where x? is the best segmentation achievable and L(·, ·) ? [0, 1].
4.3.6 Loss-augmented inference
The MAP solution to the binary CRF in eqn 4.7 given parameters u, as well as the
loss-augmented inference in eqn 4.13b were solved using graph-cuts [62]. The pair-
wise smoothness term in eqn 4.7 is not submodular so the graph-cut algorithm is
not guaranteed to return an optimal solution, however we consistently attained good
performance using this approach.
The constraint in eqn 4.13b uses the relative task loss defined in eqn 4.16. Since
the intersection-over-union score doesn’t decompose over image elements, the loss-
augmented inference problem is more difficult to solve. We can approximately solve it
by solving the simpler problem,
min
x?i?X
?u,?(Ii, x?i)?, (4.17)
using graph-cuts and applying a greedy hill climbing procedure that sequentially flips
the label of each superpixel in image i, xij, in order to maximize the loss adjusted
score of the predicted solution relative to the ground-truth, until no more improve-
ment can be attained. Alternatively a message-passing inference algorithm designed
to handle high-order potentials [112] could have been used to approximately solve the
loss-augmented inference problem.
4.3 figure-ground segmentation 93
4.3.7 DivMBest inference with Hamming dissimilarity
Given a fixed C and model parameters u from training the CRF (§ 4.3.4), we can
generate a diverse set of plausible segmentations using the DivMBest framework. In
order to compute DivMBest solutions (§ 2.2.7 ) we using Hamming dissimilarity for
the ?-function. The mth-mode is generated by solving the following minimization
problem,
x
(m)
i = argmin
x?i?X
?
j?[n]
??uT1?1(x?ij) + ?
k?[m?1]
? · [[x?ij = x(k)ij ]]
??
+
?
(s,t)?E
uT2?2(x?is, x?it) +
?
j?[n]
uc?c(x?ij) +
?
(s,t)?E
?e(x?is, x?it). (4.18)
We use the same s-t graph-cut implementation [62] to solve this problem as we use to
compute the MAP solution. The value of ? is set using cross-validation on the training
set.
4.3.8 Superpixels
In order to have a computationally efficient model that produces segmentations with
good alignment to internal image boundaries superpixels are used as opposed to
the image pixels. To produce the superpixels the SLIC superpixel segmentation al-
gorithm [1] is employed. For each image the desired number of superpixels is set to
400.
4.3.9 Experiments
The purpose of the experiments in this section is to evaluate the quality of the segmen-
tations from our figure-ground model produced by the DivMBest method.
4.3.9.1 Data sets
We experimented with four benchmark data sets of natural images designed for evalu-
ation of figure ground segmentation, and an additional data set of radiological images.
weizmann horses [10] 328 color images of horses. This is the easiest of the five
data sets, with large prominent foreground (horses in a variety of scenes).
94 divmbest experiments
graz bikes , cars , people [79] Each set containes 300 color images, generally
harder than horses: bikes and cars in a variety of orientations and locations, some
partially occluded, and people in a variety of locations/poses and with varying degree
of occlusion.
ultrasound This medical dataset contains 416 ultrasound (graylevel) images col-
lected from five hospitals with different acquisition devices, varying image quality,
noise levels and resolutions. Each image in the set contains a single lesion with vali-
dated pathology diagnosis. The ground truth segmentation for each image was created
manually by a radiologist, who marked the boundary of lesions. We include this data
set to evaluate the performance of the proposed, very general, segmentation approach
on images very different from the natural scenes in the other sets. We plan to make
the data set including annotations public.
4.3.9.2 Evaluation
The Weizmann horses and the three Graz data sets are split into a single train/test
split with 1/3 of the horses and 1/2 of the Graz data sets used as test sets [66] on
which we report performance. For cross-validation purposes the ultrasound data set is
split into five equal folds. The average of the five evaluations is reported, where one of
the five folds is used as the test set and the remaining four folds used for training. In
all the experiments the CRF learnable parameters and diversity weight ? were tuned
using cross-validation on the training set.
The goal of the evaluation in this section is to determine whether generating a diverse
set of plausible segmentations for the figure-ground problem can be beneficial. There-
fore we evaluate the oracle performance (i.e. best segmentation in the DivMBest set)
against the MAP solutions.
The model MAP and oracle performance is evaluated in the context of state-of-the-
art among the published work at the time of these experiments. For this purpose we
compare our results to those in [66], which were shown to be competitive for the
state-of-the-art title.
performance measures Performance is measured by intersection-over-union
(IoU) score which is most common measure used in semantic segmentation. IoU is
also the task loss that is used in learning the CRF. For each experiment the results are
reported in terms of average IoU over the images in the test set(s).
running time For a typical image the CRF model has approximately 400 vari-
ables. Once the bottom-up segmentation CRF is trained, producing a diverse set of
4.3 figure-ground segmentation 95
MAP Oracle %gap [66]
Weizman horses 75.4 83.0 51.3% 79.1
Graz bikes 53.1 61.4 36.1% 45.0
Graz cars 50.0 66.3 59.5% 58.8
Graz people 44.1 57.0 26.4% 47.5
Ultrasound 39.7 57.3 58.0% 26.6
Table 4.3: Segmentation performance on all data sets, in IoU values ×100. MAP: single solution
from the bottom-up CRF model. Oracle: (hindsight) best of 10 diverse solutions from
the CRF. Third column: percentage of gap (oracle-MAP) recovered by the ranking.
Last column: Figure-ground segmentation model of Kuettel et al. [66].
segmentations for a new images involves the following stages. SLIC superpixels are
extracted and the CRF features are computed ('15 sec/image on a 6-core machine).
Bottom-up inference of 10 diverse solutions using graph cuts takes approximately 5
seconds.
4.3.9.3 Results
The oracle performance of the DivMBest segmentations for all data sets is summarized
in table 4.3. A single MAP segmentation gives reasonable accuracy compared to results
from a state-of-the-art figure-ground segmentation model [66] at the time of these
experiments. But the MAP solution is inferior to the of the oracle performance over
just a small set of 10 DivMBest solutions. The oracle is in fact superior to a state-of-
the-art method that does not generate multiple segmentations. In later chapter we’ll
explore the 1-out-of-10 inference problem using the DivMBest+ReRank framework to
automatically pick the likely best segmentation from the set.
Examples of the MAP vs. DivMBest oracle figure-ground segmentations using the pro-
posed CRF are shown in figures 4.2 and 4.3 along with the ground-truth segmentations.
Note how the MAP foreground regions often "bleed" into the surrounding regions
whereas the oracle results show that there is typically a mode of the solution space
distribution that can accurately recover the foreground boundaries.
Samples of the DivMBest foreground segmentations for the Horses and Graz datasets
can be found in Appendix A.1.
96 divmbest experiments
4.4 multi-category segmentation
3 So far we have seen that the DivMBest framework can generate sets which often con-
tain segmentations better than the MAP solution for a number of segmentation tasks.
In this section we further apply the DivMBest algorithm to the multi-category segmen-
tation problem. The DivMBest solutions are evaluated against the MAP segmentation
produced by three different discrete probabilistic models.
4.4.1 Hierarchical model
The first model we consider is the Associative Hierarchical CRF of Ladicky et al. [68]
(see § 1.1.1). At the time of evaluation this model gave competitive performance on
the multi-category segmentation task. Experiments use the Automatic Labeling Envi-
ronment (ALE) [67] which is an implementation of the hierarchical CRF model by the
authors. The model incorporates a number of potentials including unary potentials
that look at local texture (based on textonboost features [106, 107]), low-level pairwise
Pn Potts potentials [55] between pixels and similar mid-level potentials between su-
perpixels, as well as a global co-occurrence potential[69].
The graph-cut inference algorithm of Ladicky et al. [68, 69] is used to compute the
MAP solution over this hierarchical model. Assuming a Hamming dissimilarity func-
tion ?(·, ·), we can compute the subsequent modes (i.e. DivMBest segmentations) by
appropriately modifying the unary potentials according to the DivMBest formulation
and rerunning the same inference algorithm.
4.4.1.1 Baselines
The same baselines as in the figure-ground experiments are used (see § 4.3). The ran-
dom baseline averaged over ten runs as well as the confidence baseline are reported. The
M-Best MAP algorithm [127] (cf. § 2.1.3) is infeasibly slow to run for this model. Since
the energy is not sub-modular computing min-marginals cannot be done efficiently.
Even with an implementation that re-uses search trees and caches ?-expansion graphs
it would take 10 years to compute each additional solution for each image. Computing
DivMBest solutions, however, takes the same amount of time as computing the MAP
solution.
3 Part of the contributions to the thesis presented in this section are found in [7], and are in collaboration
with Gregory Shakhnarovich and Dhruv Batra.
4.4 multi-category segmentation 97
Ba
ck
gr
.
Pl
an
e
Bi
cy
cl
e
Bi
rd
Bo
at
Bo
tt
le
Bu
s
C
ar
C
at
C
ha
ir
C
ow
D
.T
ab
le
D
og
H
or
se
M
.b
ik
e
Pe
rs
on
Pl
an
t
Sh
ee
p
So
fa
Tr
ai
n
T
V.
M
o.
A
ve
ra
ge
MAP 78.5 35.1 5.2 20.3 20.8 11.8 39.4 38.2 25.8 8.9 14.1 30.2 10.0 12.3 37.6 33.5 10.3 24.2 16.2 28.7 20.5 24.8
Confidence 78.5 35.1 5.3 20.1 20.7 12.6 39.4 37.9 26.8 8.9 14.1 30.2 10.3 12.2 39.5 33.4 10.6 24.2 17.3 28.4 20.5 25.1
Random 74.9 32.4 6.4 16.1 14.7 12.3 34.3 32.6 22.6 8.0 13.2 21.1 8.7 10.4 32.9 28.9 7.8 20.6 10.8 23.5 17.3 21.4
10Modes 85.6 53.9 14.6 36.9 33.6 33.2 64.2 56.3 47.7 16.1 30.3 46.8 29.1 28.7 59.0 50.0 32.5 46.7 31.2 52.9 39.0 42.3
Table 4.4: Pascal VOC 2010 val set accuracies for ALE model.
Ba
ck
gr
.
Pl
an
e
Bi
cy
cl
e
Bi
rd
Bo
at
Bo
tt
le
Bu
s
C
ar
C
at
C
ha
ir
C
ow
D
.T
ab
le
D
og
H
or
se
M
.b
ik
e
Pe
rs
on
Pl
an
t
Sh
ee
p
So
fa
Tr
ai
n
TV
.M
o.
A
ve
ra
ge
MAP 73.7 44.0 14.2 15.3 21.0 23.2 41.3 37.0 27.6 6.1 23.9 25.2 12.8 24.3 51.0 27.8 20.0 28.2 17.1 36.5 23.9 28.3
10Modes 83.4 54.4 19.6 22.4 34.5 22.2 60.8 55.5 45.8 14.0 45.5 35.1 34.8 40.1 53.6 48.7 28.0 48.7 31.2 50.5 33.9 41.1
Table 4.5: Pascal VOC 2010 test set accuracies for ALE model.
4.4.1.2 Dataset
Multi-category level experiments were carried out on the PASCAL Visual Object Classes
(VOC ) 2010 segmentation benchmark [29]. PASCAL VOC 2010 contains 21 seman-
tic categories (20 object categories ({aeroplane, bicycle, bird, bottle, car, . . . } +
background), and the task is to label every pixel in the image with one of the 20 ob-
ject categories or the background. This task is part of the PASCAL VOC challenge.
The dataset contains train, val, and test splits that contain 964 images each. The
object categories appear in natural scenes under varying appearance, lighting, and
pose. Many images contain multiple instances of the same category and more than
one category can appear in the same image.
Segmentation accuracy is scored using the standard PASCAL VOC intersection-over-
union (IoU) measure (i.e. pixelwise intersectionunion measure averaged over masks of all cate-
gories). The relevant parameters, such as the multiplier on the diversity term (?) in the
DivMBest formulation, are tuned on the val set, after the model has been trained on
train. Ground-truth segmentations are not provided for test but test set accuracies
can be obtained by submitting a single segmentation prediction per image to the VOC
evaluation server.
4.4.1.3 Results
To measure the upper-bound on segmentation accuracy achievable with the DivMBest
solutions we evaluate on the val set for which we have ground-truth. Oracle accuracy
is computed as follows: for each image the segmentation in the DivMBest set that
has highest pixel-wise IoU w.r.t ground-truth (averaged over all category masks) is
98 divmbest experiments
selected. The results on val for 10 DivMBest modes for ALE are reported in table 4.4
along with the MAP accuracy and baselines that generate multiple solutions using
different perturbation strategies on the MAP segmentation. The result on test using
10-modes is summarized in table 4.5. To illustrate how the upper-bound on segmen-
tation accuracy grows as the number of solutions increases, figure 4.4a shows a plot
of oracle accuracy versus number of DivMBest solutions. The MAP accuracy is shown
as the dashed horizonal line, and the accuracy of the confidence based solution is only
slightly better than MAP. With m = 30 solutions the oracle accuracy reaches 48% on
val. Though val and test set performance aren’t directly comparable the oracle per-
formance on test is likely better — by a significant margin — than state-of-the-art
methods at the time of experiments.2 Sample DivMBest segmentations on PASCAL2 the winning entry
of VOC2010 comp5
challenge achieved
40.1% on test
VOC 2010 val set images are shown in Appendix A.2.
4.4.1.4 Evaluating DivMBest modes
Figure 4.4b shows a plot of the distance of the DivMBest solutions (aka modes) to
the MAP solution and previous modes. The normalized (w.r.t image size) Hamming
distance between solutions monotonically increases with each additional solution. This
show that the DivMBest Hamming dissimilarity constraints (i.e. ?(µ,µ(i)) > k) are
encouraging diversity between solutions. The energy of the modes as a percentage of
the MAP energy is shown in figures 4.4c and 4.4d. A majority of modes have higher
energy than the MAP solution3. A small proportion of modes have less energy than3 guaranteed if
using exact inference MAP due to the fact that the model uses approximate inference.
4.4.2 Feed-forward model
In ALE complex interactions between image elements is captured by the hierarchical
structure of the graph and the higher-order graph cliques. An alternative approach
is presented by the Second-Order Pooling (O2P) approach of Carreira et al. [17]. In
O2P, complex interactions between regions in the image are captured by global region
descriptors that are constructed by second-order pooling of local descriptors such as
SIFT and local binary patterns (LBP) [85, 86]. Carreira et al. present a simple inference
algorithm for the O2P model, which can be applied directly to generating the diverse
segmentations of the DivMBest algorithm with Hamming dissimilarity. The details
of the model and evaluation are deferred to Chapter 5 where we also evaluate an
approach to re-ranking the DivMBest solutions.
4.4 multi-category segmentation 99
4.4.3 Convolutional neural network + dense CRF model
Current state-of-the-art semantic segmentation is done by combining very deep con-
volutional neural networks (CNNs) or residual neural networks (RNNs) with fully
connected dense pairwise CRFs [18] [43]. Deep networks are superior at building local
features that capture information at multiple spatial scales of the image, however the
output suffers from a decrease in resolution compared to the input image. Dense pair-
wise CRFs can introduce low-order dependencies between image elements, that are
not constrained to be local. Additionally local pairwise potentials can provide spatial
smoothness constraints on the solution and improve alignment with image boundaries.
Piggy-backing dense CRFs on top of deep segmentation networks combines the ben-
efits of both approaches - efficient computation of complex features that incorporate
both local and global interactions in the image along with constraints on local smooth-
ness. We can view the CNN + dense CRF pipeline as a discrete probabilistic model on
a dense pairwise graph where the unary potentials are defined by the output of the
CNN at each image element (superpixel or pixel).
The DivMBest framework is agnostic to the underlying discrete probabilistic model so
we can apply it to this model in a similar manner as previous models. Given a CNN
trained on the semantic segmentation task, the above deep network approaches use
the features from the last layer of the network to initialize the unary potentials of a
fully-connected CRF. For a fully-connected CRF where the pairwise edge potentials
are defined by a linear combination of Gaussian kernels an efficient approximate in-
ference algorithm exists [65] for computing the MAP solution. Approximate inference
is based on an iterative message passing algorithm where messages are computed
using efficient Gaussian filtering in feature space. Assuming pixel-wise Hamming di-
versity constraints between solutions the DivMBest algorithm amounts to modifying
the unary potentials and rerunning the message passing algorithm to compute succes-
sive solutions.
We investigate the benefit of applying DivMBest to one such deep neural network +
dense CRF pipeline – we use the Zoom-out network [82] with the DeepLab dense CRF
implementation [18][65]. The Zoom-out network (cf. § 1.1.1) is first pre-trained to per-
form multi-category image classification on the ImageNet dataset [96]. Subsequently
the final fully-connected layer of the CNN is modified to a fully-convolutional layer
with output feature map depth set to 214 and spatial extents up-sampled to be the 4 corresponding to
the 21 PASCAL
VOC categories
same size as the input image. The CNN is then fine-tuned in a end-to-end manner for
the semantic segmentation task using the PASCAL VOC 2012 train set + 11.3K anno-
tated PASCAL VOC 2011 images from the Semantic Boundaries Dataset [41]. Given a
trained network, the dense CRF unary potentials are initialized with the features from
the last fully-convolutional layer of the network, and the CRF hyper-parameters are
fixed to the defaults set by the implementation of Chen et al. [18]. On the PASCAL
VOC 2012 val set this pipeline achieves 72% MAP accuracy5 (IoU accuracy averaged 5 current
state-of-the-art
methods [124, 129]
that use additional
training data
currently achieve
? 85% accuracy on
PASCAL VOC 2012
100 divmbest experiments
over all categories/images).
4.4.3.1 Dataset
The segmentation results in this section are reported on images from the PASCAL
VOC 2012 benchmark [29]. It contains 4,369 images split into train (1,464 images),
val (1,449 images), and test (1,456 images) sets. The CNN is pre-trained for the 1000-
category classification task from the ImageNet Large Scale Visual Recognition Chal-
lenge (ILSVRC) [96], using ' 1.2M ILSVRC2014 images. An additional 11.3K images
from the Semantic Boundaries Dataset [41] are used along with PASCAL VOC 2012
train set to fine-tune the CNN to the 21-category PASCAL VOC segmentation task.
4.4.3.2 Results
We can explore the maximum accuracy achievable when using the DivMBest algo-
rithm to generate segmentations with the CNN + dense CRF model. To this end we
evaluate two approaches to producing oracle segmentations for each image, (1) se-
lecting the best-out-of-m segmentations based on accuracy relative to ground-truth
and, (2) constructing full image labellings from connected components found in the m
segmentations using a greedy inference approach.
The oracle accuracy versus MAP for the first approach can be seen in figure 4.5a. This
suggest that if we pick the best-out-of-40 solutions we can achieve more than 5%-point
improvement over MAP in overall segmentation accuracy. The second approach re-
lies on a greedy inference algorithm over connected components (i.e. contiguous image
regions taking the same label) found across the m segmentations. We first compute a
bag containing tuples, (Rj, `j) of connected components Rj extracted from the m seg-
mentations with corresponding category labels `j. Note that duplicate tuples can exist
if a contiguous region with corresponding label is found in more than one image seg-
mentations. We assign each connected component in the bag a score defined to be its
highest IoU with all connected components taking the same label in the ground-truth
segmentation. Starting with an empty labeling, the full image labeling for the image
is constructed using a greedy strategy of pasting the connected components in order
of decreasing IoU score until a prescribed score threshold is reached, at which point
the algorithm stops. During the pasting procedure if the current connected component
overlaps with a region in the image that has already been assigned a label then the
previous label for pixels in that region is retained. A non-maxima suppression step is
applied at each iteration: after pasting a connected component from the bag we cull
the bag of all connected components that have intersection greater than a fixed thresh-
old. The result of this greedy construction, as m varies from 1 to 40 solutions, is shown
in figure 4.5b. Notice that the accuracy of greedy inference on connected components
from the first solution (i.e. MAP) is higher than the accuracy of original MAP solution
4.4 multi-category segmentation 101
computed on the model. This is because greedy inference on connected components
culls from the final image labeling those connected components that do not align well
with the ground truth segmentation. With this approach we get a significant increase
in oracle accuracy compared to the best-out-of-m results – an almost 13% point in-
crease over MAP. An example result of composing the oracle segmentation from the
DivMBest segmentations is shown in figure 4.6.
The MAP solution from current state-of-the-art models, that combine deep neural net-
works with dense CRFs, achieve comparable accuracy6 to this greedy-inference ap- 6 ? 85% on
PASCAL VOC 2012
comp6
challenge [124, 129]
proach on the less accurate Zoom-out CNN + dense CRF model. The oracle results illus-
trate that it’s plausible to leverage the DivMBest algorithm to generate a set of diverse
segmentations that often contain highly accurate solutions even when the MAP solu-
tion from the underlying segmentation model is inaccurate. This suggests that near
state-of-the-art results can be had in the multi-category image segmentation problem7 7 as well as the
segmentation tasks
presented earlier
by devising methods with the goal of picking the best solution from the DivMBest set.
We explore one such method in Chapter 3 that is learned with the goal of re-ranking
the segmentations in set so that the best segmentation is top ranking.
4.4.4 Summary
The results on a number of semantic segmentation datasets show the utility of using
the DivMBest formulation to produce a diverse set of highly plausible segmentations.
Specifically, the oracle accuracies show that across the segmentation tasks the DivMBest
set often contains much higher quality segmentations than MAP. This validates the
alternative approach of leveraging models, in which exact of provable approximate in-
ference is tractable, by efficiently producing a diverse set of segmentations as opposed
to devising more complex models where inference becomes intractable. The oracle re-
sults highlight the importance of being able to pick the best segmentation from the
DivMBest set via ranking and we investigate this in chapter 5.
102 divmbest experiments
MAP, 34.0 oracle, 61.8 ranking, 61.8
MAP, 32.7 oracle, 59.4 ranking, 59.4
MAP, 45.0 oracle, 87.8 ranking, 86.8
MAP, 43.4 oracle, 76.7 ranking, 76.7
MAP, 50.9 oracle, 79.8 ranking, 79.8
MAP, 18.3 oracle, 82.3 ranking, 82.3
Figure 4.2: Examples of (left to right) input image with ground truth, MAP from the bottom-
up CRF model, oracle out of 10 diverse solutions. All examples are from the test
portions of Graz data sets.
4.4 multi-category segmentation 103
MAP, 58.0 oracle, 87.0 ranking, 85.1
MAP, 66.0 oracle, 81.2 ranking, 81.2
MAP, 16.7 oracle, 83.8 ranking, 83.8
MAP, 47.7 oracle, 66.4 ranking, 66.4
MAP, 75.1 oracle, 81.5 ranking, 81.5 MAP, 35.7 oracle, 93.2 ranking, 93.2
MAP, 44.2 oracle, 44.2 ranking, 0.0 MAP, 15.3 oracle, 58.8 ranking, 23.2
Figure 4.3: Examples of (left to right) input image with ground truth, MAP from the bottom-up
CRF model, oracle out of 10 diverse solutions. Examples are from the test portion
of Weizmann horses data set, and from one of the test folds of the Ultrasound data
set. Last row shows some failures.
104 divmbest experiments
 
 Number of Solutions
20
25
30
35
40
45
50
A
vg
 P
as
ca
l I
oU
 A
cc
ur
ac
y
O
O
O
O
O
O
O
1 6 11 16 21 26 31
(a)
2 6 11 16 21 26 31
0
.1
.2
.3
.4
.5
.6
.7
.8
.9
Mode Number
H
am
m
in
g 
D
is
ta
nc
e 
/ I
m
ag
e 
S
iz
e
 
 
Distance to MAP
Avg. Dist. to Previous Modes
(b)
Energy (as a % of MAP Energy)
N
um
be
r 
of
 M
od
es
<100%
?
102 1 .5 × 102
100
101
102
103
(c)
Energy (as a % of MAP Energy)
N
um
be
r 
of
 M
od
es
102 103
100
101
102
103
104
<100%
?
(d)
Figure 4.4: (a) Oracle accuracy vs. number solutions on VOC2010 val for DivMBest (red) and
confidence based perturbations (blue), along with MAP performance (black dashed).
(b) Mean hamming distances between each mode (DivMBest solution) and the MAP
solution (red), and average to previous modes (blue), normalized by image size on
PASCAL VOC 2010 val set. Also show, histogram of energies (as % of MAP) over
(c) 6 modes, (d) 31 modes, on validation set. The bar to the left of red vertical lines
indicate number of modes with energy less than or equal to MAP.
4.4 multi-category segmentation 105
 
 
1 5 10 15 20 25 30 35 40
Number of Solutions
70
72
74
76
78
A
vg
 P
as
ca
l I
oU
 A
cc
ur
ac
y
O
O
O
O
O
O O O O
MAP  72.0%
77.4%
(a)
 
 
1 5 10 15 20 25 30 35 40
Number of Solutions
70
75
80
85
A
vg
 P
as
ca
l I
oU
 A
cc
ur
ac
y
O
O
O
O
O
O
O O O
MAP  72.0%
84.7%
(b)
Figure 4.5: Oracle performance (IoU accuracy against ground-truth) on PASCAL VOC 2012
val, when (a) selecting best-out-of-m solutions, and (b) composing full image la-
bellings from connected components found among m solutions.
· · · · · ·
· · ·.
Figure 4.6: Result of composing solutions from DivMBest segments. Second and third row
show a subset of 40 DivMBest segmentations generated from the CNN+CRF model
of § 4.4.3. First row shows in order the image, ground-truth segmentation, and
composed segmentation oracle using the second approach of § 4.4.3.2. Note how
the composed segmentation oracle is a much better segmentation of the image than
the MAP solution (first segmentation in the second row).

5
D I V M B E S T + R E R A N K E X P E R I M E N T S
5.1 evaluating divmbest+rererank pipeline
To evaluate how well the proposed method in Chapter 3 re-ranks the DivMBest solu-
tions the following experiments are carried out,
1. Re-ranking of the object cut-outs generated from applying the DivMBest algo-
rithm to the figure-ground model in § 4.3.
2. Evaluate joint recognition and segmentation performance using the DivMBest+ReRank
pipeline on two multi-category segmentation models: the O2P model by Car-
reirra et al. [17], and the hierarchical ALE model by Ladicky et al. [67, 68].
5.2 figure-ground segmentation
Recall that in § 4.3 we presented a model for figure-ground segmentation and eval-
uated oracle accuracy of DivMBest solutions against MAP. We now evaluate the pro-
posed DivMBest+ReRank (cf. chapter 3) approach to segmentation where we take the
DivMBest segmentations from chapter 2 and rank them, returning the highest ranking
segmentation in the set as the final solution.
5.2.1 Re-ranking segmentations
1 Using the notation from § 3.2.1, let the ith image be denoted as xi and Yi = {y
(1)
i , . . . ,y
(m)
i }
be the set of predicted foreground masks. A foreground mask, yi, is a labelling of all
the n (super)pixels in the image where (super)pixel i can take labels from the set {0, 1},
i.e. yi ? {0, 1}n. Given a feature-function that computes a p-length feature vector on an
image/foreground mask pair, ?(xi,y
(j)
i ) : R
3×w×h × {0, 1}n ? Rp, we learn a linear
re-ranker model8, 8 refer to § 3.2.2
1 The contributions to the thesis presented in this section are found in [126], and are in collaboration with
Gregory Shakhnarovich.
107
108 divmbest+rerank experiments
Sr(yi) = ?
T?(xi,y
(j)
i ). (5.1)
5.2.2 Ranking features
There are a few considerations that guide the design of region ranking features for our
approach. These features need to be evaluated only on a small number of segmenta-
tions, hence we can afford fairly complex/expensive computation. Furthermore, these
features will be deployed to evaluate entire hypothesized foreground masks, assumed
to include high-quality ones. Therefore we can use image features that would be hard
to incorporate into the bottom-up model, in particular, shape properties of the mask
and its position in the image. In addition, we can incorporate properties of the hypoth-
esized regions that are less meaningful for the small, regular superpixels; for instance,
measurements of homogeneity of the regions. These considerations led us to design
the following features.
shape and position We extract the following properties of the hypothesized
foreground mask: area; perimeter; location of centroid; minor/major axis length, ec-
centricity and orientation for an ellipse fit to the region; area of the convex hull of the
region and its ratio to the area; Euler number (number of holes); diameter of the disk
with area equal to that of the region; extent (fraction of the bounding box occupied by
the mask); and finally size and location of the bounding box. All of these quantities
are normalized per image (e.g., area is expressed in percentage of image area).
color We compute histogram with 32 fixed bins for each color channel; we do not
use adaptive binning as in the bottom-up model, since color distribution for entire
objects is more stable than for small parts (superpixels).
texture We compute histogram of assignment to 32 textons [106], computed for
the entire training set and not per image as in the bottom-up model.
entropy For each histogram feature (color and texton) we compute its entropy.
This measures the homogeneity of hypothesized object.
5.2.3 Re-ranker training
For each of the five experiments re-ranking parameters were learned on the training
set of each dataset. The parameters were tuned by cross-validation on the training set.
5.2 figure-ground segmentation 109
[66] MAP Oracle Ranking %gap
weizman horses 79.1 75.4 83.0 79.3 51.3%
graz bikes 45.0 53.1 61.4 56.1 36.1%
graz cars 58.8 50.0 66.3 59.7 59.5%
graz people 47.5 44.1 57.0 47.5 26.4%
ultrasound 26.6 39.7 57.3 49.9 58.0%
Table 5.1: Segmentation performance on all data sets, in IoU values ×100. MAP: single solution
from the bottom-up CRF model. Oracle: (hindsight) best of 10 diverse solutions from
the CRF. Ranking: full ranking model (all features). last column: percentage of gap
(oracle-map) recovered by the ranking.
Method horses bikes cars people ultrasound
[66] 79.1 45.0 58.8 47.5 26.6
ours(full) 79.3 55.4 59.7 47.5 49.9
shape 73.7 54.8 58.6 44.7 48.2
textons 74.0 53.4 54.3 45.6 48.1
color 69.5 53.2 53.1 43.5 44.1
full-entropy 76.9 54.4 57.6 44.7 47.3
Table 5.2: Comparative results between methods and feature sets for region ranking. All num-
bers are IoU×100. Shape: only shape and position. Textons: only textons. Color: only
color histograms. full-entropy: shape, color and textons, but not their entropies.
5.2.4 Results
Table 5.1 summarizes the results of re-ranking the DivMBest solutions. In four out of
five benchmarks region ranking closes the gap between oracle and MAP performance
by up to or more than 50%. On Graz people ? 30% of the gap is closed.
In all the datasets the DivMBest+ReRank approach to figure-ground segmentation
achieves accuracy equal to or better than [66].
The contribution of each of the re-ranking features to the re-ranker performance is
summarized in Table 5.2. All the features contribute to the overall performance of the
re-ranker across all datasets, however the amount of contribution per feature differs
between datasets. For example, removing color features from the re-ranker drastically
reduces performance on the Horses dataset, probably because the color distribution of
background — which is often grass or foliage — is very different from foreground in
these images. On Ultrasound dataset the color intensity is important because lesions
110 divmbest+rerank experiments
usually appear dark on the ultrasound but the importance of color features is less
important than on Horses.
5.3 multi-category segmentation
2 We revisit the two multi-category segmentation models that we introduced in the
DivMBest experiments (cf. § 4.4), and evaluate segmentation performance of these
models when predicting the best solution in the DivMBest set for each image. To do
this we use the DivMBest+ReRank pipeline introduced in § 3.2.2.
5.3.1 Dataset
Evaluation of the DivMBest+ReRank pipeline applied to the models below is carried
out on the PASCAL VOC 2012 dataset [29], which contains the same 20 categories as
VOC 2010 but with additional images for each category. There’s a total of 4,369 images,
split into train (1,464), val (1,449), and test (1,456 images) subsets.
5.3.2 Hierarchical model
We reviewed the Associative Hierarchical CRF of Ladicky et al. [68] in § 1.1.1 and corre-
sponding ALE implementation, and showed that the oracle performance on DivMBest
segmentations improved by more than 20% over MAP on the val subset of PASCAL
VOC 2010. To evaluate how good the DivMBest+ReRank implementation is at return-
ing a high quality segmentation of the image we produce diverse segmentations and
rank them on images from the PASCAL VOC 2012 dataset.
5.3.3 Feed-forward model
We also compare against the Second-Order Pooling (O2P) implementation of Carreira
et al. [17] contrast with the Hierarchical CRF model above, in the way higher-order
dependencies are incorporated in the model. Whereas is ALE the higher-order inter-
actions are due to the hierarchical structure of the graph and high-order cliques, O2P
incorporates high-order dependencies using second-order pooling of local descriptors
over regions in the image.
2 Part of the contributions to the thesis presented in this section are found in [7], and are in collaboration
with Gregory Shakhnarovich and Dhruv Batra.
5.3 multi-category segmentation 111
InO2P, for each image location local descriptors such as SIFT and local binary patterns
(LBP) [85, 86] as well as color and location are densely computed. Given an initial set
of ? 150 candidate figure-ground masks for the image — produced using the bottom-
up CMPC segmentation algorithm [16] — the second-order statistics (i.e. vector outer-
products) of local descriptors that fall within each region are pooled (e.g. average/max)
to give global features capturing higher-order interactions between image elements.
These region-level features are fed to support-vector regressors (SVR) for each category,
that are trained to predict how well the region overlaps objects of that category.
The implementation uses a simple and efficient greedy inference strategy to produce
the final multi-category segmentation. Starting with an initial background threshold,
in decreasing order, the segment and category with highest score above the threshold
is pasted in the image. Segments with higher score are laid on top of segments with
lower score. Each time a segment is pasted the background threshold is increased,
and the process stops when there are no more segments with category score above
the threshold. The initial background threshold is set such that the average number
of segments with score above the threshold is roughly the same as the number of
objects per image in the training set. Note that we can reformulate this approach as a
CRF constructed on overlapping CPMC segments in the image with unary potentials
defined by the SVR category scores of each segment.
Through the use of a number of tricks to speedup computation, like caching pooling
results and dimensionality reduction on features (cf. [17]), the O2P implementation is
faster to train and run inference over than the ALE model.
5.3.4 Diversity and Oracles
For the analysis reported in this subsection, we used the VOC 2012 train and val
sets. ALE and O2P models were trained on VOC 2012 train, and the models were
used to produce 10 segmentations for each image in val. The Lagrangian multiplier
in the DivMBest formulation (cf. § 2.2.6) was tuned via cross-val (?ALE = 1.25 and
?O2P = 0.08).
Oracle Accuracies. Since ground-truth is known for VOC val images, we can find the
oracle accuracy, i.e. the accuracy of the best solution in the set, as described in § 4.1.2.
This accuracy is shown in figure 5.1 (lines with circles): with 10 solutions on O2P, it
reaches 60.12%, which is 15%-points higher the accuracy of MAP. Oracle accuracy with
ALE solutions show a similar increase w.r.t. ALE’s MAP.
To put these oracle numbers in context, we can try to find what is the best segmentation
accuracy achievable using the 150 CPMC segments for each image. To find a good
approximation of the best segmentation we can achieve, we can consider a greedy
algorithm that tries to find the subset of CPMC segments that best cover ground-truth
112 divmbest+rerank experiments
1 2 3 4 5 6 7 8 9 10
25
30
35
40
Number of Solutions
A
v
e
ra
g
e
 P
A
S
C
A
L
 A
c
c
u
ra
c
y
 
 
ALE?MAP
ALE?Oracle
ALE?Rerank
ALE?Classifier
ALE?Rand
(a)
1 2 3 4 5 6 7 8 9 10
45
50
55
60
Number of Solutions
A
v
e
ra
g
e
 P
A
S
C
A
L
 A
c
c
u
ra
c
y
 
 
O2P?MAP
O2P?Oracle
O2P?Rerank
O2P?Classifier
O2P?Rand
(b)
Figure 5.1: DivMBest+ReRank performance on PASCAL VOC 2012 val using (a) ALE and (b)
O2P models vs. the number of solutions.
5.3 multi-category segmentation 113
segments and then simply copies labels over from the ground-truth. This achieves
an accuracy of 80.78%. Notice that this procedure takes the supremum of accuracy
of exponentially many solutions, whereas DivMBest with 10 solutions reaches 60.12%,
closing the gap to within 21% points.
Diversity of solutions. We now turn to empirical analysis that quantifies the amount
of diversity in these solutions, and how that affects the oracle performance.
The first question to address is: how much diversity do the DivMBest solutions contain over
MAP? To answer this, we can look at the solution in the set that is most different from
MAP, as measured by average region overlap.
Let {s(m)i,1 , . . . , s
(m)
i,K } denote the set of K segments in the m
th solution for image i
and {s(1)i,1 , . . . , s
(1)
i,K ?} denote the set of segments in MAP. We can define a category-
independent covering measure, which for a given image i captures how much of the
MAP segmentation is covered by one of the subsequent solutions,
D1(y
(j)
i ) =
1?
k ?
|s
(1)
i,k ? |
K ??
k ?=1
|s
(1)
i,k ? | max
k?[K]
O(s
(1)
i,k ? , s
(j)
i,k), (5.2)
where |s(m)i,k | denotes the size of the segment and O(·, ·) is the intersection-over-union
measure of the two segments. For O2P these segments correspond to CPMC segments
[16], while in ALE the segments are connected components in the segmentation.
To get an idea of how different the most diverse solution is, we can define the minimum
cover of the MAP solution by the M segmentations for image i as:
D
(i,m)
1
.
= min
j=1,...,m
{D1(y
(j)
i )}. (5.3)
A plot of average minimum diversity in the dataset, i.e.
?
iD
(i,m)
1 /n for m = 1, . . . , 10
is shown in figure 5.2a. We can see that both models produce at least one solution that
is significantly different from the MAP. With 10 solutions, the minimum covering of
MAP drops to about 0.3 for O2P and 0.1 for ALE. Thus, on average at least one out of
10 DivMBest solutions for O2P overlaps MAP by only 10%.
Diversity of Oracle. Of course, diversity is useful only if it brings improved quality.
The previous measure simply captures diversity and can be easily affected by poor
quality solutions that are different from MAP. We can also try to characterize the di-
versity in the oracle solutions. This measure tells us how different the oracle solution
is from the MAP solution on average. Analogous to eqn. 5.2 we can compute D1(y
(?)
i )
to measure by how much the segments in the oracle segmentation cover the MAP seg-
ments. We can also use a category-specific covering measure which takes into account
label agreement to get a measure of how much the MAP segments are covered with
same labelled segments in the oracle solution,
D2(y
(?)
i ) =
1?
k ?
|s
(1)
i,k ? |
K ??
k ?=1
|s
(1)
i,k ? | maxk?[K]:
y
(?)
i,k=y
(1)
i,k ?
O(s
(1)
i,k ? , s
?
i,k), (5.4)
114 divmbest+rerank experiments
D1 Oracle Covering D2 Oracle Covering
ALE 0.55 0.45
O2P 0.61 0.58
Table 5.3: Average covering score between oracle solutions and MAP: (left) show the category-
independent measure and (right) shows the category-specific measure.
where y(?)i,k and y
(1)
i,k ? are the labels of the oracle and MAP segments respectively.
Table 5.3 summarizes these results which show that the oracle segmentations are not
simply minor perturbations of the MAP segmentations. On average the MAP covering
by oracle irrespective of segment label is less than 61% for O2P and 55% for ALE. If
we constrain the covering to be category-consistent, these numbers drop to 58% and
45% respectively. Thus, we can conclude that the oracle segmentations are not simply
minor perturbations of the MAP.
Gain from diversity.
The previous measure tells us that the oracle solution is indeed quite different from the
MAP. We now try to study how it is different – do the additional solutions introduce
new categories or new masks or both? In order to answer this question, we measure
the performance of a restricted oracle that chooses in each additional solution the best
label possible for all segments, albeit restricted to the set of labels found in MAP.
Specifically, if a segment s(j)i,k overlaps with the ground-truth background by more
than 50%, then we set its label to background. Otherwise if there is a segment gi,l ?
ygti , where y(gi,l) ? yMAP (where yMAP
.
= {y(s
(1)
i,k )|k ? [K]}), with s
(j)
i,k ? gi,l 6= ?,
we set y(s(j)i,k) = y(gi,l). If there is no such gi,l then y(s
(j)
i,k) is assigned a random
label from yMAP. Figure 5.2b shows that such a restricted oracle (O2P-oracle-label
and ALE-oracle-label) performs worse than the unrestricted oracle, indicating that the
additional solutions do in fact introduce categories present in ground-truth but not in
MAP.
Similarly, we can restrict the oracle to only take segment masks found in MAP and
assign to them the best possible labels found in y(m)i . Again figure 5.2c shows that
such a restricted oracle significantly under-performs, indicating the MAP masks are
not ideal and that the additional solutions do in fact introduce useful masks.
Thus, we can conclude that there are clear differences in both the labels and segments
of the oracle segmentations compared to the MAP.
5.3 multi-category segmentation 115
1 2 3 4 5 6 7 8 9 10
0
0.2
0.4
0.6
0.8
1
Number of Solutions
M
in
im
u
m
 C
o
v
e
ri
n
g
 o
f 
M
A
P
 
 
O2P
ALE
(a)
1 2 3 4 5 6 7 8 9 10
20
30
40
50
60
Number of Solutions
A
v
e
ra
g
e
 P
A
S
C
A
L
 A
c
c
u
ra
c
y
 
 
O2P-oracle-label
O2p-oracle
ALE-oracle-label
ALE-oracle
(b)
1 2 3 4 5 6 7 8 9 10
20
30
40
50
60
Number of Solutions
A
v
e
ra
g
e
 P
A
S
C
A
L
 A
c
c
u
ra
c
y
 
 
O2P-oracle-mask
O2p-oracle
ALE-oracle-mask
ALE-oracle
(c)
Figure 5.2: (a) Average minimum-covering (5.3) of MAP in the first j 6 10 solutions vs. j. (b)
Accuracy of an oracle restricted to labels present in the MAP, or (c) restricted to
masks present in MAP. See text for details.
116 divmbest+rerank experiments
Ba
ck
gr
.
Pl
an
e
Bi
cy
cl
e
Bi
rd
Bo
at
Bo
tt
le
Bu
s
C
ar
C
at
C
ha
ir
C
ow
D
.T
ab
le
D
og
H
or
se
M
.b
ik
e
Pe
rs
on
Pl
an
t
Sh
ee
p
So
fa
Tr
ai
n
TV
.M
o.
A
ve
ra
ge
O2P-MAP 84.8 63.7 23.4 44.9 40.8 45.1 58.0 58.8 57.6 12.1 43.8 31.0 44.8 56.2 56.8 52.3 37.1 44.0 29.5 48.6 42.9 46.5
DivMBest+ReRank 85.7 62.7 25.6 46.9 43.0 54.8 58.4 58.6 55.6 14.6 47.5 31.2 44.7 51.0 60.9 53.5 36.6 50.9 30.1 50.2 46.8 48.1
Table 5.4: PASCAL VOC 2012 test set accuracies.
5.3.5 Re-ranker features
Our re-ranker uses a number of features that we separate into a few groups. In the
discussion, below we say a label c is present in y if at least one pixel in y is labeled c.
Model features rely on properties derived from the model that produced segmentation
y– model score of y, average pixel score, number of CPMC masks used to construct
foreground, the final background threshold at the end of the greedy foreground as-
sembly, and the rank of y among the m diverse hypotheses for the given input image.
(5 dimensions)
Diversity features measure average per pixel agreement of y with the majority vote
by the diverse set (weighted or unweighted by the model scores). (2 dimensions)
Recognition features. We use outputs of object detectors from [83] to get detector-
based segmentations D1,D2, where each pixel is assigned by majority vote on detec-
tion scores (thresholded & un-thresholded). Then we compute the agreement matrix:
for every c1, c2 we count pixels assigned to c1 by y and to c2 by D1, yielding a 441-
dimensional feature. We compute max/median/min of the detection score (with and
without thresholding) for every category in y (120 dims); the average overlap between
category masks in D1,D2 and in y (2 dims); and pixelwise average detector scores
for categories in y (2 dims). We also use the the estimated posterior for each category
present in y, using the classifier from [115] (20 dimensions).
Segment features measure the geometric properties of the segments in y: perimeter,
area, and the ratio of the two; computed separately for segments in every class and for
the entire foreground (63 dimensions). Relative location of the centroids of masks for
each category pair (420 dimensions).
Label features rely on information regarding the labels assigned to masks in y, but not
the geometry of these masks. For every pair of labels c1, c2 we compute the binary co-
occurrence (1 if both categories are present in y) and the percentage of pixels assigned
to c1 & c2. (420 dimensions)
All the features above are independent of the image x; the following features rely on
image measurements as well as properties of the solution y.
5.3 multi-category segmentation 117
Boundary features. We compute the total globalPb probability of boundary response [3]
in a band along the category boundaries; for 3 widths of the band, this produces a 3-
dimensional feature (with 3 more for normalized versions). We also compute recall by
the globalPb map of the category boundaries in the y; this produces a 10 dimensional
feature for ten equally spaced precision values. Finally, we compute the histogram
(6 bins) of Chamfer distance between the boundaries in y and the thresholded glob-
alPb, and vice versa; with 10 thresholds this produces a 120 dimensional feature. For
each category, we also computed normalized histogram of globalPb responses in the
non-boundary regions (210 dims).
Entropy features. For every category (and the combined foreground) we measure the
entropy of color histograms, computed per color channel with two binning resolutions,
yielding 126 dimensions. We do the same for textons [106, 107], with a single binning,
for another 21 features.
We stress that most of these features rely on higher-order information that would be
intractable to incorporate into the CRF model used in stage 1. For instance, using
features that refer to segment boundaries is hard in CRF. However, evaluating these
features on m segmentations is easy, which allows us to use them at the re-ranking
stage.
5.3.6 Re-ranker training
The combined feature vector per solution y has 1988 dimensions. The only hyper-
parameter for the re-ranker is the regularization parameter C (3.11), which is chosen
via cross-validation on the val set9. 9 We also used
cross-validation to
evaluate the feature
set, rejecting some
additional features
not listed here that
did not contribute to
re-ranking accuracy.
One important practical question is how many diverse solutions to use. While we have
seen above that the oracle accuracy increases through M = 30 solutions, it is possible
that too many solutions make it hard to train an effective re-ranker. Indeed, we found
that the best results in cross-validation are obtained when training on 10 solutions per
image; we use the same number of diverse solutions per image when re-ranking the
test segmentations.
5.3.7 Re-ranker results
The performance of the DivMBest+ReRank pipeline on the O2P and ALE models
(Rerank), as the size of the DivMBest set grows, is reported in figure 5.1 along with
MAP and oracle accuracies. The plot also shows results of a binary classifier baseline
(Classifier) that is trained to discriminate between the best and worst segmentations
118 divmbest+rerank experiments
Binary Task Accuracies Pascal VOC Avg. Acc.
B-vs-W M-vs-W B-vs-M Best MAP Worst HR
ALE 71.9 64.4 61.7 38.0 19.1 3.2 20.5
O2P 73.9 73.1 56.3 62.8 43.6 24.5 49.0
Table 5.5: (left) Human accuracy in predicting (B)est-vs-(W)orst, (M)AP-vs-(W)orst, and (B)est-
vs-(M)AP solutions. (right) Pascal VOC accuracies over 150 images for best, MAP,
worst, and human response (HR) solutions.
in the set, and used at test time to re-rank according to classification score. As a second
baseline, we compare against randomly picking one out of the j 6 m segmentations
(Rand).
On PASCAL VOC 2012 val, the MAP segmentation IoU accuracy is 24.3% on ALE
and 45.1% on O2P. In contrast DivMBest+ReRank achieves 29.27% on ALE and 48.2%
on O2P, an increase of > 5% and > 3%-points respectively. Table 5.4 shows VOC
2012 test set performance of the O2P ranker when trained on the val set. O2P–
DivMBest+ReRank achieves a 1.6%-point performance improvement overO2P–MAP10.10 this was
state-of-the-art
results on PASCAL
VOC 2012 comp6
challenge at time of
experiments
A few examples where O2P–DivMBest+ReRank beats O2P–MAP are shown in fig-
ure 5.4. More examples can be found in Appendix B.2.
5.3.8 Re-ranker Analysis
We can consider how the re-ranker behaves in picking a solution from the DivMBest
set. In the original ranking of DivMBest solutions (i.e. order in which they were gener-
ated) figure 5.3a shows the number of images in which the oracle solution is at rank j,
for j ? {1, . . . , 10}. The oracle distribution has a heavy tail, indicating that high-quality
solutions are often found at the bottom of the list. Figure 5.3b shows the number of
images where the top re-ranked solution was originally at rank j. The re-ranked dis-
tribution has a much lighter tail which suggest that the re-ranker "plays it safe" and
often predicts MAP. The correlation between segmentation quality and re-ranker score
is shown in figure 5.3c, which indicates that the re-ranker score is well correlated with
solution quality.
5.3.9 Human Ranking Experiments
To evaluate and characterize the difficulty of the re-ranking problem we can investigate
how well people perform the task of picking a good segmentation — which symbolizes
the gold standard. 150 images were chosen from PASCAL VOC 2012 val set where
5.3 multi-category segmentation 119
the MAP segmentation was neither the worst nor best segmentation. On Amazon
Mechanical Turk (AMT), subjects were presented with three different types of binary
comparison tasks for each image: comparing Best-vs-MAP, Best-vs-Worst, and MAP-vs-
Worst segmentations in the DivMBest set. The subjects had to make the choice using
only the labellings (with category names annotated) and not the image. The subjects
were also presented with the option to provide feedback on reasons for their choice.
Figure 5.5 shows the interface with actual examples of results from AMT workers. The
workers’ comments illustrate that people are very good at discriminating good versus
bad segmentations using cues such as category co-occurrence (figure 5.5a), category
shape (figure 5.5b), and part-vs-whole relashionships (figure 5.5c). These cues provide
evidence for our choice of re-ranker features. A summary of how well subjects did on
the three tasks is shown in Table 5.5. The most difficult binary task for subjects was
choosing between Best and MAP segmentations. In the case of the O2P model picking
between MAP and Best is even more difficult that for ALE because the MAP solutions
are better for the O2P model. Note that the segmentations picked by humans (HR)
achieve substantial improvement over MAP, which is significant given that the choice
is made without seeing the original image.
5.3.10 Summary
The analysis in this section on a number of segmentation tasks shows that the set of
solutions obtained from the DivMBest stage are of significantly higher quality than
the MAP solutions. The source of diversity between solutions is also non-trivial. Re-
ranking the DivMBest set results in significant performance improvement over MAP.
The results also highlight the importance of choosing re-ranking features that can dis-
criminate good versus bad solutions within the DivMBest set. Learning rich features
for the within set discrimination task is an area for future research.
120 divmbest+rerank experiments
1 2 3 4 5 6 7 8 9 10
0
100
200
300
400
500
600
700
Solution
N
u
m
b
e
r 
o
f 
Im
a
g
e
s
(a) Oracle Solution Rank Histogram.
1 2 3 4 5 6 7 8 9 10
0
100
200
300
400
500
600
700
Solution
N
u
m
b
e
r 
o
f 
Im
a
g
e
s
(b) Predicted Solution Rank Histogram.
0 2 4 60
20
40
60
80
100
Re ranker Score
Ac
cu
ra
cy
 (%
)
Correlation Coeffient: 0.51
(c) Re-ranker Score vs Solution Accuracy.
Figure 5.3: Statistics on PASCAL VOC 2012 val with O2P model: (a),(b) show the number of
images in which the oracle / top-re-ranked solution was originally at rank j 6 10.
We can see that there is a heavy tail in the oracle distribution, but a much lighter
tail in the re-ranker, suggesting that the re-ranker “plays it safe” and predicts MAP
very frequently; (c) shows a scatter plot of re-ranker score vs solution accuracy.
5.3 multi-category segmentation 121
Figure 5.4: Cases where O2P-DivMBest+ReRank outperforms O2P-MAP. In each group of im-
ages, the first column shows the original image followed by the ground-truth, MAP,
and top re-ranked solution returned by DivMBest+ReRank. PASCAL intersection-
over-union accuracy is shown below the segmentations.
122 divmbest+rerank experiments
(a)
(b)
(c)
Figure 5.5: Example MTurk tasks along with user-provided responses which were instructive
in the creation of segmentation-specific features.
6
C O N C L U S I O N
In summary, this thesis presents an approach to obtaining performance gains from a
semantic segmentation model. Instead of achieving gains by opting for a more complex
model which would be more expensive or possibly intractable to optimize over the
gains are achieved through a careful redesign of the inference procedure that leverages
diversity between output labellings. The thesis also outlines an approach to ranking
these DivMBest segmentations to automatically pick the best from the set. It contains
the following contributions:
• A framework (DivMBest) for inferring multiple highly probable yet diverse seg-
mentations from a probabilistic structured output model. It is motivated and
derived from the integer programming problem for solving inference on proba-
bilistic graphical models with discrete output space. The DivMBest framework
is generally applicable in any setting where you have an inference model over
a structured output label distribution. What results is an elegant and practical
iterative algorithm for inference that is more akin to finding modes of the out-
put space distribution. It reuses the inference procedure from the original model,
providing means for improved prediction without incurring any cost in tractabil-
ity.
• The DivMBest formulation can accept different measures of diversity between
“modes“, such as Hamming and cardinality distance. Finding modes of a CRF un-
der Hamming dissimilarity amounts to only modifying the unary energy terms
and reusing the same MAP inference machinery to compute subsequent solu-
tions, yielding an approach that is as efficient as the underlying MAP inference
algorithm.
• A discriminative large margin approach to ranking the DivMBest segmentations
(DivMBest+ReRank) is introduced that allows for arbitrarily complex features
to evaluate each segmentation.
• Evaluation of the DivMBest and DivMBest+ReRank algorithms on a number
of semantic image labelling problems including interactive, figure-ground, and
multi-category segmentation. The results provide evidence of the benefits these
approaches offer for the segmentation task.
123
124 conclusion
• Oracle experiments on semantic segmentation show that the DivMBest approach
has the potential to achieve results comparable or better than even existing state-
of-the-art CNN models for segmentation.
• Application of the DivMBest framework to present CNN+dense CRF models
for segmentation which shows that the DivMBest and DivMBest+ReRank algo-
rithms are very much relevant to the current trends in semantic segmentation.
B I B L I O G R A P H Y
[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua,
and Sabine Süsstrunk. Slic superpixels. Tech. rep. 2010.
[2] Andrew Adams, Jongmin Baek, and Myers Abraham Davis. “Fast High-Dimensional
Filtering Using the Permutohedral Lattice.” In: Computer Graphics Forum. Vol. 29.
2. Wiley Online Library. 2010, pp. 753–762.
[3] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. “Con-
tour detection and hierarchical image segmentation.” In: IEEE transactions on
pattern analysis and machine intelligence 33.5 (2011), pp. 898–916.
[4] Pablo Arbeláez, Bharath Hariharan, Chunhui Gu, Saurabh Gupta, Lubomir
Bourdev, and Jitendra Malik. “Semantic segmentation using regions and parts.”
In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on.
IEEE. 2012, pp. 3378–3385.
[5] Adrian Barbu and Song-Chun Zhu. “Generalizing Swendsen-Wang to sampling
arbitrary posterior probabilities.” In: IEEE Transactions on Pattern Analysis and
Machine Intelligence 27.8 (2005), pp. 1239–1253.
[6] Dhruv Batra, Adarsh Kowdle, Devi Parikh, Jiebo Luo, and Tsuhan Chen. “icoseg:
Interactive co-segmentation with intelligent scribble guidance.” In: Computer Vi-
sion and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE. 2010, pp. 3169–
3176.
[7] Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich.
“Diverse m-best solutions in markov random fields.” In: European Conference on
Computer Vision. Springer. 2012, pp. 1–16.
[8] Dimitri P Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
[9] Xavier Boix, Josep M Gonfaus, Joost Van de Weijer, Andrew D Bagdanov, Joan
Serrat, and Jordi Gonzàlez. “Harmony potentials.” In: International journal of
computer vision 96.1 (2012), pp. 83–102.
[10] Eran Borenstein and Shimon Ullman. “Combined top-down/bottom-up seg-
mentation.” In: IEEE Transactions on pattern analysis and machine intelligence 30.12
(2008), pp. 2109–2125.
[11] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
125
126 Bibliography
[12] Yuri Y Boykov and M-P Jolly. “Interactive graph cuts for optimal boundary
& region segmentation of objects in ND images.” In: Computer Vision, 2001.
ICCV 2001. Proceedings. Eighth IEEE International Conference on. Vol. 1. IEEE. 2001,
pp. 105–112.
[13] Yuri Boykov and Vladimir Kolmogorov. “An experimental comparison of min-
cut/max-flow algorithms for energy minimization in vision.” In: IEEE transac-
tions on pattern analysis and machine intelligence 26.9 (2004), pp. 1124–1137.
[14] Jaime Carbonell and Jade Goldstein. “The use of MMR, diversity-based rerank-
ing for reordering documents and producing summaries.” In: Proceedings of the
21st annual international ACM SIGIR conference on Research and development in
information retrieval. ACM. 1998, pp. 335–336.
[15] João Carreira, Fuxin Li, and Cristian Sminchisescu. “Object recognition by se-
quential figure-ground ranking.” In: International journal of computer vision 98.3
(2012), pp. 243–262.
[16] Joao Carreira and Cristian Sminchisescu. “Constrained parametric min-cuts
for automatic object segmentation.” In: Computer Vision and Pattern Recognition
(CVPR), 2010 IEEE Conference on. IEEE. 2010, pp. 3241–3248.
[17] Joao Carreira, Rui Caseiro, Jorge Batista, and Cristian Sminchisescu. “Semantic
segmentation with second-order pooling.” In: European Conference on Computer
Vision. Springer. 2012, pp. 430–443.
[18] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and
Alan L Yuille. “Semantic image segmentation with deep convolutional nets and
fully connected crfs.” In: arXiv preprint arXiv:1412.7062 (2014).
[19] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and
Alan L Yuille. “Deeplab: Semantic image segmentation with deep convolu-
tional nets, atrous convolution, and fully connected crfs.” In: arXiv preprint
arXiv:1606.00915 (2016).
[20] Yizong Cheng. “Mean shift, mode seeking, and clustering.” In: IEEE transactions
on pattern analysis and machine intelligence 17.8 (1995), pp. 790–799.
[21] Michael Collins and Terry Koo. “Discriminative reranking for natural language
parsing.” In: Computational Linguistics 31.1 (2005), pp. 25–70.
[22] Michael Collins, Brian Roark, and Murat Saraclar. “Discriminative syntactic lan-
guage modeling for speech recognition.” In: Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics. Association for Computational
Linguistics. 2005, pp. 507–514.
[23] Dorin Comaniciu and Peter Meer. “Mean shift: A robust approach toward fea-
ture space analysis.” In: IEEE Transactions on pattern analysis and machine intelli-
gence 24.5 (2002), pp. 603–619.
Bibliography 127
[24] Navneet Dalal and Bill Triggs. “Histograms of oriented gradients for human
detection.” In: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on. Vol. 1. IEEE. 2005, pp. 886–893.
[25] A Philip Dawid. “Applications of a general propagation algorithm for proba-
bilistic expert systems.” In: Statistics and computing 2.1 (1992), pp. 25–36.
[26] Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. “Discriminative
reranking for spoken language understanding.” In: IEEE Transactions on Audio,
Speech, and Language Processing 20.2 (2012), pp. 526–539.
[27] Ian Endres and Derek Hoiem. “Category independent object proposals.” In:
European Conference on Computer Vision. Springer. 2010, pp. 575–588.
[28] Ian Endres and Derek Hoiem. “Category-independent object proposals with
diverse ranking.” In: IEEE transactions on pattern analysis and machine intelligence
36.2 (2014), pp. 222–234.
[29] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and
Andrew Zisserman. “The pascal visual object classes (voc) challenge.” In: Inter-
national journal of computer vision 88.2 (2010), pp. 303–338.
[30] Pedro F Felzenszwalb and Daniel P Huttenlocher. “Efficient graph-based image
segmentation.” In: International journal of computer vision 59.2 (2004), pp. 167–
181.
[31] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan.
“Object detection with discriminatively trained part-based models.” In: IEEE
transactions on pattern analysis and machine intelligence 32.9 (2010), pp. 1627–1645.
[32] Yoav Freund and Robert E Schapire. “A desicion-theoretic generalization of
on-line learning and an application to boosting.” In: European conference on com-
putational learning theory. Springer. 1995, pp. 23–37.
[33] Yoav Freund, Robert Schapire, and Naoki Abe. “A short introduction to boost-
ing.” In: Journal-Japanese Society For Artificial Intelligence 14.771-780 (1999), p. 1612.
[34] Menachem Fromer and Amir Globerson. “An LP View of the M-best MAP
problem.” In: Advances in Neural Information Processing Systems. 2009, pp. 567–
575.
[35] Keinosuke Fukunaga and Larry Hostetler. “The estimation of the gradient of a
density function, with applications in pattern recognition.” In: IEEE Transactions
on information theory 21.1 (1975), pp. 32–40.
[36] Arthur M Geoffrion. “Lagrangean relaxation for integer programming.” In: Ap-
proaches to integer programming. Springer, 1974, pp. 82–114.
[37] Chunhui Gu, Joseph J Lim, Pablo Arbeláez, and Jitendra Malik. “Recognition
using regions.” In: Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on. IEEE. 2009, pp. 1030–1037.
[38] Monique Guignard. “Lagrangean relaxation.” In: Top 11.2 (2003), pp. 151–200.
128 Bibliography
[39] Rahul Gupta, Sunita Sarawagi, and Ajit A Diwan. “Collective inference for ex-
traction mrfs coupled with symmetric clique potentials.” In: Journal of Machine
Learning Research 11.Nov (2010), pp. 3097–3135.
[40] Saurabh Gupta, Ross Girshick, Pablo Arbeláez, and Jitendra Malik. “Learning
rich features from RGB-D images for object detection and segmentation.” In:
European Conference on Computer Vision. Springer. 2014, pp. 345–360.
[41] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and
Jitendra Malik. “Semantic Contours from Inverse Detectors.” In: International
Conference on Computer Vision (ICCV). 2011.
[42] Bharath Hariharan, Pablo Arbeláez, Ross Girshick, and Jitendra Malik. “Hyper-
columns for object segmentation and fine-grained localization.” In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. 2015, pp. 447–
456.
[43] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Deep residual
learning for image recognition.” In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition. 2016, pp. 770–778.
[44] Derek Hoiem, Alexei A Efros, and Martial Hebert. “Geometric context from
a single image.” In: Computer Vision, 2005. ICCV 2005. Tenth IEEE International
Conference on. Vol. 1. IEEE. 2005, pp. 654–661.
[45] Derek Hoiem, Andrew N Stein, Alexei A Efros, and Martial Hebert. “Recover-
ing occlusion boundaries from a single image.” In: Computer Vision, 2007. ICCV
2007. IEEE 11th International Conference on. IEEE. 2007, pp. 1–8.
[46] IBM, Inc. IBM ILOG CPLEX: High-performance mathematical programming solver
for linear programming, mixed integer programming, and quadratic programming.
See https://www- 01.ibm.com/software/commerce/optimization/cplex-
optimizer/. 2017.
[47] Finn V Jensen, Steffen L Lauritzen, and Kristian G Olesen. “Bayesian updat-
ing in causal probabilistic networks by local computations.” In: Computational
statistics quarterly 4 (1990), pp. 269–282.
[48] Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. “Cutting-plane
training of structural SVMs.” In: Machine Learning 77.1 (2009), pp. 27–59.
[49] Jaechul Kim and Kristen Grauman. “Shape sharing for object segmentation.”
In: European Conference on Computer Vision. Springer. 2012, pp. 444–458.
[50] Alexander Kirillov, Bogdan Savchynskyy, Dmitrij Schlesinger, Dmitry Vetrov,
and Carsten Rother. “Inferring M-best diverse labelings in a single one.” In:
Proceedings of the IEEE International Conference on Computer Vision. 2015, pp. 1814–
1822.
Bibliography 129
[51] Alexander Kirillov, Dmytro Shlezinger, Dmitry P Vetrov, Carsten Rother, and
Bogdan Savchynskyy. “M-best-diverse labelings for submodular energies and
beyond.” In: Advances in Neural Information Processing Systems. 2015, pp. 613–
621.
[52] Alexander Kirillov, Alexander Shekhovtsov, Carsten Rother, and Bogdan Savchyn-
skyy. “Joint M-Best-Diverse Labelings as a Parametric Submodular Minimiza-
tion.” In: Advances in Neural Information Processing Systems. 2016, pp. 334–342.
[53] Timo Kohlberger, Vivek Singh, Chris Alvino, Claus Bahlmann, and Leo Grady.
“Evaluating segmentation error without ground truth.” In: International Con-
ference on Medical Image Computing and Computer-Assisted Intervention. Springer.
2012, pp. 528–536.
[54] Pushmeet Kohli and M Pawan Kumar. “Energy minimization for linear enve-
lope MRFs.” In: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Con-
ference on. IEEE. 2010, pp. 1863–1870.
[55] Pushmeet Kohli, M Pawan Kumar, and Philip HS Torr. “P3 & beyond: Solving
energies with higher order cliques.” In: Computer Vision and Pattern Recognition,
2007. CVPR’07. IEEE Conference on. IEEE. 2007, pp. 1–8.
[56] Pushmeet Kohli, M Pawan Kumar, and Philip HS Torr. “P3 & Beyond: Move
Making Algorithms for Solving Higher Order Functions.” In: IEEE Transactions
on Pattern Analysis and Machine Intelligence 31.9 (2009), pp. 1645–1656.
[57] Pushmeet Kohli and Philip HS Torr. “Efficiently solving dynamic markov ran-
dom fields using graph cuts.” In: Tenth IEEE International Conference on Computer
Vision (ICCV’05) Volume 1. Vol. 2. IEEE. 2005, pp. 922–929.
[58] Pushmeet Kohli and Philip HS Torr. “Measuring uncertainty in graph cut solu-
tions.” In: Computer Vision and Image Understanding 112.1 (2008), pp. 30–38.
[59] Pushmeet Kohli, Philip HS Torr, et al. “Robust higher order potentials for en-
forcing label consistency.” In: International Journal of Computer Vision 82.3 (2009),
pp. 302–324.
[60] Pushmeet Kohli, Philip HS Torr, et al. “Robust higher order potentials for en-
forcing label consistency.” In: International Journal of Computer Vision 82.3 (2009),
pp. 302–324.
[61] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and
techniques. MIT press, 2009.
[62] Vladimir Kolmogorov and Ramin Zabin. “What energy functions can be min-
imized via graph cuts?” In: IEEE transactions on pattern analysis and machine
intelligence 26.2 (2004), pp. 147–159.
[63] Nikos Komodakis and Nikos Paragios. “Beyond pairwise energies: Efficient op-
timization for higher-order MRFs.” In: Computer Vision and Pattern Recognition,
2009. CVPR 2009. IEEE Conference on. IEEE. 2009, pp. 2985–2992.
130 Bibliography
[64] Nikos Komodakis, Nikos Paragios, and Georgios Tziritas. “MRF optimization
via dual decomposition: Message-passing revisited.” In: 2007 IEEE 11th Interna-
tional Conference on Computer Vision. IEEE. 2007, pp. 1–8.
[65] Philipp Krähenbühl and Vladlen Koltun. “Efficient Inference in Fully Con-
nected CRFs with Gaussian Edge Potentials.” In: NIPS. 2011.
[66] Daniel Kuettel and Vittorio Ferrari. “Figure-ground segmentation by transfer-
ring window masks.” In: Computer Vision and Pattern Recognition (CVPR), 2012
IEEE Conference on. IEEE. 2012, pp. 558–565.
[67] L’ubor Ladicky? and Philip HS Torr. The Automatic Labelling Environment. 2012.
[68] L’ubor Ladicky?, Chris Russell, Pushmeet Kohli, Philip HS Torr, et al. “Associa-
tive hierarchical crfs for object class image segmentation.” In: Computer Vision,
2009 IEEE 12th International Conference on. IEEE. 2009, pp. 739–746.
[69] L’ubor Ladicky?, Chris Russell, Pushmeet Kohli, and Philip HS Torr. “Graph
cut based inference with co-occurrence statistics.” In: European Conference on
Computer Vision. Springer. 2010, pp. 239–253.
[70] L’ubor Ladicky?, Chris Russell, Pushmeet Kohli, and Philip HS Torr. “Graph
cut based inference with co-occurrence statistics.” In: European Conference on
Computer Vision. Springer. 2010, pp. 239–253.
[71] L’ubor Ladicky?, Paul Sturgess, Karteek Alahari, Chris Russell, and Philip HS
Torr. “What, where and how many? combining object detectors and crfs.” In:
European conference on computer vision. Springer. 2010, pp. 424–437.
[72] Lubor Ladicky?, Paul Sturgess, Chris Russell, Sunando Sengupta, Yalin Bastan-
lar, William Clocksin, and Philip HS Torr. “Joint optimization for object class
segmentation and dense stereo reconstruction.” In: International Journal of Com-
puter Vision (2012), pp. 1–12.
[73] Steffen L Lauritzen and David J Spiegelhalter. “Local computations with prob-
abilities on graphical structures and their application to expert systems.” In:
Journal of the Royal Statistical Society. Series B (Methodological) (1988), pp. 157–
224.
[74] Eugene L Lawler. “A procedure for computing the k best solutions to discrete
optimization problems and its application to the shortest path problem.” In:
Management science 18.7 (1972), pp. 401–405.
[75] Jonathan Long, Evan Shelhamer, and Trevor Darrell. “Fully convolutional net-
works for semantic segmentation.” In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition. 2015, pp. 3431–3440.
[76] David G Lowe. “Object recognition from local scale-invariant features.” In: Com-
puter vision, 1999. The proceedings of the seventh IEEE international conference on.
Vol. 2. Ieee. 1999, pp. 1150–1157.
Bibliography 131
[77] Michael Maire, Pablo Arbeláez, Charless Fowlkes, and Jitendra Malik. “Using
contours to detect and localize junctions in natural images.” In: Computer Vision
and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE. 2008, pp. 1–
8.
[78] Jitendra Malik, Serge Belongie, Jianbo Shi, and Thomas Leung. “Textons, con-
tours and regions: Cue integration in image segmentation.” In: Computer Vision,
1999. The Proceedings of the Seventh IEEE International Conference on. Vol. 2. IEEE.
1999, pp. 918–925.
[79] Marcin Marszalek and Cordelia Schmid. “Accurate object localization with
shape masks.” In: Computer Vision and Pattern Recognition, 2007. CVPR’07. IEEE
Conference on. IEEE. 2007, pp. 1–8.
[80] Jiri Matas, Ondrej Chum, Martin Urban, and Tomás Pajdla. “Robust wide-
baseline stereo from maximally stable extremal regions.” In: Image and vision
computing 22.10 (2004), pp. 761–767.
[81] Mohammadreza Mostajabi and Iman Gholampour. “A framework based on the
Affine Invariant Regions for improving unsupervised image segmentation.” In:
ISSPA. 2012.
[82] Mohammadreza Mostajabi, Payman Yadollahpour, and Gregory Shakhnarovich.
“Feedforward semantic segmentation with zoom-out features.” In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. 2015, pp. 3376–
3385.
[83] Roozbeh Mottaghi. “Augmenting deformable part models with irregular-shaped
object patches.” In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE
Conference on. IEEE. 2012, pp. 3116–3123.
[84] Dennis Nilsson. “An efficient algorithm for finding the M most probable config-
urationsin probabilistic expert systems.” In: Statistics and computing 8.2 (1998),
pp. 159–173.
[85] Timo Ojala, Matti Pietikäinen, and David Harwood. “A comparative study of
texture measures with classification based on featured distributions.” In: Pat-
tern recognition 29.1 (1996), pp. 51–59.
[86] Timo Ojala, Matti Pietikainen, and Topi Maenpaa. “Multiresolution gray-scale
and rotation invariant texture classification with local binary patterns.” In: IEEE
Transactions on pattern analysis and machine intelligence 24.7 (2002), pp. 971–987.
[87] George Papandreou and Alan L Yuille. “Perturb-and-map random fields: Using
discrete optimization to learn and sample from energy models.” In: Computer
Vision (ICCV), 2011 IEEE International Conference on. IEEE. 2011, pp. 193–200.
[88] Dennis Park and Deva Ramanan. “N-best maximal decoders for part models.”
In: 2011 International Conference on Computer Vision. IEEE. 2011, pp. 2627–2634.
132 Bibliography
[89] Jacob Porway and Song-Chun Zhu. “Cˆ 4: Exploring Multiple Solutions in
Graphical Models by Cluster Sampling.” In: IEEE transactions on pattern anal-
ysis and machine intelligence 33.9 (2011), pp. 1713–1727.
[90] Ruifang Ge Raymond and J Mooney. “Discriminative reranking for semantic
parsing.” In: Proceedings of the COLING/ACL on Main conference poster sessions.
Association for Computational Linguistics. 2006, pp. 263–270.
[91] Jos BTM Roerdink and Arnold Meijster. “The watershed transform: Definitions,
algorithms and parallelization strategies.” In: Fundamenta informaticae 41.1, 2
(2000), pp. 187–228.
[92] Amir Rosenfeld and Daphna Weinshall. “Extracting foreground masks towards
object recognition.” In: Computer Vision (ICCV), 2011 IEEE International Confer-
ence on. IEEE. 2011, pp. 1371–1378.
[93] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. “Grabcut: Interac-
tive foreground extraction using iterated graph cuts.” In: ACM transactions on
graphics (TOG). Vol. 23. 3. ACM. 2004, pp. 309–314.
[94] Carsten Rother, Tom Minka, Andrew Blake, and Vladimir Kolmogorov. “Coseg-
mentation of image pairs by histogram matching-incorporating a global con-
straint into mrfs.” In: Computer Vision and Pattern Recognition, 2006 IEEE Com-
puter Society Conference on. Vol. 1. IEEE. 2006, pp. 993–1000.
[95] Carsten Rother, Pushmeet Kohli, Wei Feng, and Jiaya Jia. “Minimizing sparse
higher order energy functions of discrete variables.” In: Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE. 2009, pp. 1382–
1389.
[96] Olga Russakovsky et al. “ImageNet Large Scale Visual Recognition Challenge.”
In: International Journal of Computer Vision (IJCV) 115.3 (2015), pp. 211–252. doi:
10.1007/s11263-015-0816-y.
[97] Bryan C Russell, William T Freeman, Alexei A Efros, Josef Sivic, and Andrew
Zisserman. “Using multiple segmentations to discover objects and their extent
in image collections.” In: Computer Vision and Pattern Recognition, 2006 IEEE
Computer Society Conference on. Vol. 2. IEEE. 2006, pp. 1605–1614.
[98] Chris Russell, Pushmeet Kohli, Philip HS Torr, et al. “Exact and approximate in-
ference in associative hierarchical networks using graph cuts.” In: arXiv preprint
arXiv:1203.3512 (2012).
[99] Chris Russell, Pushmeet Kohli, Philip HS Torr, et al. “Exact and approximate in-
ference in associative hierarchical networks using graph cuts.” In: arXiv preprint
arXiv:1203.3512 (2012).
[100] Koen EA Van de Sande, Jasper RR Uijlings, Theo Gevers, and Arnold WM
Smeulders. “Segmentation as selective search for object recognition.” In: Com-
puter Vision (ICCV), 2011 IEEE International Conference on. IEEE. 2011, pp. 1879–
1886.
Bibliography 133
[101] Benjamin Sapp, Alexander Toshev, and Ben Taskar. “Cascaded models for ar-
ticulated pose estimation.” In: European conference on computer vision. Springer.
2010, pp. 406–420.
[102] Yaar Schnitman, Yaron Caspi, Daniel Cohen-Or, and Dani Lischinski. “Inducing
semantic segmentation from an example.” In: Asian conference on computer vision.
Springer. 2006, pp. 373–384.
[103] Libin Shen, Anoop Sarkar, and Franz Josef Och. “Discriminative reranking for
machine translation.” In: HLT-NAACL. 2004, pp. 177–184.
[104] Jianbo Shi and Jitendra Malik. “Normalized cuts and image segmentation.” In:
IEEE Transactions on pattern analysis and machine intelligence 22.8 (2000), pp. 888–
905.
[105] Naum Zuselevich Shor. Minimization methods for non-differentiable functions. Vol. 3.
Springer Science & Business Media, 2012.
[106] Jamie Shotton, John Winn, Carsten Rother, and Antonio Criminisi. “Texton-
boost: Joint appearance, shape and context modeling for multi-class object
recognition and segmentation.” In: European conference on computer vision. Springer.
2006, pp. 1–15.
[107] Jamie Shotton, John Winn, Carsten Rother, and Antonio Criminisi. “Texton-
boost for image understanding: Multi-class object recognition and segmenta-
tion by jointly modeling texture, layout, and context.” In: International Journal of
Computer Vision 81.1 (2009), pp. 2–23.
[108] Karen Simonyan and Andrew Zisserman. “Very deep convolutional networks
for large-scale image recognition.” In: arXiv preprint arXiv:1409.1556 (2014).
[109] Vikas Sindhwani and S Sathiya Keerthi. “Large scale semi-supervised linear
SVMs.” In: Proceedings of the 29th annual international ACM SIGIR conference on
Research and development in information retrieval. ACM. 2006, pp. 477–484.
[110] Paul Sturgess, Karteek Alahari, Lubor Ladicky, and Philip HS Torr. “Combin-
ing appearance and structure from motion features for road scene understand-
ing.” In: BMVC 2012-23rd British Machine Vision Conference. BMVA. 2009.
[111] Daniel Tarlow, Inmar E Givoni, and Richard S Zemel. “HOP-MAP: Efficient
Message Passing with High Order Potentials.” In: AISTATS. Vol. 5. 2010, p. 6.
[112] Daniel Tarlow and Richard S Zemel. “Structured Output Learning with High
Order Loss Functions.” In: AISTATS. 2012, pp. 1212–1220.
[113] Daniel Tarlow, Kevin Swersky, Richard S Zemel, Ryan Prescott Adams, and
Brendan J Frey. “Fast exact inference for recursive cardinality models.” In: arXiv
preprint arXiv:1210.4899 (2012).
[114] Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Al-
tun. “Large margin methods for structured and interdependent output vari-
ables.” In: Journal of machine learning research 6.Sep (2005), pp. 1453–1484.
134 Bibliography
[115] Jasper Uijlings, Koen van de Sande, Arnold Smeulders, Theo Gevers, Nicu Sebe,
and Cees Snoek. “The Most Telling Window for Image Classification.” In: ICCV
Pascal VOC Workshop. 2011.
[116] Andrea Vedaldi and Stefano Soatto. “Quick shift and kernel methods for mode
seeking.” In: European Conference on Computer Vision. Springer. 2008, pp. 705–
718.
[117] Paul Viola and Michael J Jones. “Robust real-time face detection.” In: Interna-
tional journal of computer vision 57.2 (2004), pp. 137–154.
[118] Martin J Wainwright, Tommi S Jaakkola, and Alan S Willsky. “MAP estimation
via agreement on trees: message-passing and linear programming.” In: IEEE
transactions on information theory 51.11 (2005), pp. 3697–3717.
[119] Martin J Wainwright, Michael I Jordan, et al. “Graphical models, exponen-
tial families, and variational inference.” In: Foundations and Trends® in Machine
Learning 1.1–2 (2008), pp. 1–305.
[120] Song Wang and Jeffrey Mark Siskind. “Image segmentation with ratio cut.” In:
IEEE Transactions on Pattern Analysis and Machine Intelligence 25.6 (2003), pp. 675–
690.
[121] David J Weiss and Benjamin Taskar. “Structured Prediction Cascades.” In: AIS-
TATS. 2010, pp. 916–923.
[122] David Weiss, Benjamin Sapp, and Ben Taskar. “Sidestepping intractable infer-
ence with structured ensemble cascades.” In: Advances in Neural Information
Processing Systems. 2010, pp. 2415–2423.
[123] Tomas Werner. “A linear programming approach to max-sum problem: A re-
view.” In: IEEE transactions on pattern analysis and machine intelligence 29.7 (2007),
pp. 1165–1179.
[124] Zifeng Wu, Chunhua Shen, and Anton van den Hengel. “Wider or Deeper:
Revisiting the ResNet Model for Visual Recognition.” In: CoRR abs/1611.10080
(2016). url: http://arxiv.org/abs/1611.10080.
[125] Payman Yadollahpour, Dhruv Batra, and Gregory Shakhnarovich. “Discrimina-
tive re-ranking of diverse segmentations.” In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. 2013, pp. 1923–1930.
[126] Payman Yadollahpour and Gregory Shakhnarovich. “Region ranking for figure-
ground segmentation.” 2014.
[127] Chen Yanover and Yair Weiss. “Finding the M most probable configurations
using loopy belief propagation.” In: Advances in neural information processing
systems 16 (2004), p. 289.
[128] Jian Yao, Sanja Fidler, and Raquel Urtasun. “Describing the scene as a whole:
Joint object detection, scene classification and semantic segmentation.” In: Com-
puter Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE. 2012,
pp. 702–709.
Bibliography 135
[129] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.
“Pyramid Scene Parsing Network.” In: arXiv preprint arXiv:1612.01105 (2016).

Part I
A P P E N D I X

A
A P P E N D I X A
a.1 sample of divmbest solutions from figure-ground model
For overall document size considerations the results in this appendix section have been
moved to http://ttic.uchicago.edu/~pyadolla/papers/thesis.pdf.
a.2 sample of results when divmbest is applied to muli-category seg-
mentation
For overall document size considerations the results in this appendix section have been
moved to http://ttic.uchicago.edu/~pyadolla/papers/thesis.pdf.
139

B
A P P E N D I X B
b.1 example re-ranking results
For overall document size considerations the results in this appendix section have been
moved to http://ttic.uchicago.edu/~pyadolla/papers/thesis.pdf.
141
142 appendix b
b.2 highest ranked vs . map
For overall document size considerations the results in this appendix section have been
moved to http://ttic.uchicago.edu/~pyadolla/papers/thesis.pdf.
