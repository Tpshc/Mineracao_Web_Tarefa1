REASONING WITH SHAPES 1
Reasoning with shapes: profiting cognitive
susceptibilities to infer linear mapping
transformations between shapes
Vahid Jalili
Abstract—Visual information plays an indispensable role in our daily interactions with environment. Such information is manipulated
for a wide range of purposes spanning from basic object and material perception to complex gesture interpretations. There have been
novel studies in cognitive science for in-depth understanding of visual information manipulation, which lead to answer questions such
as: how we infer 2D/3D motion from a sequence of 2D images? how we understand a motion from a single image frame? how we see
forest avoiding trees?
Leveraging on congruence, linear mapping transformation determination between a set of shapes facilitate motion perception. Present
study methodizes recent discoveries of human cognitive ability for scene understanding. The proposed method processes images
hierarchically, that is an iterative analysis of scene abstractions using a rapidly converging heuristic iterative method. The method
hierarchically abstracts images; the abstractions are represented in polar coordinate system, and any two consecutive abstractions
have incremental level of details. The method then creates a graph of approximated linear mapping transformations based on circular
shift permutations of hierarchical abstractions. The graph is then traversed in best-first fashion to find best linear mapping
transformation. The accuracy of the proposed method is assessed using normal, noisy, and deformed images. Additionally, the present
study deduces (i) the possibility of determining optimal mapping linear transformations in logarithmic iterations with respect to the
precision of results, and (ii) computational cost is independent from the resolution of input shapes.
Index Terms—Reasoning with shapes linear transformation determination cognitive simulation hierarchical abstractions
F
1 INTRODUCTION
V ISION system is studied in orthogonal disciplines span-ning from neurophysiology and psychophysics to com-
puter science all with uniform objective: understand the
vision system and develop it into an integrated theory
of vision. In general, vision or visual perception is the
ability of information acquisition from environment, and
it’s interpretation. According to Gestalt theory, visual ele-
ments are perceived as patterns of wholes rather than the
sum of constituent parts [1]. The Gestalt theory through
emergence, invariance, multistability, and reification properties
(aka Gestalt principles), describes how vision recognizes
an object as a whole from constituent parts. There is an
increasing interested to model the cognitive aptitude of
visual perception; however, the process is challenging. In
the following, a challenge (as an example) per object and
motion perception is discussed.
1.1 Why do things look as they do?
In addition to Gestalt principles, an object is characterized
with its spatial parameters and material properties. Despite
of the novel approaches proposed for material recognition
(e.g., [2]), objects tend to get the attention. Leveraging on
an object’s spatial properties, material, illumination, and
background; the mapping from real world 3D patterns
(distal stimulus) to 2D patterns onto retina (proximal stim-
ulus) is many-to-one non-uniquely-invertible mapping [3],
• Oregon Health & Science University, Portland, OR, USA
[4]. There have been novel biology-driven studies for con-
structing computational models to emulate anatomy and
physiology of the brain for real world object recognition
(e.g., [5], [6], [7]), and some studies lead to impressive ac-
curacy. For instance, testing such computational models on
gold standard controlled shape sets such as Caltech101 and
Caltech256, some methods resulted <60% true-positives [7],
[8], [9], [10]. However, Pinto et al. [11] raised a caution
against the pervasiveness of such shape sets by highlight-
ing the unsystematic variations in objects features such as
spatial aspects, both between and within object categories.
For instance, using a V1-like model (a neuroscientist’s null
model) with two categories of systematically variant objects,
a rapid derogate of performance to 50% (chance level) is
observed [7]. This observation accentuates the challenges
that the infinite number of 2D shapes casted on retina from
3D objects introduces to object recognition.
Material recognition of an object requires in-depth fea-
tures to be determined. A mineralogist may describe the
luster (i.e., optical quality of the surface) with a vocabulary
like greasy, pearly, vitreous, resinous or submetallic; he may
describe rocks and minerals with their typical forms such as
acicular, dendritic, porous, nodular, or oolitic. We perceive
materials from early age even though many of us lack such
a rich visual vocabulary as formalized as the mineralo-
gists [12]. However, methodizing material perception can be
far from trivial. For instance, consider a chrome sphere with
every pixel having a correspondence in the environment;
hence, the material of the sphere is hidden and shall be
inferred implicitly [12], [13]. Therefore, considering object
ar
X
iv
:1
70
9.
00
15
8v
1 
 [
cs
.C
V
] 
 1
 S
ep
 2
01
7
REASONING WITH SHAPES 2
material, object recognition requires surface reflectance, var-
ious light sources, and observer’s point-of-view to be taken
into consideration.
1.2 What went where?
Motion is an important aspect in interpreting the interaction
with subjects, making the visual perception of movement
a critical cognitive ability that helps us with complex tasks
such as discriminating moving objects from background, or
depth perception by motion parallax. Cognitive susceptibil-
ity enables the inference of 2D/3D motion from a sequence
of 2D shapes (e.g., movies [14], [15], [16]), or from a single
image frame (e.g., the pose of an athlete runner [17], [18]).
However, its challenging to model the susceptibility because
of many-to-one relation between distal and proximal stimu-
lus, which makes the local measurements of proximal stim-
ulus inadequate to reason the proper global interpretation.
One of the various challenges is called motion correspondence
problem [19], [20], [21], [22], which refers to recognition of
any individual component of proximal stimulus in frame-1
and another component in frame-2 as constituting different
glimpses of the same moving component. If one-to-one
mapping is intended, n! correspondence matches between n
components of two frames exist, which is increased to 2n for
one-to-any mappings. To address the challenge, Ullman [20]
proposed a method based on nearest neighbor principle,
and Dawson [22] introduced an auto associative network
model. Dawson’s network model [22] iteratively modifies
the activation pattern of local measurements to achieve a
stable global interpretation. In general, his model applies
three constraints as it follows: (i) nearest neighbor principle
(shorter motion correspondence matches are assigned lower
costs), (ii) relative velocity principle (differences between two
motion correspondence matches), and (iii) element integrity
principle (physical coherence of surfaces). According to ex-
perimental evaluations (e.g., [20], [21], [23]), these three
constraints are the aspects of how human visual system
solves the motion correspondence problem. Eom et al. [24]
tackled the motion correspondence problem by considering
the relative velocity and the element integrity principles.
They studied one-to-any mapping between elements of cor-
responding fuzzy clusters of two consecutive frames. They
have obtained a ranked list of all possible mappings by
performing a state-space search.
1.3 How a stimuli is recognized in the environment?
Human subjects are often able to recognize a 3D object
from its 2D projections in different orientations [25]. A
common hypothesis for this spatial ability is that, an object
is represented in memory in its canonical orientation, and a
mental rotation transformation is applied on the input image,
and the transformed image is compared with the object in
its canonical orientation [25]. The time to determine whether
two projections portray the same 3D object: (i) increase
linearly with respect to the angular disparity [25], [26],
[27], and (ii) is independent from the complexity of the 3D
object [28]. Shepard and Metzler [29] interpreted this finding
as it follows: human subjects mentally rotate one portray at a
constant speed until it is aligned with the other portray.
1.4 State of the Art
The linear mapping transformation determination between
two objects is generalized as determining optimal linear
transformation matrix for a set of observed vectors, which
is first proposed by Grace Wahba in 1965 [30] as it follows.
Given two sets of n points {v1, v2, . . . vn}, and {v?1 , v?2 . . . v?n},
where n ? 2, find the rotation matrix M (i.e., the orthogonal
matrix with determinant +1) which brings the first set into the
best least squares coincidence with the second. That is, find M
matrix which minimizes
n?
j=1
|v?j ?Mvj |2 (1)
Multiple solutions for the Wahba’s problem have been
published, such as Paul Davenport’s q-method. Some no-
table algorithms after Davenport’s q-method were pub-
lished; of that QUaternion ESTimator (QUEST) [31], Fast
Optimal Attitude Matrix (FOAM) [32] and Slower Optimal
Matrix Algorithm (SOMA) [32], and singular value decom-
position (SVD) based algorithms, such as Markleys SVD-
based method [33].
In statistical shape analysis, the linear mapping trans-
formation determination challenge is studied as Procrustes
problem. Procrustes analysis finds a transformation ma-
trix that maps two input shapes closest possible on each
other. Solutions for Procrustes problem are reviewed in [34],
[35]. For orthogonal Procrustes problem, Wolfgang Kabsch
proposed a SVD-based method [36] by minimizing the
root mean squared deviation of two input sets when the
determinant of rotation matrix is 1. In addition to Kab-
schs partial Procrustes superimposition (covers translation
and rotation), other full Procrustes superimpositions (covers
translation, uniform scaling, rotation/reflection) have been
proposed [34], [35]. The determination of optimal linear
mapping transformation matrix using different approaches
of Procrustes analysis has wide range of applications, span-
ning from forging human hand mimics in anthropomorphic
robotic hand [37], to the assessment of two-dimensional
perimeter spread models such as fire [38], and the analysis
of MRI scans in brain morphology studies [39].
1.5 Our Contribution
The present study methodizes the aforementioned men-
tioned cognitive susceptibilities into a cognitive-driven lin-
ear mapping transformation determination algorithm. The
method leverages on mental rotation cognitive stages [40]
which are defined as it follows: (i) a mental image of
the object is created, (ii) object is mentally rotated until a
comparison is made, (iii) objects are assessed whether they
are the same, and (iv) the decision is reported. Accord-
ingly, the proposed method creates hierarchical abstractions
of shapes [41] with increasing level of details [42]. The
abstractions are presented in a vector space. A graph of
linear transformations is created by circular-shift permuta-
tions (i.e., rotation superimposition) of vectors. The graph
is then hierarchically traversed for closest mapping linear
transformation determination.
Despite of numerous novel algorithms to calculate lin-
ear mapping transformation, such as those proposed for
Procrustes analysis, the novelty of the presented method is
REASONING WITH SHAPES 3
being a cognitive-driven approach. This method augments
promising discoveries on motion/object perception into a
linear mapping transformation determination algorithm.
2 METHOD
Basic manipulations vs. complex calculations: An
infant has intuitive understanding of numbers and shapes,
and can distinguish numerical and identity invariance of
objects [43] regardless of object domain [44]; an ability that
surprisingly extends to non-object entities (e.g., action [45]).
It lets us argue that an infant has basic understanding
of transformations by primary perception of numbers and
shapes. This intuitive ability is based on an early develop-
ment of approximate number system. This ability encourages
present study to concentrate on basic operations and visual
properties of shapes for the linear mapping transformation
determination task.
Abstract vs. detailed representations: Of the entire
environment within our visual range, only the essential
information for the action in progress is prominent and
the rest of the details are ignored [46] (aka cognitive inhi-
bition [47]). For instance, while crossing a street only the
information about the direction and speed of cars on the
street are required; details such as plate number of the cars
or clothes drivers wore, generally not consciously registered
in the visual perception. This highlights the significant role
that abstractions play in reducing the amount of information
to be considered. Additionally, Ballard [48] and Agre [49]
further explained this ability as deictic strategies where eye
fixation point is used to guide body movement (modeling
the behavior at the embodiment level) while fixation point
can rapidly change to different location [50].
In the following, we discuss how the proposed method
abstracts images and determines linear mapping transfor-
mations between the images.
2.1 Shape representation
The overall procedure of the presented method is indepen-
dent from the color model of input shape (i.e., RGB, Cyan
Magenta Yellow Key (CMYK), Hue Saturation Value (HSV),
B&W, binary, and etc.).Present study manipulates binary
representation of shapes; while extension to other color
models is straightforward and requires the modification of
segment aggregation function (discussed in Section 2.2). The
motivations of binarizing shapes are threefold. First, simple
aggregation functions such as count can be applied on bi-
nary shapes. This improves the readability of the presented
method, and avoids various color-model-based aggregation
functions, which are beyond the scope of this manuscript.
Second, real world objects incorporate spatial parame-
ters and materials, introducing distal-to-proximal stimulus
mapping challenges, and motion correspondence problem.
The objectives of present study are to methodize principle
cognitive susceptibilities for transformation determination,
and the fact that binary shapes are not as sensible to
aforementioned challenges as colorful shapes are, makes the
binary model suitable for present study. The binary color
model, is a common model among the motion correspon-
dence problem studies (e.g., [51], [52].
   
A B C 
   
   
 
Fig. 1. A: input shape in RGB color model; B: intermediate gray-scale
representation of the input; C: binary representation of the input. The
proposed method manipulates the binary representation.
Third, a distance transform and topological skeleton ex-
traction from binary shapes is straightforward as opposed
to colorful shapes. These transformations are applicable
alternatives for the segment aggregation functions discussed
in Section 2.2. Despite of the promising methods that exist
in literature (e.g., topological volume skeletonization [53],
or various methods on distance transform algorithms [54]),
to best of our knowledge, none of the proposed methods
are comprehensive in the consideration of full explanatory
real world object characteristics such as material, illumina-
tion, and surface reflectance. For instance, the pattern in
a chromium plated sphere in an image frame is indeed
reflecting the surrounding environment and the sphere itself
is determined implicitly [12]. However, binary shapes mask
similar properties encouraging least ambiguity.
Present study manipulates binary shapes. In this regard,
first a colored shape is converted to its corresponding gray-
scale B&W frame. The procedure is by estimating the lu-
minance for every pixel x, y of an image frame in RGB
color model (e.g., panel A on Fig. 1) as Lxy = 0.2126R +
0.7152G + 0.0722B (see panel B on Fig. 1). The resulted
B&W image frame is then binarized by normalizing Lxy
as Bxy = bLxy/128c for the binary pixel Bxy (see panel
C on Fig. 1). Note that, Lxy ? {0, 1, . . . 255}, therefore,
Bxy ? {0, 1}.
2.2 Shape segmentation
Shape segmentation is a well-studied subject in the field
of image processing (e.g., [55]). Segmentation commonly
proceeds shape semantic analysis, thus highlighting the
necessity of adapting segmentation method to the objectives
of the study. Accordingly, the segmentation procedure is
defined as it follows, which emphasizes the relative location
of the pixels to facilitate liner mapping transformation.
In present study, shapes are considered in two Dimen-
sional (2D) Euclidean space. An image frame is segmented
inN sectors andM segments (see Fig. 2). Sectors are divisions
of the image frame in angle (?) direction of polar coordinate
system and are denoted by Euclidean unit vector ~Vn for
n ? {1, 2, . . . N}. Segments are isometric divisions of sectors
in radius (r) direction of polar coordinate system, denoted
by the Euclidean unit vector ~Vnm for m ? {1, 2, . . .M}.
Accordingly, all sectors have equal number of segments, and
segments are the smallest segmentation units. Let I denote
REASONING WITH SHAPES 4
??,?
?1,2
?1,3
?2,1?2,2?2,3
?3,1
?3,2
?3,3
?4,1 ?4,2 ?4,3
?1,1
??,?
?1,3
Fig. 2. This shape illustrates segmentation withN = 4 sectors andM =
3 segments. A segment represents an area of the frame. For instance,
~V1,1, ~V1,2, and ~V1,3 respectively represent the areas shaded in yellow,
blue, and cyan. Each area is composed of a set of pixels aggregated in
?nm. For instance, ?1,1, ?1,2, and ?1,3 denote the aggregation of pixels
located at yellow, green, and blue shaded areas respectively.
segmentation matrix defined as it follows.
I =
???
Segment 1 . . . Segment M
Sector 1 ~V11 . . . ~V1M
. . .
...
. . .
...
Sector N ~VN1 . . . ~VNM
???
Each element Vnm is a tuple of ?x, y, ??, where ? is an
aggregated value of a portion of the image frame which
is represented by Vnm. Note that, the dimension of segmen-
tation matrix is independent from the resolution of input
image frame.
The area represented by a segment ~Vnm is characterized
by two boundaries environing it, and it is defined in polar
coordinate system as it follows.
r ?
]
m? 1
M
,
m
M
]
(2)
? ?
]
360(n? 1)
N
,
360n
N
]
(3)
A pixel at r?, ?? Polar coordinate is a member of ~Vnm
segment if and only if the coordinates of the pixel fall in the
boundaries of the segment. Accordingly, the membership of
a pixel at x, y Cartesian coordinate to ~Vnm segment depends
on the following condition.
tan?1(
x
y
) ?
]
360(n? 1)
N
,
360n
N
]
(4)
|
?
x2 + y2| ?
]
m? 1
M
,
m
M
]
(5)
2.3 Shape abstraction
A shape is abstracted by aggregating pixels in the area of
each segment. The binary representation of a shape enables
the use of simple count aggregate function that is the number
of pixels represented by the segment with the value of 1. Let
?nm denote the aggregated value of segment ~Vnm which is
defined as Snm = |{Bxy|Bxy = 1}| for all x and y of pixels
belonging to ~Vnm segment.
The proposed method operates upon ?nm only, and
is independent from x and y components. Therefore, the
segmentation matrix, I , is modified as it follows; and is
called abstraction matrix (?).
? =
???
Segment 1 . . . Segment M
Sector 1 ?11 . . . ?1M
. . .
...
. . .
...
Sector N ?N1 . . . ?NM
???
The matrix is independent from the coordinates of each
vector in 2D Euclidean space. However, the coordinates are
implicitly approximated in the order of each of the vectors.
For instance, ?2m refers to m-th segment on 2× (360/N)-th
sector. Finally the ? matrix is normalized using coefficient of
variation method.
2.4 Translation
In most previous works such as Kabsch algorithm [36],
translating input shapes to a position such that their cen-
troid coincide with the center of coordinate system, or
any specific coordinate, is a mandatory preprocessing. In
general, translation superimposition is inevitable for both
partial and full Procrustes superimpositions.
The abstraction vectors of ? matrix are independent from
x and y parameters, hence translation is not essential for the
proposed method. However, an alternative application for
translation is defined, which enables partial match determi-
nation between shapes. For this application, the translation
process between the two input shapes could be interpreted
as moving the segmentation center of one shape to coordi-
nates pointed out by the segmentation vectors of the other
shape (a process similar to translation superimposition in
Procrustes analysis). Let Tx and Ty denote translation on x
and y direction respectively; and T = Tx × Ty (Cartesian
product of translations on x and y coordinates) be the set
of all possible translations. The partial match between two
shapes is determined by state-space search performed on
the T set, which is by applying all the transformations of T
on the second shape, and assessing the similarity between
the first and second shape (see section 2.6).
2.5 Rotation
Rotation is a rigid body motion of a space that maintains
at least one point at its original location; here we fix the
segmentation center and move the segmentation vectors.
In other words, given that a frame is partitioned into
360/N equal sectors, any (360/N)j degrees of rotation for
j ? {0, 1, . . . N ? 1} is implemented as j units of circular
shift on ? (see Fig.3).
Following the aforementioned objective of using basic
operations, rotation is implemented using circular shift
operation on ?. Accordingly, given N sectors (given that
rotation is a rigid body transformation, this operation is
independent from M ), a set of rotation angles that are im-
plemented using circular shift on ?, is defined as it follows.
R =
{
360
N
i
???? i = 0, 1, . . . N ? 1} (6)
REASONING WITH SHAPES 5
??,?
??,?
1 ?1,1 ?1,2
2 ?2,1 ?2,2
3 ?3,1 ?3,2
4 ?4,1 ?4,2
5 ?5,1 ?5,2
6 ?6,1 ?6,2
Shape A
1
2
3
4
5
6
1 ?3,1 ?3,2
2 ?4,1 ?4,2
3 ?5,1 ?5,2
4 ?6,1 ?6,2
5 ?1,1 ?1,2
6 ?2,1 ?2,2
??,?
??,?
Shape B
1
2
3
4
5
6
Fig. 3. Abstractions of two inputs, Shape A and Shape B, are
given with partitioning parameters N = 6 and M = 2. There
is a 120? rotation difference between the two shapes. Given N
and M , the set of rotation angles based on (360/60)j is R =
{0?, 60?, 120?, 180?, 240?, 300?, 360?}. These rotations are imple-
mented using circular shift on ?. Accordingly, if the transformation
between inputs is (360/60)j degrees of rotation, it is superimposable
by j circular shifts of ?. Therefore, four times circular shift on ?B results
highest similarity value (e.g., J = 1), which yields 120? rotation as the
best linear mapping transformation between Shape A and Shape B.
The set R defines a discreet set of rotation angles which
are hierarchically extended to a continuous domain using
an iterative procedure discussed in Section 2.7.
2.6 Similarity measurement
To determine the best linear mapping transformation, the
presented method runs a state-space search on transforma-
tions, by transforming the second shape, and assessing it’s
similarity with the first shape. In general, let ? = T ×R be
the Cartesian product of T translation and R rotation. Note
that, if the alternative application for translation introduced
in Section 2.4 is of no interest, thence ? = R. The linear
mapping transformations between Shape A and Shape B, are
ranked based on the similarity coefficients J(?A, ??B) for
? ? ?.
The similarity between any two elements ?Anm ? ?A and
?Bnm ? ?B is measured using Jaccard similarity coefficient,
denoted j(?Anm, ?
B
nm), and is calculated as it follows.
j(?Anm, ?
B
nm) =
|?Anm ? ?Bnm|
?Anm + ?
B
nm
(7)
The similarity between two abstracted shapes, denoted
J(?A,?B), is calculated as the sum of all pairwise Jaccard
similarity indexes as it follows.
J(?A,?B) =
?
nm
j(?Anm, ?
B
nm) (8)
Tanimoto similarity coefficient [56] is an alternative to
Jaccard index; however since both methods yield similar
results, Jaccard index is chosen. Additionally, other alter-
natives to Jaccard index are: Sørensen similarity index [57],
BrayCurtis dissimilarity [58] (also known as Czekanowski
similarity index), Pearson product moment correlation, and
earth mover’s distance [59].
The similarity assessment between any two ?Anm and
?Bnm is also optionally extended by a neighborhood opera-
tion. Let j(Nk(nm, i)) denote the Jaccard index of neighbor
Nk of element n, m at i-th distance. The extended similarity
coefficient j? is calculated as it follows for d neighbors:
j?(?Anm, ?
B
nm) =J(?
A
nm, ?
B
nm)
+
d?
i=1
??logd+2(d+ 2? i)?
Ki
j(Nk(nm, i))
??
(9)
This is an adaptive logarithmic neighborhood operation
which assigns heavier weight to closer elements than re-
motes.
2.7 Iteration
How long does it take us to understand the gist of a
shape? Henderson et al. [60] obtained a typical scene fix-
ation of 304ms with 100% luminance, and Rayner [61]
estimated 233ms fixation time for an adult reading nor-
mal text (Kowler et al. [62] measured fixation patterns for
reading reversed letters). The conceptual and perceptual
information understood from a glance at an image frame is
a function of the glance duration. Fei-Fei et al. [63] studied
the perception depth over time. He resulted that we perceive
sensory information (e.g., dark and light) in roughly 50ms,
at 107ms we determine more semantic aspects (e.g., people,
room, urban, and water) with considerable accuracy, it takes
150ms to determine an object (e.g., dog), and at 500ms we
achieve maximum perception (e.g., identify dog as German
shepherd). Greene et al. [41] conducted similar study and
established a perceptual benchmark to types of information
we perceive during early perceptual processing; and they
inferred that it takes 63ms to determine naturalness of an
image frame and 78ms to understand whether its forest or
not. Such global-to-local view cognitive abilities inspired
the present study to see transformation determination as
a multi-step procedure an opposed to the some single-step
methods.
The longer we are exposed to a shape, the more we un-
derstand from it; in other words, the amount of information
we perceive from a shape is the function of the number
of image processing iterations performed on the shape.
Accordingly, present study defines a converging heuristic
iterative method that determines the gist of shapes at ini-
tial approximation (highest abstraction that corresponds to
sensory information such as light/dark classification [63]),
and by translation superimposition followed by similarity
assessment procedure, the most abstracted transformation is
determined. Then, segmentation parameters are iteratively
incremented. At each iteration, a new abstraction with more
details than its preceding abstraction is made. Also, at each
iteration, approximated transformations of the preceding
iteration are tuned using more detailed abstraction. This
process is analogous to: from light, through animal and dog,
to German shepherd [41], [63].
In general, the proposed method determines an initial
approximation of best mapping transformations, and tunes
those through successive iterations. The permutations of
transformations at each iteration form a state-space that is
traversed in best-first search fashion. This approach follows
the traits of Greedy algorithm [64] that determines local
REASONING WITH SHAPES 6
optimal choice. To best of our knowledge, cognitive commu-
nity descriptions of processing segmentations, well overlaps
local optimal search method. However, one may consider
updating the procedure to follow traits of global optimal
search methods to best adapt the application requirements.
Let l ? N denote an iteration coefficient which is initial-
ized with a user-defined parameter ?. Let ?lA be abstrac-
tion of shape A at iteration l with Nl = 2l sectors and
Ml = 2
l segments. For the purpose of readability of the
method, the number of sectors and segments are chosen
to be identical; however, the extension of the method to
support divers parameters is straight-forward. According
to this generalization, the amount of details represented by
each abstraction grows exponentially through the iterations.
Also, the growth rate can be update to best adapt the
application requirements by changing either the growth
function or considering l ? R.
Let ?l be the set of all transformations to be applied on
?lA at iteration l for ?? = T × R. Let ?l = {?1 . . . ?i . . . ?}
be the set of top- transformations (i.e., highest similarity) at
iteration l for  being a user-defined parameter. The iteration
l tunes best transformations of iteration l ? 1. Accordingly,
?l consists of all ?l tunes which is formally defined as it
follows for the user-defined parameter ? that specifies the
tuning range.
?j ? {0, 1, . . . ?} : ?l = {(2l???(l?1)i)± j} (10)
For instance, suppose ? = 3 then N = 8 and M =
8 and assuming only rotation superimposition, we obtain
?3 = {0, 1, 2, . . . 7} which are the number of circular shifts
on ?A that corresponds to {0?, 45?, 90?, . . . 315?}. Suppose
 = 1, ?3 = 2, and ? = 2, accordingly ?4 is calculated as it
follows. ?4 = {(24?3 × 2)± {0, 1}}
which corresponds to {67.5?, 90?, 112.5?}. The pseudo code
of the iteration procedure is given in Algorithm 1.
Algorithm 1 Iteration Algorithm
1: procedure ITERATE
2: l? ?
3: ?l ? T ×R
4: Nl ? 2l
5: Ml ? 2l
6: Build ?lA and ?
l
B
7: ?? apply ?l on ?lA and get top- transformations
8: if l < max l then
9: l? l + 1
10: ?l ? all tunes of ?l?1
11: Goto 03
12: else
13: report ?l as best mapping transformations
2.8 Validation and verification
If the difference between two input image frames is ?? and
? ? ?? , according to Section 2.5, then ? is determined
using circular shifts on ?. However, if ? /? ?? , then ?
is determined using the iterative process. To prove that,
consider the following hypothesis:
?n ? {1, 2, . . . N} : 360
N
(n? 1) < ? ? 360
N
n (11)
By definition of segmentation, these are the boundaries
of n-th region which is divided into M equal segments.
According to the hypothesis, ? belongs to one and only one
region, therefore as much as the range is narrowed-down,
we get closer and closer to ? (the motivation of iteration
procedure). At each iteration, the results of former iteration
are tuned until a result with a user-defined accuracy (?) is
determined. The algorithm performs maximum c iterations
which is calculated as it follows.
• The segmentation area should be as narrow as ??,
therefore:
? =
360
N
n? 360
N
(n? 1)? ? = 360
N
(12)
• Segmentation grows exponentially through iterations,
hence:
Nl = 2
l ? 360
?
= 2l ? l = log2
360
?
? l = dlog2
360
?
e
(13)
3 RESULTS
The accuracy of proposed method is assessed using 100+
pairs of image frames with diverse resolutions spanning
from 50× 50 to 1000× 1000 pixels, and including different
categories (e.g., animals, cars, airplane, people, and abstract
images). Additionally, the impact of noisy image frames
to the accuracy of the proposed method, is assessed using
image frames with up to 70% of random noise
The evaluations are designed as it follows: (i) Shape A
is a BMP image, or an abstract shape composed of lines,
circles, and random noise drawn using features integrated
in the implemented tool, (ii) Shape B is obtained by ? degrees
rotation of Shape A plus a random percentage of noise,
(iii) Shape A and Shape B are used as inputs for the proposed
method, and (iv) the central tendency of determined rota-
tions is measured as weighted arithmetic mean among top-3
(WM3) rotations, and it is compared with ?. The assessments
are performed with default parameters which are: ? = 3,
? = 10,  = 10, and the similarity between two segments is
calculated excluding neighbor segments. The results of the
experiments are discussed as it follows.
3.1 Top transformations converge rapidly
The fundamental argument of iterations is to progressively
increase the level of details on the image frame abstraction,
and accordingly, iteratively improve the accuracy of the cal-
culated approximated transformations, until a user-defined
precision criterion is met. Weighted sample variance among
top-3 (WV3) approximated transformations provides a mea-
sure of dispersion on top approximations. The WV3 reflects
the variability in the top-3 approximated transformations,
such that: a small WV3 suggests a very reliable WM3, while
a large WV3 reflects an uncertainty about the “best” lin-
ear mapping transformation. According to the experiments,
WV3 gets closer to 1 in a few iterations which yields (a)
rapid convergence among top approximated transforma-
tions (this confirms the validation of iteration procedure
discussed in Section 2.8), (b) WV3 ? 1 in few iterations (> 6)
confirms the accuracy of rapidly converged approximated
transformations.
REASONING WITH SHAPES 7
T S D T S
l = 3 270 37 0 l = 3 193 97.2
l = 4 270 18 0 l = 4 269 18.4
l = 5 270 9.2 0 l = 5 259 9.2
l = 6 270 4.6 0 l = 6 261 7
l = 7 270 2.3 0 l = 7 263 8.7
1
0
Min  Max MaxMin 
Shape A Shape B
2.6
Highest 
Score
Lowest 
Score
D
66.9
9
1
0.7
Shape A Shape B
??? ???
l = 3
l = 4
l = 5
l = 6
l = 7
0°
315°
270°
225°
180°
135°
90°
45°
??? ???
0°
315°
270°
225°
180°
135°
90°
45°
A1 B1
A2 B2
Fig. 4. Shape A is loaded from a BMP image, and Shape B is obtained by 270? rotation of Shape A. The ? matrices of both shapes at different
iterations are presented by circular heatmaps. T: determined transformation, S: standard deviation among top-3 determined transformations, D:
difference between actual and determined transformations. The normalized similarity index J(?A, ??B), ?? ? ? is plotted using a circular heapmap
for all the iterations, see panels A2 and B2.
3.2 Tuning out the cognitive noise
Selective and visual attention filter irrelevant stimuli to
the subject’s task by mechanisms such as habituation and
cognitive inhibition. There have been promising efforts to
model the ability (e.g., [65]) since the spotlight [66] and zoom
lens [67] models. Additionally, perceived visual information
are function of an observer’s distance to an object. This
aspect has variety of applications namely is Olivia et al. [68]
that incorporates this aspect with hybrid images. A hybrid
image is composed of two image frames with low and high
spatial frequencies, such that either is perceived as noise
as a function of observer’s distance to the hybrid image
frame. In other words, the image of high spatial frequency
is dominant at closer distance, while the image with low
frequency is perceived at far distance. Whether the noise
is a masked image or it is an irrelevant stimuli, it does
REASONING WITH SHAPES 8
not impact the perceived information from an image frame.
Therefore, the performance of proposed method in approx-
imating linear mapping transformation using noisy image
frames, is assessed by experiments where a percentage of
Shape B is covered with random noise.
To this extend, an experiment of four tests, T1, T2, T3,
and T4 is conducted (see Fig. 5). The tests have Shape A
in common which is a BMP image of a bee. The Shape B is
created by 234? rotation of Shape A, and differs among test
in the amount of incorporated random noise. The subject in
the Shape A (i.e., the bee) is represented by? 230K pixels (of
584K pixels of the image frame). A portion of 120K pixels
(out of the ? 230K pixels) is subject to random noise. This
portion is intentionally chosen to cover the body of the bee
which presents the majority of perceptible features of the
subject. Given that the pixels are binary and the figure is
represented by pixels of value 1 (see Section 2.1), the random
noise is created by setting the value of a random pixel to
1 in the subject-to-noise portion of Shape B. The random
noise is added through an iteration of 0, 5K, 50K, and 500K
random pixel selections (a pixel can be selected multiple
times) respectively for T1, T2, T3, and T4 (see Fig. 5);
such that, the majority of perceptible features on Shape B
are covered with random noise at T4.
The initial segmentation parameter (? = 3) provides
a limited number of variant initial approximations (see
Section 2.7). Therefore, the WV3 at first iteration (i.e., l = 3)
of the T1, T2, T3, and T4 show relatively high dispersion,
which indicate the inconsistency of WM3 (see Fig. 5). The
initial approximations are tuned at second iteration (i.e.,
l = 4) which improve WV3 tenfold (from 118 to 18) for
the T1, T2, and T3. Despite of a minor discrepancy, WM3
of the tests T1, T2, and T3 are relatively close to actual
transformation (i.e., 234?). However, the considerable noise
of T4 prevents its WV3 convergence at the same rate as
of T1, T2, and T3 (see Fig. 5). The third iteration (i.e.,
l = 5) improves approximations, and it brings WV3 of all
the test to a same scale, and accordingly provides reliable
WM3. Further iterations squeeze the approximations and
reach to WV3 = 1.1 for all tests at sixth iteration (i.e.,
l = 8) which indicates a considerable consistency of WM3.
Therefore, the method determines WV3 and WM3 for all test
at the same scale, given the considerable amount of noise
(specially at T4). This confirms that even a low amount
of perceptible features of the figures is adequate to tune
the initial approximations to reliable approximations. For
details of the noise impact on other approximations, refer to
Supp. Fig.2.17-20.
3.3 Image resolution defines maximum number of iter-
ations
When abstracting an image frame, up until a certain itera-
tion, a segment consists of multiple pixels. However beyond
that iteration, a segment might be smaller than a pixel (i.e.
one pixel belongs to multiple segments). To determine a
segment to which a pixel belongs to, the method rounds the
position of the pixel. Therefore, beyond a certain iteration,
the rounding procedure could potentially increase the dis-
tance between the abstractions of two image frames. In such
condition, the WV3 converges up-until a certain iteration,
Shape 1 
T1: Shape 2 T2: Shape 2 T3: Shape 2 T4: Shape 2 
234° rotation 234° rotation + 5? noise 234° rotation + 50? noise 234° rotation + 500? noise 
     
  
 
1
10
100
L = 3 L = 4 L = 5 L = 6 L = 7 L = 8
St
an
d
ar
d
 D
e
vi
at
io
n
 V
al
u
e
Iterations
Standard Deviation
Among Top-3 Approximations
T1
T2
T3
T4
0
45
90
135
180
225
270
315
360
L = 3 L = 4 L = 5 L = 6 L = 7 L = 8
D
e
gr
e
e
s 
o
f 
R
o
ta
ti
o
n
Iterations
Weighted Arithmetic Mean
Among Top-3 Approximations
T1
T2
T3
T4
Fig. 5. Evaluation of random noise impact on transformation determina-
tion.s
and it is saturated beyond that iteration, and accordingly is
the WM3 (see Supp. Fig.2.22-23). Therefore, maximum num-
ber of iterations, and accordingly the number of segments
and sectors are the function of shape resolution.
3.4 Pin-pointed transformation vs. condensed approxi-
mations
The linear mapping transformation between two shapes
is determined either as a single transformation with con-
siderable discrepancy with the rest of the approximations
(e.g., panel A on Fig. 4), or a condensed distribution of ap-
proximated transformations around actual transformation
(e.g., panel B on Fig. 4). This behavior originates from the
discreet representation of image frames (raster graphics);
such that, when drawing a Shape B from Shape A, a pixel
of Shape A is mapped to a rounded position on Shape B.
Therefore, pixels of Shape A could overlap as mapped on
Shape B. For instance, the two pixels at ?x1 = 4, y1 = 4?
, ?x2 = 4, y2 = 5? belonging to the segment/sector Vnm
of Shape A, with 70? rotation, respectively map to positions
?x?1 = ?0.562, y?1 = 5.628? and ?x?2 = ?1.33, y?2 = 6.262?.
As the coordinates are rounded, the two pixels map to
position ??1, 6? belonging to the segment/sector Vn?m? of
Shape B. Therefore, two pixels of Shape A map to one pixel
on Shape B (surjective linear transformation). Accordingly,
as abstracting the shapes using aggregation function count
(see Section 2.2), the abstraction parameters are calculated
as it follows: ?nm = 2 and ?n?m? = 1 (e.g., see comparison
of ? value distribution plots on Supp. Fig.2.1-22). Hence,
comparing ?nm and ?n?m? results to j(?nm, ?n?m?) = 0.33
as opposed to expected j(?nm, ?n?m?) = 1. Such scenarios
prevents “pin-pointing” the actual transformation (in this
case 70?) and rather provides a condensed distribution
of transformations around actual transformation (e.g., see
panel B on Fig. 4).
3.5 A small similarity is sufficient to determine a reli-
able linear mapping approximation
Ideal scenario for comparing two shapes is when there exist
a one-to-one correspondence (injective/surjective) between
pixels of two the shapes. However, for variety of reasons
REASONING WITH SHAPES 9
discussed as it follows, the rotation function on raster
graphics is surjective. For instance, rotation function may
map multiple pixels of Shape A to one pixel of Shape B,
causing a percentage of deformation on Shape B (e.g., see
supp. Fig.2.21), and preventing “pin-pointing” actual trans-
formation (as above-discussed). Additionally, shapes are
possibly subject to noise, which would prevent one-to-one
correspondence between the two shapes (non-surjective).
Moreover, Shape A may consist of congruent figures (e.g.,
two side-by-side circles of the same radius), and if Shape B
is determined by ?? rotation of Shape A, then in addition
to ??, multiple rotation angles may also map the congruent
shapes on each other. In such cases, actual transformation
is determined using incongruent elements (e.g., saddle area,
or pedal of the bicycle on Fig. 4). Such prominent details not
only improve approximations for congruent shapes, but are
also advantageous when the majority of the shape is covered
by noise (e.g., Fig.5) or is deformed (e.g., Supp. Fig.2.21).
The method discussed in present study, minimizes the
impact of such discrepancies on linear mapping transfor-
mation determination, by calculating the similarity of two
corresponding segments independently from the rest of the
segments (an adaptive neighborhood operation of custom
range is optionally enabled). Therefore, a higher similarity
between few segments is adequate to determine mapping
transformation with considerable accuracy. The experiments
on deformed, congruent, and noisy image frames illustrate
the accuracy of the proposed method on such scenarios.
4 CONCLUSION AND DISCUSSION
Understanding human brain mechanism and adapting it to
different disciplines has always been of interest in various
disciplines, such as in deep learning [69], [70]. Ballard de-
fined deictic computation [48] that benefits computational
methods to study the connection between body and real
world movements, to cognitive tasks. Roger B. Nelsens
“Proofs without Words” [71] and Martin Gardners “aha!
Solutions” [72] encouraged the present research to focus
on a cognitive-oriented approach for linear mapping trans-
formation between two shapes. The proposed method iter-
ates over different abstractions of image frames, from the
most abstracted to the most detailed, while tuning the top
transformations of previous iteration to obtain best mapping
linear transformations.
The proposed method is implemented and tested over
variety of inputs. The method is assessed for its accuracy
on determining linear mapping transformations. The exper-
iments showed that the output of the method is reliable even
under challenging conditions such as deformed and noise
image frames. Additionally, the size of abstraction matrix
(?) is independent from the size of the input images frames,
and accordingly the computational cost of the proposed
method is independent from the resolution of input image
frames.
REFERENCES
[1] K. Koffka, Principles of Gestalt psychology. Routledge, 2013, vol. 44.
[2] L. Sharan, C. Liu, R. Rosenholtz, and E. H. Adelson, “Recogniz-
ing materials using perceptually inspired features,” International
journal of computer vision, vol. 103, no. 3, pp. 348–371, 2013.
[3] J. J. DiCarlo and D. D. Cox, “Untangling invariant object recogni-
tion,” Trends in cognitive sciences, vol. 11, no. 8, pp. 333–341, 2007.
[4] B. Horn, Robot vision. MIT press, 1986.
[5] D. G. Lowe, “Distinctive image features from scale-invariant key-
points,” International journal of computer vision, vol. 60, no. 2, pp.
91–110, 2004.
[6] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio,
“Robust object recognition with cortex-like mechanisms,” IEEE
transactions on pattern analysis and machine intelligence, vol. 29, no. 3,
pp. 411–426, 2007.
[7] H. Zhang, A. C. Berg, M. Maire, and J. Malik, “Svm-knn: Discrim-
inative nearest neighbor classification for visual category recogni-
tion,” in 2006 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR’06), vol. 2. IEEE, 2006, pp. 2126–
2136.
[8] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features:
Spatial pyramid matching for recognizing natural scene cate-
gories,” in 2006 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR’06), vol. 2. IEEE, 2006, pp.
2169–2178.
[9] J. Mutch and D. G. Lowe, “Multiclass object recognition with
sparse, localized features,” in 2006 IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition (CVPR’06), vol. 1.
IEEE, 2006, pp. 11–18.
[10] G. Wang, Y. Zhang, and L. Fei-Fei, “Using dependent regions
for object categorization in a generative framework,” in 2006
IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR’06), vol. 2. IEEE, 2006, pp. 1597–1604.
[11] N. Pinto, D. D. Cox, and J. J. DiCarlo, “Why is real-world visual
object recognition hard?” PLoS Comput Biol, vol. 4, no. 1, p. e27,
2008.
[12] E. H. Adelson, “On seeing stuff: the perception of materials by
humans and machines,” in Photonics West 2001-electronic imaging.
International Society for Optics and Photonics, 2001, pp. 1–12.
[13] S. A. Shafer and B. A. Maxwell, “Color as a carrier of physical
information,” Color perception: Philosophical, psychological, artistic
and computation perspectives, pp. 52–71, 2000.
[14] S. A. Niyogi and E. H. Adelson, “Analyzing and recognizing
walking figures in xyt,” in Computer Vision and Pattern Recognition,
1994. Proceedings CVPR’94., 1994 IEEE Computer Society Conference
on. IEEE, 1994, pp. 469–474.
[15] J. Little and J. Boyd, “Recognizing people by their gait: the shape
of motion,” Videre: Journal of Computer Vision Research, vol. 1, no. 2,
pp. 1–32, 1998.
[16] J. B. Hayfron-Acquah, M. S. Nixon, and J. N. Carter, “Automatic
gait recognition by symmetry analysis,” Pattern Recognition Letters,
vol. 24, no. 13, pp. 2175–2183, 2003.
[17] F. Wang and Y. Li, “Learning visual symbols for parsing human
poses in images,” arXiv preprint arXiv:1304.6291, 2013.
[18] D. Ramanan, “Learning to parse images of articulated bodies,” in
Advances in neural information processing systems, 2006, pp. 1129–
1136.
[19] F. Attneave, “Apparent movement and the what-where connec-
tion.” Psychologia: An International Journal of Psychology in the
Orient, 1974.
[20] S. Ullman, The interpretation of visual motion. Massachusetts Inst
of Technology Pr, 1979.
[21] V. S. Ramachandran and S. M. Anstis, “The perception of apparent
motion,” Scientific American, vol. 254, no. 6, pp. 102–109, 1986.
[22] M. R. Dawson, “The how and why of what went where in ap-
parent motion: modeling solutions to the motion correspondence
problem.” Psychological review, vol. 98, no. 4, p. 569, 1991.
[23] J. E. Cutting and D. R. Proffitt, “The minimum principle and the
perception of absolute, common, and relative motions,” Cognitive
Psychology, vol. 14, no. 2, pp. 211–246, 1982.
[24] K.-Y. Eom, J.-Y. Jung, and M.-H. Kim, “A heuristic search-based
motion correspondence algorithm using fuzzy clustering,” Inter-
national Journal of Control, Automation and Systems, vol. 10, no. 3,
pp. 594–602, 2012.
[25] L. BARTOSHUK, M. HARNED, and L. PARKS, “Mental rotation
of three-dimensional objects,” Anim. Behav, vol. 8, p. 54, 1960.
[26] L. A. Cooperau and R. N. Shepard, “The time required to prepare
for a rotated stimulus,” Memory & Cognition, vol. 1, no. 3, pp. 246–
250, 1973.
[27] L. A. Cooper, “Demonstration of a mental analog of an external
rotation,” Perception & Psychophysics, vol. 19, no. 4, pp. 296–302,
1976.
REASONING WITH SHAPES 10
[28] L. A. Cooper and R. N. Shepard, “Chronometric studies of the
rotation of mental images.” 1973.
[29] R. Shepard and J. Metzler, “Mental rotation of 3-dimensional
objects.[article],” Science, vol. 171, p. 3972, 1971.
[30] G. Wahba, “A least squares estimate of satellite attitude,” SIAM
review, vol. 7, no. 3, pp. 409–409, 1965.
[31] M. D. Shuster and S. Oh, “Three-axis attitude determination from
vector observations,” Journal of Guidance, Control, and Dynamics,
2012.
[32] F. L. Markley, “Attitude determination using vector observations:
a fast optimal matrix algorithm,” 1993.
[33] ——, “Attitude determination using vector observations and the
singular value decomposition,” The Journal of the Astronautical
Sciences, vol. 36, no. 3, pp. 245–258, 1988.
[34] J. C. Gower and G. B. Dijksterhuis, Procrustes problems. Oxford
University Press on Demand, 2004, no. 30.
[35] T. Viklands, “Algorithms for the weighted orthogonal procrustes
problem and other least squares problems,” 2006.
[36] W. Kabsch, “A solution for the best rotation to relate two sets of
vectors,” Acta Crystallographica Section A: Crystal Physics, Diffrac-
tion, Theoretical and General Crystallography, vol. 32, no. 5, pp. 922–
923, 1976.
[37] Z. Xu, V. Kumar, Y. Matsuoka, and E. Todorov, “Design of an
anthropomorphic robotic finger system with biomimetic artificial
joints,” in 2012 4th IEEE RAS & EMBS International Conference on
Biomedical Robotics and Biomechatronics (BioRob). IEEE, 2012, pp.
568–574.
[38] T. J. Duff, D. M. Chong, P. Taylor, and K. G. Tolhurst, “Pro-
crustes based metrics for spatial validation and calibration of two-
dimensional perimeter spread models: A case study considering
fire,” Agricultural and forest meteorology, vol. 160, pp. 110–117, 2012.
[39] M. Mart??n-Loeches, E. Bruner, J. M. de la Cue?tara, and R. Colom,
“Correlation between corpus callosum shape and cognitive per-
formance in healthy young adults,” Brain Structure and Function,
vol. 218, no. 3, pp. 721–731, 2013.
[40] A. M. Johnson, “Speed of mental rotation as a function of problem-
solving strategies,” Perceptual and motor skills, vol. 71, no. 3, pp.
803–806, 1990.
[41] M. R. Greene and A. Oliva, “The briefest of glances the time course
of natural scene understanding,” Psychological Science, vol. 20,
no. 4, pp. 464–472, 2009.
[42] T. Konkle, T. F. Brady, G. A. Alvarez, and A. Oliva, “Scene memory
is more detailed than you think the role of categories in visual
long-term memory,” Psychological Science, vol. 21, no. 11, pp. 1551–
1556, 2010.
[43] V. Izard, G. Dehaene-Lambertz, and S. Dehaene, “Distinct cerebral
pathways for object identity and number in human infants,” PLoS
Biol, vol. 6, no. 2, p. e11, 2008.
[44] K. Wynn, P. Bloom, and W.-C. Chiang, “Enumeration of collective
entities by 5-month-old infants,” Cognition, vol. 83, no. 3, pp. B55–
B62, 2002.
[45] T. Sharon and K. Wynn, “Individuation of actions from continuous
motion,” Psychological Science, vol. 9, no. 5, pp. 357–362, 1998.
[46] H. Intraub, “The representation of visual scenes,” Trends in cogni-
tive sciences, vol. 1, no. 6, pp. 217–222, 1997.
[47] C. M. MacLeod, “The concept of inhibition in cognition,” Inhibition
in cognition, pp. 3–23, 2007.
[48] D. H. Ballard, M. M. Hayhoe, P. K. Pook, and R. P. Rao, “Deictic
codes for the embodiment of cognition,” Behavioral and Brain
Sciences, vol. 20, no. 04, pp. 723–742, 1997.
[49] P. E. Agre and D. Chapman, “Pengi: An implementation of a
theory of activity.” in AAAI, vol. 87, no. 4, 1987, pp. 286–272.
[50] D. H. Ballard, “Animate vision,” Artificial intelligence, vol. 48, no. 1,
pp. 57–86, 1991.
[51] B. Girod, G. Greiner, and H. Niemann, Principles of 3D image
analysis and synthesis. Springer Science & Business Media, 2013,
vol. 556.
[52] H. Hirschmuller and D. Scharstein, “Evaluation of stereo matching
costs on images with radiometric differences,” IEEE transactions
on pattern analysis and machine intelligence, vol. 31, no. 9, pp. 1582–
1599, 2009.
[53] S. Takahashi, Y. Takeshima, and I. Fujishiro, “Topological volume
skeletonization and its application to transfer function design,”
Graphical Models, vol. 66, no. 1, pp. 24–49, 2004.
[54] R. Fabbri, L. D. F. Costa, J. C. Torelli, and O. M. Bruno, “2d
euclidean distance transform algorithms: A comparative survey,”
ACM Computing Surveys (CSUR), vol. 40, no. 1, p. 2, 2008.
[55] N. R. Pal and S. K. Pal, “A review on image segmentation tech-
niques,” Pattern recognition, vol. 26, no. 9, pp. 1277–1294, 1993.
[56] D. J. Rogers and T. T. Tanimoto, “A computer program for classi-
fying plants.” Science (New York, NY), vol. 132, no. 3434, pp. 1115–
1118, 1960.
[57] T. Sørensen, “{A method of establishing groups of equal ampli-
tude in plant sociology based on similarity of species and its
application to analyses of the vegetation on Danish commons},”
Biol. Skr., vol. 5, pp. 1–34, 1948.
[58] J. R. Bray and J. T. Curtis, “An ordination of the upland forest com-
munities of southern wisconsin,” Ecological monographs, vol. 27,
no. 4, pp. 325–349, 1957.
[59] Y. Rubner, C. Tomasi, and L. J. Guibas, “The earth mover’s distance
as a metric for image retrieval,” International journal of computer
vision, vol. 40, no. 2, pp. 99–121, 2000.
[60] J. M. Henderson and A. Hollingworth, “Eye movements during
scene viewing: An overview,” Eye guidance in reading and scene
perception, vol. 11, pp. 269–293, 1998.
[61] K. Rayner, “Eye movements in reading and information process-
ing: 20 years of research.” Psychological bulletin, vol. 124, no. 3, p.
372, 1998.
[62] E. Kowler and S. Anton, “Reading twisted text: Implications for
the role of saccades,” Vision research, vol. 27, no. 1, pp. 45–60, 1987.
[63] L. Fei-Fei, A. Iyer, C. Koch, and P. Perona, “What do we perceive
in a glance of a real-world scene?” Journal of vision, vol. 7, no. 1,
pp. 10–10, 2007.
[64] T. H. Cormen, Introduction to algorithms. MIT press, 2009.
[65] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. Lai, N. Davis, and
F. Nuflo, “Modeling visual attention via selective tuning,” Artificial
intelligence, vol. 78, no. 1, pp. 507–545, 1995.
[66] C. W. Eriksen and J. E. Hoffman, “Temporal and spatial charac-
teristics of selective encoding from visual displays,” Perception &
psychophysics, vol. 12, no. 2, pp. 201–204, 1972.
[67] C. W. Eriksen and J. D. S. James, “Visual attention within and
around the field of focal attention: A zoom lens model,” Perception
& psychophysics, vol. 40, no. 4, pp. 225–240, 1986.
[68] A. Oliva, A. Torralba, and P. G. Schyns, “Hybrid images,” in ACM
Transactions on Graphics (TOG), vol. 25, no. 3. ACM, 2006, pp.
527–532.
[69] G. E. Hinton, “Learning multiple layers of representation,” Trends
in cognitive sciences, vol. 11, no. 10, pp. 428–434, 2007.
[70] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algo-
rithm for deep belief nets,” Neural computation, vol. 18, no. 7, pp.
1527–1554, 2006.
[71] M. S. MacKenzie and R. B. Nelsen, Proofs without words: Exercises
in visual thinking. MAA, 1993, no. 1.
[72] M. Gardner, Aha! Aha! insight. Scientific American, 1978, vol. 1.
