Speeding-up the decision making of a learning agent using an ion trap quantum
processor
Theeraphot Sriarunothai,1 Sabine Wo?lk,2, 1 Gouri Shankar Giri,1 Nicolai
Friis,3, 2 Vedran Dunjko,2, 4 Hans J. Briegel,2, 5 and Christof Wunderlich1, ?
1Department Physik, Naturwissenschaftlich-Technische Fakulta?t,
Universita?t Siegen, Walter-Flex-Str. 3, 57068 Siegen, Germany
2Institute for Theoretical Physics, University of Innsbruck, Technikerstraße 21a, 6020 Innsbruck, Austria
3Institute for Quantum Optics and Quantum Information,
Austrian Academy of Sciences, Boltzmanngasse 3, 1090 Vienna, Austria
4Max Planck Institute of Quantum Optics Garching 85748, Germany
5Department of Philosophy, University of Konstanz, 78457 Konstanz, Germany
We report a proof-of-principle experimental demonstration of a quantum speed-up for learn-
ing agents utilizing a small-scale quantum information processor based on radiofrequency-driven
trapped ions. The decision-making process of a quantum learning agent within the projective simu-
lation paradigm for machine learning is modeled in a system of two qubits. The latter are realized in
the hyperfine levels of two frequency-addressed ions exposed to a static magnetic field gradient. The
deliberation algorithm is implemented using single-qubit rotations and two-qubit conditional quan-
tum dynamics. We show that the deliberation time of this quantum learning agent is quadratically
improved with respect to comparable classical learning agents. The performance of this quantum-
enhanced learning agent highlights the potential of scalable ion trap quantum processors taking
advantage of machine learning.
INTRODUCTION
The past decade has seen the parallel advance of two
research areas — quantum computation [1] and artifi-
cial intelligence [2] — from abstract theory to practical
applications and commercial use. Quantum computers,
operating on the basis of information coherently encoded
in superpositions of states that could be considered clas-
sical bit values, hold the promise of exploiting quantum
advantages to outperform classical algorithms, e.g., for
searching databases [3], factoring numbers [4], or even for
precise parameter estimation [5, 6]. At the same time, ar-
tificial intelligence and machine learning have become in-
tegral parts of modern automated devices using classical
processors (see, e.g., [7–10]). Despite this seemingly si-
multaneous emergence and promise to shape future tech-
nological developments, the overlap between these areas
still offers a number of unexplored problems [11]. It is
hence of fundamental and practical interest to determine
how quantum information processing and autonomously
learning machines can mutually benefit from each other.
Within the area of artificial intelligence, a central com-
ponent of modern applications is the learning paradigm
of an agent interacting with an environment [2, 12, 13]
illustrated in Fig. 1 (A), which is usually formalized as
so-called reinforcement learning. This entails receiving
perceptual input and being able to react to it in different
ways. The learning aspect is manifest in the reinforce-
ment of the connections between the inputs and actions,
where the correct association is (often implicitly) speci-
fied by a reward mechanism, which may be external to
? wunderlich@physik.uni-siegen.de
the agent. In this very general context, an approach to
explore the intersection of quantum computing and arti-
ficial intelligence is to equip autonomous learning agents
with quantum processors for their deliberation procedure
1. That is, an agent chooses its reactions to perceptual
input by way of quantum algorithms or quantum random
walks. The agent’s learning speed can then be quantified
in terms of the average number of interactions with the
environment until targeted behavior (reactions triggering
a reward) is reproduced by the agent with a desired effi-
ciency. Unfortunately, the learning speed cannot generi-
cally be improved by incorporating quantum technologies
into the agent’s design [14].
However, a recent model [15] for learning agents based
on projective simulation (PS) [13] allows for a generic
speed-up in the agent’s deliberation time during each in-
dividual interaction. This quantum improvement in the
reaction speed has been established within the reflecting
projective simulation (RPS) variant of PS [15]. There,
the desired actions of the agent are chosen according to a
specific probability distribution that can be modified dur-
ing the learning process. This is of particular relevance
to adapt to rapidly changing environments [15], as we
shall elaborate on in the next section. For this task, the
deliberation time of classical RPS agents is proportional
to the quantities 1/? and 1/. These characterize the
time needed to generate the specified distribution in the
agent’s internal memory and the time to sample a suit-
able (e.g., rewarded rather than an unrewarded) action
from it, respectively. A quantum RPS (Q-RPS) agent, in
1 Other approaches that we will not further discuss here concern
models where the environment, and the agent’s interaction with
it may be of quantum mechanical nature as well [14].
ar
X
iv
:1
70
9.
01
36
6v
1 
 [
qu
an
t-
ph
] 
 5
 S
ep
 2
01
7
2
(A)
En
vi
ro
nm
en
t
Sensors
Actuators
Projective
Simulation
Memory
Percepts
Actions
Agent
(B)
Q-RPS
FIG. 1. Learning agent & quantum reflecting projec-
tive simulation (Q-RPS). (A) Learning agents receive per-
ceptual input (“percepts”) from and act on the environment.
The projective simulation (PS) decision-making process draws
from the agent’s memory and can be modeled as a random
walk in a clip network, which, in turn, is represented by a
stochastic matrix P . (B) Q-RPS agents enhance the relative
probability of (desired) actions (green columns) compared to
other clips (grey) that may include undesired actions or per-
cepts (blue) within the stationary distribution of P before
sampling, achieving a quadratic speed-up w.r.t. to classical
RPS agents.
contrast, is able to obtain such an action quadratically
faster, i.e., within a time of the order 1/
?
?.
Here, we report on the first proof-of-principle exper-
imental demonstration of quantum-enhanced reinforce-
ment learning system, complementing the recent exper-
imental works in the context of (un)supervised learning
[16–18]. We model the deliberation process of an RPS
learning agent in a system of two qubits that are encoded
in the energy levels of one ion each. Within experimen-
tal uncertainties, our results confirm the agent’s action
output according to the desired distributions and within
deliberation times that are quadratically improved with
respect to comparable classical agents. This laboratory
demonstration of speeding up a learning agent’s delibera-
tion process can be seen as the first experiment combining
novel concepts from machine learning with the potential
of ion trap quantum computers where complete quantum
algorithms have been demonstrated [19–22] and feasible
concepts for scaling up [23–25] are vigorously pursued.
THEORETICAL FRAMEWORK OF RPS
A generic picture for modeling autonomous learning
scenarios is that of repeated rounds of interaction be-
tween an agent and its environment. In each round the
agent receives perceptual input (“percepts”) from the
environment, processes the input using an internal de-
liberation mechanism, and finally acts upon (or reacts
to) the environment, i.e., performs an “action” (see, e.g.,
Ref. [13]). Depending on the reward system in place and
the given percept, such actions may be rewarded or not,
which leads the agent to update its deliberation process,
the agent learns.
Within the projective simulation (PS) [13] paradigm
for learning agents, the decision-making procedure is cast
as a (physically motivated) stochastic diffusion process
within an episodic compositional memory (ECM), i.e., a
(classical or quantum) random walk in a representation
of the agent’s memory containing the interaction history.
One may think of the ECM as a network of clips that
can correspond to remembered percepts, remembered ac-
tions, or combinations thereof. Mathematically, this clip
network is described by a stochastic matrix (defining a
Markov chain) P = (pij), where the pij with 0 ? pij ? 1
and
?
i pij = 1 represent transition probabilities between
the clips labeled i and j with i, j ? {1, 2, . . . , N}. The
learning process is implemented through an update of
the N ×N matrix P , which, in turn, serves as a basis for
the random walks in the clip network. Different types of
PS agents vary in their deliberation mechanisms, update
rules, and other specifications.
In particular, one may distinguish between PS agents
based on “hitting” and “mixing”. For the former type of
PS agent, a random walk could, for instance, start from
a clip c1 called by the initially received percept. The
first “step” of the random walk then corresponds to a
transition to clips cj with probabilities p1j . The agent
then samples from the resulting distribution {p1j}j . If
such a sample provides an action, e.g., if the clip ck is
“hit”, this action is selected as output, otherwise the
walk continues on from the clip ck. An advanced variant
of the PS model based on “mixing” is reflecting projec-
tive simulation (RPS) [15]. There, the Markov chain is
first “mixed”, i.e., an appropriate number 2 of steps are
applied until the stationary distribution is attained (ap-
proximately), before a sample is taken. This, or other im-
plementations of random walks in the clip network pro-
vide the basis for the PS framework for learning. The
classical PS framework can be used to solve standard
textbook problems in reinforcement learning [26–28], and
has recently been applied in advanced robotics [29], adap-
tive quantum computation [30], as well as in the machine-
generated design of quantum experiments [31].
Here, we focus on RPS agents, where the deliberation
process based on mixing allows for a speed-up of Q-RPS
agents w.r.t. to their classical counterparts [15]. In con-
trast to basic hitting-based PS agents, the clip network of
RPS agents is structured into several sub-networks, one
for each percept clip, and each with its own stochastic
matrix P . In addition to being stochastic, these matri-
2 The mixing time depends on the spectral gap ? of the Markov
chain P , i.e., the difference between the two largest eigenvalues
of P [15].
3
ces specify Markov chains which are ergodic [15], which
ensures that the Markov chain in question has a unique
stationary distribution, i.e., a unique eigenvector ? with
eigenvalue +1, P? = ?. Starting from any initial state,
continued application of P (or its equivalent in the quan-
tized version) mixes the Markov chain, leaving the system
in the stationary state.
As part of their deliberation process, RPS agents gen-
erate stationary distributions over their clip space, as
specified by P , which is updated as the agent learns.
These distributions have support over the whole sub-
network clip space, and additional specifiers – flags –
are used to ensure an output from a desired sub-set of
clips. For instance, standard agents are presumed to
output actions only, in which case only the actions are
“flagged”3. This ensures that an action will be output,
while maintaining the relative probabilities of the actions.
The same mechanism of flags, which can be thought of
as short-term memory, is also used to eliminate iterated
attempts of actions which did not yield rewards in recent
time-steps. This leads to a more efficient exploration of
correct behavior.
In the quantum version of RPS, each clip ci is repre-
sented by a basis vector |i? in a Hilbert space H. The
mixing process is then realized by a diffusion process on
two copies of the original Hilbert space. On the doubled
spaceH?H a unitary operator W (P ) (called the Szegedy
walk operator [32, 33]) and a quantum state |??? with
W (P ) |??? = |??? take the roles of the classical objects
P and ?. Both W (P ) and |??? depend on a set of uni-
taries Ui on H that act as Ui |0? =
?
j
?
pij |j? for some
reference state |0? ? H, i.e., |??? =
?
i
?
ai |i? ? Ui |0?.
The more intricate construction of W (P ) is given in de-
tail in [34]. The feature of the quantum implementation
of RPS that is crucial for us here is an amplitude am-
plification similar to Grover’s algorithm [3], which incor-
porates the mixing of the Markov chain and allows out-
putting flagged actions after an average of O(1/
?
) calls
to W (P ), where  is the probability of sampling an action
from the stationary distribution. The algorithm achiev-
ing this is structured as follows. After an initialization
stage where |??? is prepared, a number of diffusion steps
are carried out. Each such step consists of two parts. The
first part is a reflection over the states corresponding to
actions in the first copy of H, i.e., an operation
refA = 2
n?
i=1
|i??i| ? 1? 1? 1, (1)
where A = span{|1? , . . . , |n?} denotes the subspace of
the clip network corresponding to actions. In the second
part, an approximate reflection over the state |???, the
mixing, is carried out, i.e., an operation designed to ap-
proximate ref?? = 2 |??????| ? 1 [15]. This second step
3 Such flags are rudimentary emoticons defined in[13]
involves O(1/
?
?) calls to W (P ). The two-part diffu-
sion steps are repeated O(1/
?
) times before a sample is
taken from the resulting state by measuring in the basis
{|i?}i=1,...,N . If an action is sampled, the algorithm con-
cludes and that action is chosen as output. Otherwise,
all steps are repeated. Since the algorithm amplifies the
probability of sampling an action (almost) to unity, car-
rying out the deliberation procedure with the help of such
a Szegedy walk hence requires an average of O(1/
?
?)
calls to W (P ). In comparison, a classical RPS agent
would require an average of O(1/?) applications of P to
mix the Markov chain, and an average of O(1/) sam-
ples to find an action. Q-RPS agents can hence achieve
a quadratic speed-up in their reaction time.
Here, it should be noted that, its elegance not with-
standing, the construction of the approximate reflection
for general RPS networks is extremely demanding for
current quantum computational architectures. Most no-
tably, this is due to the requirement of two copies of
H, on which frequently updated coherent conditional op-
erations need to be carried out [34–36]. However, as
we shall explain now, these requirements can be cir-
cumvented for the interesting class of rank-one Markov
chains. In this special case, the entire Markov chain P
can be represented on one copy of H by a single uni-
tary UP = Ui ?i, since all columns of P are identical.
Conceptually, this simplification corresponds to a situ-
ation where each percept-specific clip network contains
only actions and the Markov chain is mixed in one step
(? = 1). In such a case one uses flags to mark desired ac-
tions. Interestingly, these minor alterations also allow to
establish a one-to-one correspondence with the hitting-
based basic PS using two-layered networks, which has
been applied in classical task environments [26–31].
Let us now discuss how the algorithm above is mod-
ified for the rank-one case with the flagging mechanism
in place. First, we restrict A to be the subspace of the
flagged actions only, assuming that there are n  N
of these, and we denote the corresponding probabili-
ties within the stationary distribution by a1, . . . , an. In
the initialization stage, the state |?? =
?
i=1,...,N
?
ai |i?
is prepared. Then, an optimal number of k diffusion
steps [3] is carried out, where
k = round
(
?
4
?

? 12
)
, (2)
and  =
?
i=1,...,n ai is the probability to sample a flagged
action from the stationary distribution. Within the dif-
fusion steps, the reflections over all actions of Eq. (1) are
replaced by reflections over flagged actions, i.e.,
refA = 2
n?
i=1
|i??i| ? 1. (3)
In the rank-one case, the reflections over the stationary
distribution ? becomes an exact reflection ref? over the
state |?? and can be carried out on one copy of H [34].
After the diffusion steps, a sample is taken and the agent
4
checks if the obtained action is marked with a flag. If
this is the case, the action is chosen as output, otherwise
the algorithm starts anew.
While a classical RPS agents requires an average of
O(1/) samples until obtaining a flagged action, this
number reduces to O(1/
?
) for Q-RPS agents. This
quantum advantage is particularly pronounced when the
overall number of actions is very large compared to n
and the environment is unfamiliar to the agent or has re-
cently changed its rewarding pattern, in which case  may
be very small. Given some time, both agents learn to as-
sociate rewarded actions with a given percept, suitably
add or remove flags, and adapt P (and by extension ?).
In the short run, however, classical agents may be slow
to respond and the advantage of a Q-RPS agent becomes
apparent. Despite the remarkable simplification of the
algorithm for the rank-one case with flags, the quadratic
speed-up is hence preserved.
EXPERIMENTAL IMPLEMENTATION OF
RANK-ONE RPS
The proof-of-principle experiment that we report in
this paper experimentally demonstrates the quantum
speed-up of quantum-enhanced learning agents. That is,
we are able to empirically confirm both the quadratically
improved scaling of O(1/
?
), and the correct output ac-
cording to the tail of the stationary distribution. Here,
 denotes the initial probability of finding a flagged ac-
tion within the stationary distribution ? = {ai}) for the
average number of calls of the diffusion operator before
sampling one of the desired actions. The tail is defined
as the first n components of ?. The latter means that
aj/ak = bj/bk ?j, k ? {1, . . . , n}, where bj denotes the
final probability that the agent obtains the flagged action
labeled j. Note that the Q-RPS algorithm enhances the
overall probability of obtaining a flagged action such that
? ?
n?
i=1
bi >
n?
i=1
aj = , (4)
whilst maintaining the relative probabilities of the
flagged actions according to the tail of ?, as illustrated
in Fig. 1 (B).
For the implementation we hence need at least a three-
dimensional Hilbert space that we realize in our experi-
ment using two qubits encoded in the energy levels of two
trapped ions (see the experimental section): Two states
to represent two different flagged actions (represented in
our experiment by |00? and |01?), and at least one addi-
tional state for all non-flagged actions (|10? and |11? in
our experiment). The preparation of the stationary state
is implemented by
|?? = UP (?1, ?2) |0? = R1(?1, ?2 )R2(?2,
?
2 ) |0? , (5)
where Rj(?, ?) is a single-qubit rotation on qubit j, i.e.,
Rj(?, ?) = exp
[
i ?2 (Xj cos?? Yj sin?)
]
. (6)
FIG. 2. Quantum circuit for Q-RPS. A rank-one Q-RPS
is implemented using two qubits. The diffusion step consist-
ing of reflections over the flagged actions and the stationary
distribution (shown once each) is repeated k times, where k
is given by Eq. (2). The specific pulse sequence implementing
this circuit is explained in the Supplementary Material.
Here, Xj , Yj , and Zj denote the usual Pauli operators
of qubit j. The total probability  = a00 + a01 for a
flagged action within the stationary distribution is then
determined by ?1 via
 = cos2(?1/2), (7)
whereas ?2 determines the relative probabilities of ob-
taining one of the flagged actions via
a00/ = cos
2(?2/2). (8)
The reflection over the flagged actions refA is here
given by a Z rotation, defined by Rj,z(?) = exp[?i ?2Zj ],
with rotation angle ?? for the first qubit,
refA = R1,z(??) = exp[i?2Z1]. (9)
The reflection over the stationary distribution can be per-
formed by a combination of single-qubit rotations deter-
mined by ?1 and ?2 and a CNOT gate given by
ref? = R1(?1 ? ?, ?2 )R2(?2 +
?
2 ,
?
2 )UCNOT
×R1(??1 ? ?, ?2 )R2(??2 ?
?
2 ,
?
2 ), (10)
which can be understood as two calls to UP (once in
terms of U†P ) supplemented by fixed single-qubit oper-
ations [34]. The total gate sequence for a single diffusion
step (consisting of a reflection over the flagged actions
followed by a reflection over the stationary distribution)
can hence be decomposed into single-qubit rotations and
CNOT gates and is shown in Fig. 2. The speed-up of
the rank-one Q-RPS algorithm w.r.t. a classical RPS
agent manifests in terms of a quadratically smaller av-
erage number of calls to UP (or, equivalently, to the dif-
fusion operator D = ref? refA) until a flagged action is
sampled. Since the final probability of obtaining a de-
sired action is ? ?
?
i=1,...,n bi, we require 1/? samples on
5
average, each of which is preceded by the initial prepara-
tion of |?? and k diffusion steps. The average number of
uses of UP to sample correctly is hence C =
(
2k()+1
)
/?,
which we refer to as ’cost’ in this paper. In the following,
it is this functional relationship between C and  that we
put to the test, along with the predicted ratio a00/a01 of
occurrence of the two flagged actions.
The experimental setup
Two 171Yb+ ions are confined in a linear Paul trap with
axial and radial trap frequencies of 2? × 117 kHz and
2? × 590 kHz, respectively. After Doppler cooling the
two ions form a linear Coulomb crystal, which is exposed
to a static magnetic field gradient of 19 T/m, generated
by a pair of permanent magnets. The ion-ion spacing
in this configuration is approximately 10 µm. Magnetic
gradient induced coupling (MAGIC) between ions results
in an adjustable qubit interaction mediated by the com-
mon vibrational modes of the Coulomb crystal [37]. In
addition, qubit resonances are individually shifted as a
result of this gradient and become position dependent.
This makes the qubits distinguishable and addressable
by their frequency of resonant excitation. The address-
ing frequency separation for this two-ion system is about
3.7 MHz. All coherent operations are performed using
radio frequency (RF) radiation near 12.6 GHz, match-
ing the respective qubit resonances [38]. A more detailed
description of the experimental setup can be found in
Refs. [21, 37, 39].
The qubits are encoded in the hyperfine manifold
of each ion’s ground state, representing an effective
spin 1/2 system. The qubit states |0? and |1? are
represented by the energy levels |2S1/2 , F = 0 ? and
|2S1/2 , F = 1,mF = +1 ? respectively. The ions are
Doppler cooled on the resonance |2S1/2 , F = 1 ? ?
|2P1/2 , F = 0 ? with laser light near 369 nm. Optical
pumping into long-lived meta-stable states is prevented
using laser light near 935 nm and 638 nm. The vibra-
tional excitation of the Doppler cooled ions is further
reduced by employing RF sideband cooling for both the
center of mass mode and the stretch mode . This leads to
a mean vibrational quantum number of ?n? ? 5 for both
modes. The ions are then initialized in the qubit state
|0? by state selective optical pumping with a 2.1 GHz
red-shifted Doppler-cooling laser on the |2S1/2 , F = 1 ?
? |2P1/2 , F = 1 ? resonance.
The desired qubit states are prepared by applying an
RF pulse resulting in a coherent qubit rotation with pre-
cisely defined rotation angle and phase (Eq. (6)). The
required number of diffusion steps is then applied to
both qubits, using appropriate single-qubit rotations and
a two-qubit ZZ-interaction given by
UZZ(?) = exp [i
?
2Z1Z2], (11)
which is directly realizable with MAGIC [37]. A CNOT
gate (UCNOT) can then be performed via
UCNOT = e
?i ?4R2(
?
2 ,
3?
2 )UZZ(
?
2 )R2(
?
2 , 0)R2,z(
?
2 )R1,z(?
?
2 ).
The required number of single qubit gates is optimized
by combining appropriate single qubit rotations together
from refA and ref? (see Fig. 2). Thus, we can simplify
the algorithm to
D =R2(?2,
?
2 )R1(?1,
?
2 )R2,z(?
?
2 )R1,z(
?
2 )
UZZ(
?
2 )R2(??2,
?
2 )R1(?1,
?
2 ), (12)
as shown in Fig. 1 of the Supplementary Materials.
During the evolution time of 4.24 ms for each diffu-
sion step both qubits are protected from decoherence by
applying universally robust (UR) dynamical decoupling
(DD) pulses [40]. The complete pulse sequence for the
experiment reported here can be found in Fig. 1 of the
Supplementary Materials.
Finally, projective measurements on both qubits are
performed in the computational basis {|0? , |1?} by scat-
tering laser light near 369 nm on the |2S1/2 , F = 1 ? ?
|2P1/2 , F = 0 ? transition, and detecting resonance flu-
orescence using an electron multiplying charge coupled
device (EMCCD) to determine the relative frequencies
b00, b01, b10, b11 for obtaining the states |00?, |01?, |10?,
and |11?, respectively.
Results
As discussed above, our goal is to test the two char-
acteristic features of rank-one Q-RPS: (i) the scaling of
the average cost C with , and (ii) the sampling ratio for
the different flagged actions. For the former, we expect a
scaling of 1/
?
, while we expect the ratio of the number
of occurrences of the two actions to be maintained with
respect to the relative probabilities given by the station-
ary distribution. Therefore, our first set of measurements
studies the behavior of the cost C as a function of the
total initial probability . The second set of measure-
ments studies the behavior of the output probability ra-
tio rf = b00/b01 as a function of input probability ratio
ri = a00/a01.
For the former, a series of measurements is performed
for different values of  corresponding to k = 1 to k = 7
diffusion steps after the initial state preparation. To ob-
tain the cost C =
(
2k() + 1
)
/?, where ? = b00 + b01,
we measure the probabilities b00 and b01 after k diffu-
sion steps and repeat the experiment 1600 times for fixed
. The average cost is then plotted against  as shown
in Fig. 3. The experimental data shows that the cost
decreases with  as 1/
?
, as desired. This is in good
agreement with the behavior expected for the ideal Q-
RPS algorithm. In the range of chosen probabilities  in
our experiment, the experimental result of Q-RPS out-
performs the classical RPS, as shown in Fig. 3. There-
fore, we demonstrate that our experimental efficiency is
6
0 0.05 0.1 0.15 0.2 0.25 0.3
0
5
10
15
20
25
Ideal Q-RPS
Ideal RPS
Experiment
Fit
FIG. 3. Scaling behavior of the learning agent’s cost
employing Q-RPS and RPS. After the preparation of |??,
k diffusion steps are applied before an action is sampled. This
procedure is repeated until a flagged action is obtained, ac-
cumulating a certain cost C, whose average is shown on the
vertical axis. Measurements are performed for different val-
ues of  corresponding to k = 1 to k = 7 diffusion steps. The
dashed black line and the solid blue line represent the behav-
ior expected for ideal Q-RPS (1/
?
) and ideal classical RPS
(1/), respectively. The fit to the experimental data confirms
that the scaling behavior follows a 1/
?
 behavior, and thus is
consistent with Q-RPS. The data show that the experimental
Q-RPS outperforms the classical RPS within the range of 
chosen in the experiment. The error bars on the x-axis repre-
sent the uncertainties in preparing the quantum states. The
error bars on the y-axis represent the statistical errors.
already good enough not only to get a better scaling, but
also to really beat the classical algorithm despite the off-
set in the cost function and the finite precision of our
quantum algorithm. The deviation from the ideal be-
havior is attributed to a small detuning of the RF pulses
implementing coherent operations, as we discuss in the
Supplementary Materials.
For the second set of measurements, we select a few cal-
culated probabilities a00 and a01 in order to obtain differ-
ent values of the input ratio ri = a00/a01 between 0 and
2, whilst keeping k() in a range between k = 1 and k = 3.
For these probabilities a00 and a01, the corresponding ro-
tation angles ?1 and ?2 of RF pulses intended for prepa-
ration are extracted using Eq. (7) and Eq. (8). We then
perform the Q-RPS algorithm for the specific choices of
k and repeat 1600 times to estimate the probabilities b00
and b01. We finally obtain the output ratio rf = b00/b01,
which is plotted against the input ratio in Fig. 4. The
0 0.5 1 1.5 2
0
0.5
1
1.5
2
FIG. 4. Output distribution. A comparison of the output
ratio of two flagged actions at the end of the algorithm with
the corresponding input ratio is shown. Measurements are
performed with k = 1 (red square) and k = 3 (blue circle)
diffusion steps. The black dashed line shows the behavior of
the ideal Q-RPS. The red and blue dashed lines, each repre-
senting a linear fit to the corresponding set of data, confirm
that the initial probability ratio is maintained. Error bars
represent statistical errors.
experimental data follows a straight line with an offset
from the behavior rf/ri = 1 expected for an ideal Q-RPS
agent. The slopes of the two fitted linear functions agree
within their respective error showing that the deviation
of the output ratio from the ideal result is independent of
the number of diffusion steps. In addition, this indicates
that this deviation is not caused by the quantum algo-
rithm itself, but by the initial state preparation and/or
by the final measurement process where such a devia-
tion can be caused by an asymmetry in the detection
fidelity. Indeed, the observed deviation is well explained
by a typical asymmetry in the detection fidelity of 3% as
encountered in the measurements presented here. This
implies reliability of the quantum algorithm also for a
larger number of diffusion steps. A detailed discussion of
experimental sources of error is given in the Supplemen-
tary Materials.
CONCLUSION
We investigate a quantum enhanced deliberation pro-
cess of a learning agent implemented in an ion trap quan-
tum processor. Our approach is centered around the pro-
jective simulation [13] model for reinforcement learning.
7
In a proof-of-principle experimental demonstration, we
verify that the deliberation process of the quantum learn-
ing agent is quadratically faster compared to that of a
classical learning agent. The experimental uncertainties
in the reported results, which are in excellent agreement
with our error model, do not interfere with this genuine
quantum advantage in the agent’s deliberation time. We
achieve results for the cost C for up to 7 diffusion steps
corresponding to an initial probability  = 0.01 to choose
a flagged action. The systematic variation of the the ra-
tio ri between the input probabilities, a00 and a01 for
flagged actions and the measurement of the ratio rf be-
tween the learning agent’s output probabilities, b00 and
b01 as a function of ri shows that the quantum algorithm
is reliable independent of the number of diffusion steps.
This experiment highlights the potential of a quantum
computer in the field of quantum enhanced learning and
artificial intelligence. A practical advantage, of course,
will become evident once larger percept spaces and gen-
eral rank-N Q-RPS are employed. Such extensions are,
from the theory side, unproblematic given that the mod-
ularized nature of the algorithm makes it scalable. An
experimental realization of such large-scale quantum en-
hanced learning will be feasible with the implementa-
tion of scalable quantum computer architectures. Mean-
while, all essential elements of Q-RPS have been success-
fully demonstrated in the proof-of-principle experiment
reported here.
ACKNOWLEDGMENTS
T. S., S. W., G. S. G. and C. W. acknowledge fund-
ing from Deutsche Forschungsgemeinschaft and from
Bundesministerium fu?r Bildung und Forschung (FK
16KIS0128). G. S. G. also acknowledges support from
the European Commission’s Horizon 2020 research and
innovation program under the Marie Sk lodowska-Curie
grant agreement number 657261. H. J. B. and N. F.
acknowledge support from the Austrian Science Fund
(FWF) through Grants No. SFB FoQuS F4012 and the
START project Y879-N27, respectively.
[1] M. A. Nielsen and I. L. Chuang, Quantum Computation
and Quantum Information (Cambridge University Press,
Cambridge, U.K., 2000).
[2] S. J. Russell and P. Norvig, Artificial Intelligence: A
Modern Approach, 2nd ed. (Pearson Education, 2003).
[3] L. K. Grover, in Proceedings of the Twenty-eighth Annual
ACM Symposium on Theory of Computing , STOC ’96
(ACM, New York, NY, USA, 1996) pp. 212–219.
[4] P. W. Shor, in Proceedings 35th Annual Symposium on
Foundations of Computer Science (1994) pp. 124–134.
[5] V. Giovannetti, S. Lloyd, and L. Maccone, Science 306,
1330 (2004).
[6] N. Friis, D. Orsucci, M. Skotiniotis, P. Sekatski, V. Dun-
jko, H. J. Briegel, and W. Du?r, New J. Phys. 19, 063044
(2017).
[7] K. Lim, Y. Hong, Y. Choi, and H. Byun, PLOS ONE
12, e0173317 (2017).
[8] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre,
G. van den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe,
J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap,
M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hass-
abis, Nature 529, 484 (2016).
[9] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu,
J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,
A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wier-
stra, S. Legg, and D. Hassabis, Nature 518, 529 (2015).
[10] J. Schaeffer, N. Burch, Y. Bjo?rnsson, A. Kishimoto,
M. Mu?ller, R. Lake, P. Lu, and S. Sutphen, Science
317, 1518 (2007).
[11] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
N. Wiebe, and S. Lloyd, “Quantum machine learning,”
(2016), arXiv:1611.09347.
[12] R. Sutton and A. Barto, Reinforcement learning (The
MIT Press, 1998).
[13] H. J. Briegel and G. De las Cuevas, Sci. Rep. 2, 400
(2012).
[14] V. Dunjko, J. M. Taylor, and H. J. Briegel, Phys. Rev.
Lett. 117, 130501 (2016).
[15] G. D. Paparo, V. Dunjko, A. Makmal, M. A. Martin-
Delgado, and H. J. Briegel, Phys. Rev. X 4, 031002
(2014).
[16] D. Riste?, M. P. da Silva, C. A. Ryan, A. W. Cross, A. D.
Co?rcoles, J. A. Smolin, J. M. Gambetta, J. M. Chow, and
B. R. Johnson, npj Quantum Information 3, 16 (2017).
[17] Z. Li, X. Liu, N. Xu, and J. Du, Phys. Rev. Lett. 114,
140504 (2015).
[18] X.-D. Cai, D. Wu, Z.-E. Su, M.-C. Chen, X.-L. Wang,
L. Li, N.-L. Liu, C.-Y. Lu, and J.-W. Pan, Phys. Rev.
Lett. 114, 110504 (2015).
[19] D. Hanneke, J. P. Home, J. D. Jost, J. M. Amini,
D. Leibfried, and D. J. Wineland, Nat. Phys. 6, 13
(2010).
[20] T. Monz, D. Nigg, E. A. Martinez, M. F. Brandl,
P. Schindler, R. Rines, S. X. Wang, I. L. Chuang, and
R. Blatt, Science 351, 1068 (2016).
[21] C. Piltz, T. Sriarunothai, S. S. Ivanov, S. Wo?lk, and
C. Wunderlich, Sci. Adv. 2, e1600093 (2016).
[22] S. Debnath, N. M. Linke, C. Figgatt, K. A. Landsman,
K. Wright, and C. Monroe, Nature 536, 63 (2016).
[23] D. Kielpinski, C. Monroe, and D. J. Wineland, Nature
417, 709 (2002).
[24] C. Monroe, R. Raussendorf, A. Ruthven, K. R. Brown,
P. Maunz, L.-M. Duan, and J. Kim, Phys. Rev. A 89,
022317 (2014).
[25] B. Lekitsch, S. Weidt, A. G. Fowler, K. Mølmer, S. J.
Devitt, C. Wunderlich, and W. K. Hensinger, Sci. Adv.
3, e1601540 (2017).
8
[26] J. Mautner, A. Makmal, D. Manzano, M. Tiersch, and
H. J. Briegel, New Generation Computing 33, 69 (2015).
[27] A. A. Melnikov, A. Makmal, and H. J. Briegel, “Pro-
jective simulation applied to the grid-world and the
mountain-car problem,” (2014), arXiv:1405.5459.
[28] A. Makmal, A. A. Melnikov, V. Dunjko, and H. J.
Briegel, IEEE Access 4, 2110 (2016).
[29] S. Hangl, E. Ugur, S. Szedmak, and J. Piater, in Proceed-
ings 2016 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS) (IEEE, 2016) p. 2799.
[30] M. Tiersch, E. J. Ganahl, and H. J. Briegel, Sci. Rep. 5,
12874 (2015).
[31] A. A. Melnikov, H. Poulsen Nautrup, M. Krenn, V. Dun-
jko, M. Tiersch, A. Zeilinger, and H. J. Briegel, “Active
learning machine learns to create new quantum experi-
ments,” (2017), arXiv: 1706.00868.
[32] M. Szegedy, in 45th Annual IEEE Symposium on Foun-
dations of Computer Science (IEEE, 2004).
[33] F. Magniez, A. Nayak, J. Roland, and M. Santha, SIAM
Journal on Computing 40, 142 (2011).
[34] V. Dunjko, N. Friis, and H. J. Briegel, New J. Phys. 17,
023006 (2015).
[35] N. Friis, V. Dunjko, W. Du?r, and H. J. Briegel, Phys.
Rev. A 89, 030303 (2014).
[36] N. Friis, A. A. Melnikov, G. Kirchmair, and H. J. Briegel,
Sci. Rep. 5, 18036 (2015).
[37] A. Khromova, C. Piltz, B. Scharfenberger, T. F. Gloger,
M. Johanning, A. F. Varo?n, and C. Wunderlich, Phys.
Rev. Lett. 108, 220502 (2012).
[38] C. Piltz, T. Sriarunothai, A. Varo?n, and C. Wunderlich,
Nat. Commun. 5, 4679 (2014).
[39] S. Wo?lk, C. Piltz, T. Sriarunothai, and C. Wunderlich,
J. Phys. B: At. Mol. Opt. Phys. 48, 075101 (2015).
[40] G. T. Genov, D. Schraft, N. V. Vitanov, and T. Half-
mann, Phys. Rev. Lett. 118, 133202 (2017).
[41] N. V. Vitanov, T. F. Gloger, P. Kaufmann, D. Kauf-
mann, T. Collath, M. Tanveer Baig, M. Johanning, and
C. Wunderlich, Phys. Rev. A 91, 033406 (2015).
SUPPLEMENTARY MATERIAL
Materials and Methods
We discuss some details of the experimental imple-
mentation. The detailed pulse sequence for the reported
experiments is shown in Fig. 5. Radio frequency (RF)
pulses tuned to the respective qubit resonance frequencies
near 12.6 GHz are applied for all coherent manipulations.
The RF power is carefully adjusted for each ion in order
to achieve an equal Rabi frequency of 20.92(3) kHz. For
each single-qubit rotation, Rj(?, ?) during the prepara-
tion and diffusion steps, ?1 and ?2 represent the rotation
angle according to Eq. (6 - 8)(main text).
We replace Rz(
?
2 ) by R(
?
2 ,
?
2 )R(
?
2 , 0)R(
?
2 ,
3?
2 ) and
Rz(??2 ) by R(
?
2 ,
?
2 )R(
?
2 , ?)R(
?
2 ,
3?
2 ).
During the conditional evolution time, a set of ten
UR14 RF ?-pulses (equaling a total of 140 pulses) is
applied [40] to protect the qubits from decoherence by
dynamical decoupling (DD). Each set is comprised of 14
error correcting pulses (Fig. 5) with appropriately chosen
phase ?:(
0,
6?
7
,
4?
7
,
8?
7
,
4?
7
,
6?
7
, 0, 0,
6?
7
,
4?
7
,
8?
7
,
4?
7
,
6?
7
, 0
)
.
Since the phases of the ?-pulses are symmetrically ar-
ranged in time, only the first seven pulses are shown in
Fig. 5. The last pulse is also shown to visualize the spac-
ing of these pulses with respect to the start and end of
evolution time, compared to the intermediate pulses. The
maximum interaction time of 30 ms required to realize
the deliberation algorithm (corresponding to 7 diffusion
steps) presented in the main text is 60 times longer than
the qubit coherence time. Such a long coherent interac-
tion time is accomplished by the DD pulses applied to
each qubit simultaneously.
Laser light near 369 nm is applied to the ions for cool-
ing, state preparation, and state selective read-out as
described in the main text. The durations are: 30 ms
for Doppler cooling, 100 ms for sideband cooling on the
center-of-mass mode, 100 ms for sideband cooling on the
stretch mode, 0.25 ms for initialization in state |0? of the
ions, and 2 ms for detection.
Projective measurements on both qubits are performed
to determine the states |00?, |01?, |10?, and |11?. Two
thresholds are used to distinguish between dark and
bright states of the ions, thus discarding 10% of all mea-
surements as ambiguous events with a photon count that
lies in the region of two partially overlapping Poissonian
distributions representing the dark and bright states of
the ions [39, 41].
Error discussion
In this section, we discuss deviations of the experi-
mental data from idealized theory predictions. In par-
ticular, for the chosen values of  and the corresponding
optimal k(), it is expected that the probability of ob-
taining a flagged action is close to 100%. However, the
success probability in our experiment lies between 66%
(for k = 7) and 88% (for k = 1). In what follows, we
discuss several reasons for this.
Scaling error
Even in an ideal scenario without noise or experimen-
tal imperfections the success probability ??, as defined in
Eq.(4) of the main text, after k diffusion steps is usually
not equal to unity, and depends on the specific value of
. This behavior originates from the step-wise increase of
the number of diffusion steps k = round( ?
4
?

? 12 ) in the
algorithm. The success probability is hence only 100%
if k is an integer without rounding. The change of the
ideal success probability with deviations of  from such
specific values is largest for small numbers of diffusion
steps (e.g., k = 1) and can drop down to 82% (neglecting
9
FIG. 5. Experimental sequence for Q-RPS. RF1 and RF2 each indicate a time axis for a qubit. The qubits are prepared
in the desired input states using single-qubit rotations implemented by applying RF pulses. For each RF pulse, the two
parameters within the parentheses represent the specific rotation angle and phase according to Eq. (6 - 8) in the main text.
Also, dynamical decoupling (DD) during conditional evolution, (Uzz(?/2)) (indicated by a green box) is implemented using RF
pulses (indicated in yellow). Ten sets of 14 pulses each (UR14 [40] ) are applied during the evolution time ? = 4.24 ms with a
J-coupling between the two ions of 2? × 59 Hz. The diffusion step is repeated k times according to Eq. (2) in the main text.
Laser light near 369 nm is used for cooling and to initialize the ions in the qubit state |0? ? |2S1/2 , F = 0 ?. At the end of the
coherent manipulation, laser light is used again for state selective detection and also for Doppler cooling.
the cases where it is not advantageous to use a quantum
algorithm at all). For larger numbers of diffusion steps,
the exact value of  does not play an important role any
more for the ideal success probability provided that the
correct number of diffusion steps is chosen. For example,
for k = 6, the ideal success probability is larger than 98%
independently of the exact value of . Throughout this
paper, we have chosen  in such a way, that ( ?
4
?

? 12 )
is always close to an integer (see Tab. I), such that the
deviation from a 100% success probability due to the the-
oretically chosen  is negligible compared to other error
sources.
However, in a real experiment, the initial state, and
therefore , can only be prepared with a certain accuracy.
This can lead to an inaccurate estimation of the optimal
number of diffusion steps. As opposed to the ideal case,
an assumed accuracy of ± 1% for the preparation only
has a small effect on the success probability ? (drop of less
than 5%) for  0.01, corresponding to k ? 3. However,
when ? does not fulfil the aforementioned condition and
approaches ? 0.01 from above, corresponding to k = 6,
then the success probability drops down to ? = 70% due
to a non-optimal choice of k.
The preparation accuracy depends on the detuning ??
of the RF pulses for single-qubit rotations as well as on
the uncertainty ?? in the determination of the Rabi fre-
quency ?. The calibration of our experiment revealed
??/? < 0.05 and ??/? = 0.0015 leading to an error in
 of ±2.5·10?3 and a decrease of the success probability ?
of less than 0.04. The detuning ?? and the uncertainty
of the Rabi frequency ?? not only influence the state
preparation at the beginning of the quantum algorithm,
but also its fidelity, as is detailed in the next paragraph.
To prevent decoherence during conditional evolution,
we use 140 MW ?-pulses per diffusion step and ion.
Therefore, already a small detuning influences the fidelity
of the algorithm. Consequently, the error induced by de-
tuning is identified as the main error source leading, for
example, to ?? ? 0.70 for k = 6 and ??/? = 0.03. This
error is much larger than the error caused by dephasing
(that is still present after DD is applied), or the detection
error. To estimate the error by dephasing, we assume an
exponential decay with ?? ? 1/14 for a single diffusion
step of duration ? ? 4 ms which would lead to ?? ? 0.90
10
TABLE I. Experimentally realized success probabilities. Initial theoretical probabilities, , of finding a flagged action
within the stationary distribution for various diffusion steps are shown. Success probabilities (?), that are theoretically calculated
and experimentally measured, for diffusion steps k = 1-7 are also shown.
Theory Theory
k a00 a01  b00 b01 ?
1 0.1371 0.1371 0.2742 0.4966 0.4966 0.9932
2 0.0493 0.0493 0.0987 0.4996 0.4996 0.9993
3 0.0252 0.0252 0.0504 0.4999 0.4999 0.9998
4 0.0152 0.0152 0.0305 0.5000 0.5000 1.0000
5 0.0102 0.0102 0.0204 0.5000 0.5000 1.0000
6 0.0073 0.0073 0.0146 0.5000 0.5000 1.0000
7 0.0055 0.0055 0.0110 0.5000 0.5000 1.0000
Experiment
k b00 b01 ?
1 0.449(15) 0.440(15) 0.89(2)
2 0.347(15) 0.353(15) 0.70(2)
3 0.438(16) 0.334(15) 0.77(2)
4 0.422(15) 0.336(15) 0.76(2)
5 0.407(17) 0.331(16) 0.74(2)
6 0.431(17) 0.324(16) 0.76(2)
7 0.365(15) 0.299(14) 0.66(2)
0 0.05 0.1 0.15 0.2 0.25 0.3
0
5
10
15
20
25
FIG. 6. Detuning errors. The influence of the detuning of
the RF pulses on the fidelity of the Q-RPS algorithm is shown
for three different values of the relative detuning ??/?. Black
lines indicate the results of numerical simulations of the com-
plete Q-RPS algorithm taking different values of the relative
detuning ??/? into account. Most of the experimental data
(red circles) lie close to the ??/? = 0.03 line.
for k = 6. Here, ? indicates the experimentally diagnosed
rate of dephasing, and ? is the time of coherent evolu-
tion. The influence of the detuning on the cost of our al-
gorithm is shown in Fig. 6 for different detunings. Here,
we simulated the complete quantum algorithm including
the experimentally determined dephasing and detection
errors for ??/? ? {0.01, 0.03, 0.05}. The experimental
data is consistent with an average relative detuning of
??/? = 0.03. Note that the detuning not only influ-
ences the single-qubit rotations that are an integral part
of the quantum algorithm, but also leads to errors dur-
ing the conditional evolution when dynamical decoupling
pulses are applied.
Ratio error
In the ideal algorithm, the output ratio rf = b00/b01
of the two flagged actions represented by the states |00?
and |01? at the end of the algorithm equals the input
ratio ri. However, in the experiment we have observed
deviations from rf/ri = 1. During the measurements
for the investigation of the scaling behavior (Fig. 3 in
the main text), we fixed ri = 1. The observed output
ratios are varying by 0.98 ? rf/ri ? 1.33. That is, the
probability b00 to obtain the state |00? is increased w.r.t.
b01. Also during the measurement testing the output
ratio, we observe that the output ratios are larger than
the input ratios.
An asymmetric detection error could be the cause for
this observation. Typical errors in our experiment are
given by the probability to detect a bright ion (|1?) with
a probability of dB = 0.06 as dark, and a dark ion (|0?)
with a probability of dD = 0.02 as bright. In Fig. 7 we
compare the measured output ratios with the calculated
output ratios assuming the above mentioned detection
errors only.
Fig. 7 shows that the experimental data, both for one
step and for three steps, are well approximated by the
11
TABLE II. Input and output distributions. Input and output ratios, ri and rf respectively, of the two flagged actions
represented by the states |00? and |01? for diffusion steps k = 1 and k = 3 are shown.
Theory Experiment
k a00 a01 ri b00 b01 rf
1 0.00271 0.27144 0.01 0.061(7) 0.809(12) 0.075(9)
1 0.07257 0.20159 0.36 0.290(14) 0.583(15) 0.50(3)
1 0.11383 0.16032 0.71 0.415(15) 0.466(15) 0.89(4)
1 0.14107 0.13309 1.06 0.488(15) 0.389(15) 1.25(6)
1 0.16040 0.11376 1.41 0.519(13) 0.351(12) 1.48(6)
1 0.17482 0.09933 1.76 0.566(15) 0.305(14) 1.85(10)
1 0.13708 0.13708 1.00 0.468(16) 0.401(16) 1.17(6)
3 0.00458 0.04578 0.10 0.127(10) 0.718(14) 0.176(14)
3 0.01633 0.03402 0.48 0.301(15) 0.518(16) 0.58(3)
3 0.02328 0.02707 0.86 0.442(16) 0.451(16) 0.98(5)
3 0.02788 0.02248 1.24 0.510(16) 0.354(15) 1.44(8)
3 0.03114 0.01922 1.62 0.551(16) 0.305(14) 1.81(10)
3 0.03357 0.01679 2.00 0.586(15) 0.268(13) 2.19(12)
0 0.5 1 1.5 2
0
0.5
1
1.5
2
FIG. 7. Ratio errors. The measured values (red and blue
circles) of the input and output ratios are compared to a sim-
ulation (solid black line) of the Q-RPS algorithm taking into
account the experimentally determined detection error. The
solid line corresponds to an output ratio expected taking into
account an unbalanced detection error where dB = 0.06 for
bright ions and dD = 0.02 for dark ions.
simulation when the experimentally determined detec-
tion error is taken into account. Thus, the deviation of
the measured ratios from the ideal result can be traced
back mainly to the unbalanced detection error. In ad-
dition, also errors in the preparation of the input states
play a role, especially when preparing very large or very
small ratios leading to either a00 or a01 being close the the
preparation accuracy of ? 2.5 · 10?3. At the same time,
the detuning plays a less prominent role in these mea-
surements because fewer dynamical decoupling pulses
where required due to the small number of diffusion steps.
Moreover, the detuning during these measurements could
be kept below ??/? = 0.03 leading to an average suc-
cess probability of ? = 0.85% also for k = 3 diffusion
steps compared to ? = 0.77% for k = 3 during the mea-
surements investigating the scaling (see Tab. I).
