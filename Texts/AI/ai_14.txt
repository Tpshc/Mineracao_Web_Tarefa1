ar
X
iv
:1
70
9.
01
12
2v
1 
 [
cs
.A
I]
  4
 S
ep
 2
01
7
An original version of this paper appeared in UAI-17
Exact Inference for Relational Graphical Models with Interpreted Functions:
Lifted Probabilistic Inference Modulo Theories
Rodrigo de Salvo Braz Ciaran O’Reilly
Artificial Intelligence Center
SRI International
Menlo Park, CA USA
Abstract
Probabilistic Inference Modulo Theories (PIMT)
is a recent framework that expands exact infer-
ence on graphical models to use richer languages
that include arithmetic, equalities, and inequali-
ties on both integers and real numbers. In this pa-
per, we expand PIMT to a lifted version that also
processes random functions and relations. This
enhancement is achieved by adapting Inversion,
a method from Lifted First-Order Probabilistic
Inference literature, to also be modulo theories.
This results in the first algorithm for exact proba-
bilistic inference that efficiently and simultane-
ously exploits random relations and functions,
arithmetic, equalities and inequalities.
1 INTRODUCTION
Graphical models such as Bayesian networks and Markov
networks (Pearl, 1988) are a well-principled way of rep-
resenting probabilistic models. They represent the de-
pendences between random variables in a factorized form
that can be exploited by probabilistic inference algorithms
(such as Variable Elimination (Zhang and Poole, 1994) and
Belief Propagation (Pearl, 1988)) for greater efficiency.
However, traditional graphical models representations are
unable to represent other types of useful structures:
• Relational structure occurs when families of random
variables share the same dependences. Suppose a
model involves random variables sunny, happy(Ann),
happy(Bob), . . . and the analogous dependences
P (happy(Ann)|sunny), P (happy(Bob)|sunny) and
so on. Intuitively, this structure can be exploited for
greater efficiency since the same inference can often
be performed only once for an entire family of random
variables. Traditional graphical model representa-
tions cannot explicitly indicate this structure and thus
algorithms cannot directly exploit it.
• Algebraic structure occurs when dependences be-
tween random variables (such as conditional proba-
bilities) can be compactly described by an algebraic
expression. For example, it may be that P (x ?
{1, . . . , 1000}|y ? {1, . . . , 1000}) is equal to ifx =
y then0.8 else0.2/999. Intuitively, this can be ex-
ploited for greater efficiency because large groups of
values can be treated in the same way (in this case, all
pairs of values (x, y) for which x 6= y are equally prob-
able). Traditional graphical model representations, how-
ever, represent such functions as a lookup table (a large
1000 × 1000 one for this example)1, thus not explicitly
representing the algebraic structure, which prevents al-
gorithms from directly exploiting it.
These types of structure are commonly found in real-world
applications and fields such as probabilistic programming,
so research has been conducted on exploiting both:
• Lifted probabilistic inference (Poole, 2003;
de Salvo Braz, 2007; Milch et al., 2008;
Van den Broeck et al., 2011; Kersting, 2012) ex-
ploits relational structure that is explicitly represented
with richer languages generally known as Probabilistic
Relational Models. These representations resemble
universal quantification in first-order logic such as
?x ? People P (happy(x)|sunny) for People a
discrete set {Ann,Bob, . . . }. In logic terminology, this
employs uninterpreted functions, since the random
relations are Boolean functions without a fixed inter-
pretation (for example, the value of happy(Ann) is not
fixed in advance).
• Probabilistic Inference Modulo Theories (PIMT)
(de Salvo Braz et al., 2016) exploits algebraic struc-
ture by explicitly representing and manipulating func-
tion definitions of random variable dependences in the
form of algebraic expressions. In logic terminology, this
employs interpreted functions, because functions like
1But note exceptions in Boutilier et al. (1996);
Sanner and Abbasnejad (2012).
equality (=) and arithmetic functions (+, × etc), among
others, have fixed interpretations: 3 = 3 is always TRUE,
and 1 + 3 is always 4.
In this paper, we present the first lifted probabilistic al-
gorithm on languages with interpreted functions. This
is done by unifying these two lines of research by in-
corporating uninterpreted functions and an important lifted
probabilistic inference operation, Inversion, into PIMT.
(Another major lifted operation, Counting (de Salvo Braz,
2007; Milch et al., 2008), is left for future work.) We call
this fusion of lifted inference and PIMT Lifted Probabilis-
tic Inference Modulo Theories (LPIMT).
Moreover, we achieve this unification modularly, by using
PIMT’s ability to be extended by solvers for specific theo-
ries, and by encapsulating lifted inference into it as an un-
interpreted functions theory solver. This means the algo-
rithm can apply lifted inference even to unanticipated theo-
ries, because solvers for different theories are orthogonally
applied by the general, theory-agnostic level of PIMT.
Casting lifted inference in the PIMT framework also makes
it simpler and more powerful than previous lifted infer-
ence methods. Besides the above mentioned advantages,
it also uses standard mathematical notation (e.g., not mak-
ing a hard distinction between “logical” and “random” vari-
ables); does not separate constraints from potential func-
tion definitions; and accepts random function applications
even as arguments to other functions (nested applications).
2 BACKGROUND
2.1 Graphical Models and Variable Elimination
Graphical models are a standard framework for reasoning
with uncertainty. The most common types are Bayesian
networks and Markov networks. In both cases, a joint
probability distribution for each assignment tuple x to N
random variables is defined as a normalized product of non-
negative real functions {?i}i?1..M , where 1..M is short for
{1, . . . ,M}, each of them applied to a subtuple xi of x:2
P (x) =
1
Z
?
i
?i(xi),
where Z is a normalization constant equal to
?
x
?
i ?i(xi). Functions ?i are called factors and
map each assignment on their arguments to a potential,
a non-negative real number that represents how likely the
assignment xi is. This representation is called factorized
due to its breaking the joint probability into this product.
In Bayesian networks, M = N and factors are conditional
probabilities P (xi|Pai), for each random variable xi in x,
where Pai are its parents in a directed acyclic graph.
2For simplicity, we use the same symbols for both random
variables and their values, but the meaning should be clear.
The marginal probability (MAR) problem consists of
computing P (q) =
?
r P (x), where q is a subtuple
of x containing queried variables, and r are all the re-
maining variables in x. It can be shown that P (q) =
1
Zq
?
r
?
i ?i(xi) for Zq a normalization constant over q.
Therefore, the problem can be easily reduced to computing
a summation over products of factors, which the rest of
the paper focuses on.
The belief update (BEL) problem consists of computing
P (q|e), the posterior probability on variables q given as-
signment e to evidence variables (an observation). BEL can
be easily reduced to two instances of MAR, since Bayes’
theorem establishes that P (q|e) = P (q, e)/P (e), where
P (q, e) is the joint value of q and e.
Computing
?
r
?
i ?i(xi) is crucial for inference with
graphical models, but solving it naively has exponential
cost in the size of r. Variable Elimination (VE) is an algo-
rithm that takes advantage of the factorized representation
to more efficiently compute this sum. For example, con-
sider the following summation given a factorization on the
variables (h)appy, (w)eekday, (t)emperature, (m)onth with
range sizes of 2 (true or false), 7 (weekdays), 3 (hot, mild,
cold), and 12 (months) respectively:
?
h,w,t,m
P (h|w, t)P (w)P (t|m)P (m).
A naive computation iterates over 504 assignments to
(h,w, t,m). However, the joint probability factorization
enables us to manipulate the expression and compute it
with iterations over fewer assignments:
?
h,w,t,m
P (h|w, t)P (w)P (t|m)P (m)
=
?
h,w,t
P (h|w, t)P (w)
?
m
P (t|m)P (m).
Now, we sum m out (or eliminate it), obtaining a new
factor ?, defined on t alone, that replaces the last summa-
tion. This requires going over each value t and computing
?(t) =
?
m P (t|m)P (m) (thus iterating over the values
of m). Therefore, computing ? takes iterating over 36 as-
signments to t,m and we are left with the following new
summation of a product of functions:
?
h,w,t
P (h|w, t)P (w)?(t)
=
?
w
P (w)
?
h,t
P (h|w, t)?(t)
=
?
w
P (w)??(w) (after 42 assignments to w, h, t)
= ??? (after 7 assignments to w).
Variable Elimination therefore decreases the number of re-
quired iterations to 85, a six-fold reduction, by exploiting
the fact that not all variables share factors with all variables
(for example, m only shares a factor with t).
VE applies not only to summations of products, but to any
commutative associative semiring (Bistarelli et al., 1997):
maximization of products, disjunctions of conjunctions,
conjunctions of disjunctions, and so on. We use ? and
? for the additive and multiplicative operators of the
semiring. We call quantifiers the intensional versions of
operators: ? for ?, ? for ?,
?
and
?
for + (for discrete and
continuous indices respectively),
?
for ×, Max for max,
and so on. Quantifiers corresponding to ? and ? are de-
noted
?
and
?
, respectively. We use
?
for denoting any
type of quantifier. VE works by selecting the next variable
v in r to be eliminated and computing a new factor
?(u) =
?
v
?
i?f(v)
?i(. . . , v, . . . ),
where f(v) is the set of indices of factors of v and u are the
variables sharing factors with v. It then includes ? in the
product of factors and proceeds until all variables in r are
eliminated.
2.2 Probabilistic Inference Modulo Theories
Typically, graphical model inference algorithms assume
that factors can only be accessed as opaque lookup tables,
that is, by providing an assignment to its random variables.
This requires Variable Elimination to iterate over all assign-
ments to all random variables in a summation, as seen in the
example for computing ?(t) with Variable Elimination.
However, often the definitions of factors are available in the
form of symbolic algebraic expressions that use operators
from specific theories. For example, the conditional proba-
bility of temperature given month, and the prior probability
of month in our original example could be represented by3
P (t|m) = if m ? 3
then if t = cold then 0.8 else 0.1
else if t = cold then 0.4 else 0.3
P (m) = 1/12.
While this form does not preclude regular VE to access it
as a lookup table (by computing its result given an assign-
ment to t and m), the symbolic expression is itself available
as a data structure. This makes the structure of the factor
evident; for example, specific values of m do not matter,
but only whether m ? 3.
Probabilistic Inference Modulo Theories (PIMT), ex-
ploits this available symbolic information for much faster
inference. It does so by using Symbolic Evaluation Mod-
ulo Theories (SEMT), which is a method for simplifying
3Note that P (t|m) sums up to 1 for each m because hot and
mild both have the else case probability mass.
(and, when possible, completely evaluating) symbolic alge-
braic expressions, including eliminating quantifiers such as
?
,
?
, ?, and ?. SEMT applies to general expressions, and
not only probabilistic reasoning-related ones. PIMT is sim-
ply the use of SEMT applied to expressions that happen to
be (unnormalized) marginal probabilities, so we mostly re-
fer to SEMT from now on, keeping in mind that it includes
PIMT as a special case.
SEMT and PIMT are similar to Satisfiability Modulo The-
ories (SMT) solvers (Barrett et al., 2009; de Moura et al.,
2007), which also take modular theory-specific solvers
to solve multi-theory problems. However, they gener-
alize SMT in two important ways: they are quantifier-
parametric, that is, they eliminate multiple types of quan-
tifiers and deal with expressions of any type, including
Boolean, numeric and categorical, as opposed to SMT only
solving existential quantification on Boolean-valued for-
mulas; and they are symbolic, that is, can process free
variables and return results expressed in terms of them, as
opposed to assuming all variables to be existentially quan-
tified as in SMT.
SEMT is a variant of SGDPLL(T ) (de Salvo Braz et al.,
2016) slightly generalized to deal with any expression, and
not just sequences of nested quantifiers, and to make use
of contexts, defined below. We give an overview of SEMT
here but refer to the SGDPLL(T ) reference for details.
SEMT receives a pair (E,C) as input. E is the expression
being evaluated, and C the context. Let x be the free vari-
ables in E. Context C is a formula on x that indicates that
only the assignments on x satisfying C need be considered.
For example, the expression ifx 6= 1 then2 else4, un-
der context x = 2 ? x = 3, can be safely evaluated to 4
because x 6= 1 is never true under that context.
SEMT traverses the expression top-down and evaluates
each sub-expression according to a set of rewriting
rules (presented below) until no rule applies to any sub-
expressions. It also keeps track of the context holding for
sub-expression, depending on the expressions above it. For
example, in ifx = 1 then
?
i?1..10:i6=3 ? else?, ? is
evaluated under context x = 1 ? i 6= 3.
Let E[?/?] be the substitution of all occurrences of ex-
pression? in expressionE by ?. The SEMT rewriting rules
are the following (an example is provided afterwards):
• simplification: simplifiers for each theory are applied
when possible: examples are 1+2 ? 3, x = x ? true,
if true then1 else2 ? 1, 0×x ? 0 and so on. These
always decrease the size of the expression.
• literal determination: if the expression is a literal L un-
der a context C and ?V : C ? L, where V are the free
variables in C and L, rewrite the expression to TRUE; if
?V : C ? ¬L, rewrite it to FALSE; the tautology may
be decided by SEMT itself or an SMT solver;
• factoring out: if ?1 does not involve index i,
?
i?D:C(?1 ? ?2) ? ?1 ?
?
i?D:C ?2.
• if-splitting: if the expression is ? and contains a
literal L undefined by the context C, rewrite it to
ifL then?[L/TRUE] else?[L/FALSE].
• quantifier-splitting: rewrite expressions of the form
?
i?D:C ?, where i is a variable of type D, F is
a conjunction of literals in the theory for type D,
and ? is expression containing a literal L contain-
ing i, to a new expression containing two quantified
expressions, each with one less literal in its body:
(
?
i:F?L ?[L/TRUE]
)
?
(
?
i:F?¬L ?[L/FALSE]
)
.
• theory-specific quantifier elimination: if ? does not
contain any literals, solve
?
i?D:F ? with a provided,
modular theory-specific solver for the type of i.
SEMT always returns a quantifier-free expression since
the theory-specific quantifier elimination is eventually in-
voked for each quantifier. For an expression representing
a marginal over a product of factors, it reproduces Variable
Elimination by summing out one variable at a time, with the
advantage of exploiting factors represented symbolically.4
Consider the computation of ?(t) =
?
m P (t|m)P (m) be-
low. Regular VE requires iterating over 36 assignments to
t,m, but SEMT only needs to consider the 4 truth assign-
ments for literals t = cold and m ? 3:
?
m
(if m ? 3 then if t = cold then 0.8 else 0.1
else if t = cold then 0.4 else 0.3)/12
? (by if-splitting on t = cold and simplification)
if t = cold
then
?
m
(if m ? 3 then 0.8 else 0.4)/12
else
?
m
(if m ? 3 then 0.1 else 0.3)/12
? (by quantifier-splitting on m ? 3 in first summation)
if t = cold
then
?
m:m?3
(if TRUE then 0.8 else 0.4)/12
+
?
m:¬(m?3)
(if FALSE then 0.8 else 0.4)/12
else
?
m
(if m ? 3 then 0.1 else 0.3)/12
4The variable order used by VE is encoded in the order of
quantifiers, and efficient ordering can be introduced by a rule that
re-orders sequences of quantifiers.
? (by simplification and quantifier-splitting on m ? 3)
if t = cold then 0.8/12× 3 + 0.4/12× 9
else 0.1/12× 3 + 0.3/12× 9
? if t = cold then 0.5 else 0.25,
which represents the resulting new factor ?(t) (which hap-
pens to be P (t)) to be used in the next steps of VE. Note
that the expressions above are not just a mathematical argu-
ment, but the actual data structures manipulated by SEMT.
SEMT (and consequently PIMT) is modulo theories be-
cause symbolic evaluation is theory-agnostic, working for
any theory T given an encapsulated theory solver that
provides quantifier elimination solvers and simplification
rules for T . Theory solvers must be provided only for
the simpler expressions of the type
?
i?D:F ? where ? is
literal-free and F is a conjunction of literals in the theory.
de Salvo Braz et al. (2016) details a solver for summation
over polynomials, with literals in difference arithmetic on
bounded integers. A similar solver for integrals over poly-
nomials and linear real arithmetic literals has also been de-
fined since. An example of SEMT on multiple theories with
only 4 cases is:
?
i?1..7
?
x?[0;10]
if i ? 3 thenifx < 5 thenx2 else0 else i
?
?
i?1..7
if i ? 3 then
?
x?[0;10]
ifx < 5 thenx2 else0
else
?
x?[0;10]
i
?
?
i?1..7
if i ? 3 then
?
x?[0;10]:x<5
x2 +
?
x?[0;10]:x?5
0
else 10i
?
?
i?1..7
if i ? 3 then 125/3 else 10i
?
(
?
i?1..7:i?3
125/3
)
+
(
?
i?1..7:i<3
10i
)
? 625/3 + 30 ? 715/3.
2.3 Relational Graphical Models
Relational graphical models (RGM) specify probability
distributions over families of random variables that can
be seen as relations. There are many RGM languages
in the literature; they vary significantly in syntax and su-
perficial features, but are typically equivalent and con-
vertible to one another. Markov Logic Networks (MLN)
(Richardson and Domingos, 2004) is a well-known one. It
consists of a set of weight formulas, an example of which
is
2.5 : Smoker(Bob)
1.4 : Smoker(X) ? Friends(X,Y ) ? Smoker(Y )
for logical variables X and Y ranging over a finite set D =
{Ann,Bob, Charlie, . . .}.
The random variables in this MLN are the groundings,
or instantiations, of the relations in it for every assign-
ment to the logical variables: Sm(Ann), Sm(Bob),
. . . , Fr(Ann,Ann), Fr(Ann,Bob), . . . (we ab-
breviate Smoker and Friends from now on). A
formula with weight w defines a factor for each of its
instantiations (one for each assignments to its logical
variables). The potential assigned by such factors is
ew if the formula is true, and 1 otherwise. Therefore,
some of the factors of this MLN are: ?1(Sm(Bob)),
?2(Sm(Ann), F r(Ann,Bob), Sm(Bob)),
?2(Sm(Ann), F r(Ann,Charlie), Sm(Charlie)), and
?2(Sm(Bob), F r(Bob,Ann), Sm(Ann)), where ?1 and
?2 are potential functions applied to all factors instantiated
from the first and second formulas respectively:
?1(s) = if s then e
2.5
else1
?2(s1, f, s2) = if s1 ? f ? s2 then e
1.4
else 1.
Because the number of instantiations can be huge, perform-
ing inference on RGMs by simply instantiating them as reg-
ular graphical models is often infeasible. The next section
describes Inversion, one of the operations used in lifted
probabilistic inference, which exponentially improves ef-
ficiency in RGMs in many cases.
3 INVERSION AND INVERSION
MODULO THEORIES
This section presents this paper’s main contributions. We
present a new formulation for RGMs and Inversion that is
more algebraically standard, and then use this formulation
to generalize Inversion to Inversion Modulo Theories.
3.1 RGMS with Function-Valued Random Variables
While the RGM literature considers Sm(Ann),
Sm(Bob), . . . as individual (Boolean) random vari-
ables, it is conceptually simpler, and more mathematically
standard, to talk about Sm as a single random variable that
happens to be function-valued. From this point of view,
this MLN has only two random variables: Sm and Fr,
and defines the following joint probability distribution:
P (Sm,Fr) =
1
Z
× ?1(Sm(Bob))
×
?
X?D
?
Y ?D
?2(Sm(X), F r(X,Y ), Sm(Y ))
where Z is defined as usual, as well as marginalization:
P (Sm) =
?
Fr?D×D?Boolean
P (Sm,Fr).
Note that, given the form of P (Sm,Fr), marginal proba-
bilities in RGMs are sums of products of factors, includ-
ing intensional products using
?
. This fact is heavily ex-
ploited by Inversion.
To compute the marginal of a specific function applica-
tion, say, Sm(Ann), we need to marginalize over all re-
maining random variables, that is, Fr and all Sm(x) for
x ? D \ {Ann}. This requires splitting the variable Sm
into two distinct function-valued random variables: Sm? :
{Ann} ? Boolean and Sm?? : D \ {Ann} ? Boolean,
and replace each application Sm(?) of the original Sm on
argument expression ? by
if ? = Ann thenSm?(Ann) elseSm??(?). Then we can
compute
P (Sm?) =
?
Sm??:D\{Ann}?Boolean
?
Fr
P (Sm?, Sm??, F r).
The above shows that solving RGMs is simply equivalent
to allowing function-valued random variables in the model.
However, summations over functions are expensive due to
their high number of possible values (2|D|
2
for Fr, for in-
stance). The next section describes a method that exponen-
tially decreases this cost in some cases.
3.2 Inversion on Function-Valued Variables
Lifted probabilistic inference algorithms seek to exploit
the structure of random functions for greater efficiency. It
includes a few operations, but in this paper we consider
only one: Inversion (also called Lifted Decomposition).
Inversion uses the fact that summations indexed by func-
tions of products of factors may under certain conditions be
transformed into exponentially cheaper summations over
“slices” of the original function. Its name comes from the
fact that a summation of products becomes a cheaper prod-
uct of a summation (
??
?
??
), thus “inverting” the
quantifiers. Consider an example in which the body ? of
the sum-product depends on a single application of a func-
tion f ranging over the set of functions 1..10 ? 1..5:
?
f?(1..10?1..5)
?
x?1..10
?(f(x)) =
?
x?1..10
?
f?({x}?1..5)
?(f(x)).
Note that, while f ranges over (1..10 ? 1..5) on the left-
hand side, and thus over 510 possible values, the domain
of f on the right-hand side is a singleton set, because x
is bound by the now outer
?
x. This reduces the number
of possible values of f to only 5, making iterating over it
exponentially cheaper.
The equality holds because x “slices” f into independent
portions:
?
f?(1..10?1..5)
?
x?1..10
?(f(x))
=
?
f1?{1}?1..5
. . .
?
f10?{10}?1..5
?(f1(1))× ...× ?(f10(10))
=
?
f1?{1}?1..5
?(f1(1)) . . .
?
f10?{10}?1..5
?(f10(10)) (*)
=
(
?
f1?{1}?1..5
?(f1(1))
)
. . .
(
?
f10?{10}?1..5
?(f10(10))
)
=
(
?
f?{1}?1..5
?(f(1))
)
. . .
(
?
f?{10}?1..5
?(f(10))
)
=
?
x?1..10
?
f?({x}?1..5)
?(f(x)).
Once transformed in this way, we proceed as follows:
?
x?1..10
?(1) + ?(2) + ?(3) + ?(4) + ?(5)
=
?
x?1..10
?? = (??)10 = ???
by using the fact that constant ?? does not depend on x.
The above transformation is valid because
?
x ?(f(x))
contains 10 factors, each of them only involving the ap-
plication of f to a single value of x. This means they can
be factored out of the summations indexed by other appli-
cations of the function, resulting in smaller and equivalent
summations that are computed only once and then expo-
nentiated. Similar transformations may be applied even if
there are products over more than one variable:
?
f?A1×A2?B
?
x
?
y
?(f(x, y)) =
?
x
?
y
?
f?{(x,y)}?B
?(f(x, y))
by an analogous argument.
However, the transformation is not always valid:
?
f?A1×A2?B
?
x
?
y
?(f(x, y), f(y, x))
6=
?
x
?
y
?
f?{(x,y)}?B
?(f(x, y), f(y, x)).
To see this, consider a pair (a, b) ? A1 × A2.
The summation
?
f?{(a,b)}?B ?(f(a, b), f(b, a)) de-
pends on f(a, b) and f(b, a), which both occur in
?
f?{(b,a)}?B ?(f(b, a), f(a, b)), and this shared depen-
dence prevents factoring as in step (*) above.
This does not mean that having more than one application
of f in ? admits no Inversion, but it may restrict the inver-
sion to just some of the products:
?
f?((A1×A2×A3)?B)
?
x
?
y
?(f(x, y, x), f(x, 3, x)) (1)
=
?
x
?
f?(({x}×A2×{x})?B)
?
y
?(f(x, y, x), f(x, 3, x)),
which does not decrease the function’s domain size to 1 but
still exponentially decreases the evaluation cost.
All these cases are covered by the following theorem:
Theorem 3.1 (Inversion). Let E be an expression in which
all applications of a function f have their first k argu-
ments (without loss of generality because they can be per-
mutated) equal to xi1 , . . . , xik for i a k-tuple of indices in
{1, . . . ,m}. If operator ? distributes over operator ?,
?
f?(A1×···×An)?B
?
x1
· · ·
?
xm
E
=
?
x1
· · ·
?
xm
?
f?({(xi1 ,...,xik )}×Ak+1×···×An)?B
E,
which is exponentially cheaper to evaluate.
Note that E may contain quantifiers itself; this was the case
in Equation (1). The theorem’s proof mirrors the operations
shown in the examples above.
Crucially, Theorem 3.1 uses a syntactic check that relies
on the language allowing only simple terms (variables and
constants) as arguments to functions. It therefore does not
apply to the important extensions in which more complex
terms, using theory-specific operators such as arithmetic or
inequalities, or even operators from unanticipated theories,
are used as function arguments. We will see how this can
be done with a semantic check in the next section.
3.3 Inversion Modulo Theories
We now present this paper’s main contribution: Inversion
Modulo Theories, which is a version of the lifted prob-
abilistic inference operation Inversion in the presence of
factors defined through symbolic algebraic expressions in-
volving multiple theories. Like regular Inversion, Inver-
sion Modulo Theories does not cover every possible prob-
lem, in which case one must fall back to currently used
methods like grounding or sampling. Future work includes
generalizing other lifted inference methods like Counting
(de Salvo Braz, 2007; Milch et al., 2008) to also be modulo
theories, thus decreasing the need for the fallback methods.
Previous work on Inversion assumed RGMs expressed in
a simple language in which the only allowed arguments
in function applications are variable or constant symbols.
This enables the Inversion condition to consist of a syn-
tactic check. However, once we allow function arguments
to be arbitrary terms from any of the available theories,
these syntactic tests no longer apply, since the arguments
semantics depends on the particular theory and the syntac-
tic check does not take semantics into account. The new In-
version test must apply to arguments from any theory and,
in fact, to arguments in even new, as of yet unanticipated
theories. This requires this new test to be theory-agnostic,
and defined in such a way that theory solvers may be trans-
parently employed.
We start by defining ocf [E], an expression representing the
portion of the domain of a given function f involved in a
given expression E.
Definition 3.1. The set of argument tuples for f : A ?
B occurring in an expression E is denoted ocf [E] and
inductively defined as follows:
• if E does not contain f , ocf [E] is ?;
• if E is f(t) for t a tuple, ocf [E] is {t};
• if E is f , ocf [E] is A;
• if E is ifC thenE1 elseE2 and C does not contain
f , then ocf [E] is ifC then ocf [E1] else ocf [E2];
otherwise, ocf [E] is ocf [C] ? ocf [E1] ? ocf [E2];
• if E is g(t1, . . . , tk) for g a function symbol distinct
from f , or E is {t1, . . . , tk}, ocf [E] is
ocf [t1] ? · · · ? ocf [tk];
• if E is Qx?T :CE? for Q an arbitrary quantifier,
– if C does not contain f ,
then ocf [E] is ocf [T ] ?
?
x?T :C ocf [E
?];
– otherwise, ocf [E] is
ocf [T ] ?
?
x?T (ocf [C] ? ocf [E
?]).
For example,
ocf
[
f(x, y) +
?
z?1..10:z!=3
f(x, z)
?
w?1..10
f(w, z)
]
= {(x, y)} ?
?
z?1..10:z!=3
(
{(x, z)} ?
?
w?1..10
{(w, z)}
)
,
in which x and y are free variables.
As a further preliminary step, we add simplification rules
for tuples, and for testing whether sets are empty, to be
used by SEMT. This will be useful later in manipulating
ocf [E] expressions.
Theorem 3.2 (Tuple and Empty Set Simplifiers). The fol-
lowing tuple and empty set simplifiers
1. (r1, . . . , rn) = (s1, . . . , sn) ?
r1 = s1 ? · · · ? rm = sm (or its negation for 6=).
2. t ? {t1, . . . , tn} ? (t = t1) ? t ? {t2, . . . , tn}.
3. t ?
?
i?D:C ? ? ?i ? D : (C ? t ? ?).
4.
(
?
i?D:C ?
)
= ? ? ?i ? D : (¬C ? ? = ?).
5. S ? ? = ? ? TRUE.
6. S ? ? = ? ? S = ?.
7. S ? {t1, . . . , tn} = ? ?
(t1 /? S) ? (S ? {t2, . . . , tn} = ?).
8.
(
?
i?D:C ?
)
?
(
?
i??D?:C? ?
?
)
= ? ?
?i ? D : C ? ?i? ? D? : C? ? (? ? ?? = ?).
9. (S1 ? S2) ? S3 = ? ? (S1 ? S3) ? (S2 ? S3) = ?.
10. S1 ? S2 = ? ? S1 = ? ? S2 = ?.
11. {t1, . . . , tn} = ? ? FALSE if n > 0, TRUE otherwise,
when included in SEMT, rewrite ocf [E] = ? expressions to
equivalent formulas free of tuple and set expressions.
The proof is presented in Supplementary Materials. It is
based on gradual distribution of ? over ?, and conversion
of intersections to comparisons between set elements:
?
x?1..5
(
{(x, y)} ?
?
z?3..5
{(z, 3)}
)
?
?
w?1..10
{(1, w)} = ?
? (rule 8)
?x ? 1..5 : ?w ? 1..10 :
(
{(x, y)} ?
?
z?3..5
{(z, 3)}
)
? {(1, w)} = ?
? (rule 9 distributes ? over ?)
?x ? 1..5 : ?w ? 1..10 :
{(x, y)} ? {(1, w)} ?
(
?
z?3..5
{(z, 3)}
)
? {(1, w)} = ?
? (rule 10)
?x ? 1..5 : ?w ? 1..10 :
{(x, y)} ? {(1, w)} = ? ?
(
?
z?3..5
{(z, 3)}
)
? {(1, w)} = ?
? (rules 7, 2, and 8)
?x ? 1..5 : ?w ? 1..10 :
(x, y) 6= (1, w) ? ?z ? 3..5 : {(z, 3)} ? {(1, w)} = ?
? (rule 1 on first conjunct, rules 7 and 2 on second one)
?x ? 1..5 : ?w ? 1..10 :
(x 6= 1 ? y 6= w) ? ?z ? 3..5 : (z, 3) 6= (1, w)
? (rule 1 for breaking tuples)
?x ? 1..5 : ?w ? 1..10 :
(x 6= 1 ? y 6= w) ? ?z ? 3..5 : z 6= 1 ? 3 6= w
? (integer-specific solver for
?
= ? for z, w and x)
?x ? 1..5 : ?w ? 1..10 : (x 6= 1 ? y 6= w) ? 3 6= w
? ?x ? 1..5 : FALSE ? FALSE.
Deciding whether a set expression is equivalent to the
empty set is crucial for Inversion Modulo Theories, which
can now be formally stated:
Theorem 3.3 (Inversion Modulo Theories). Let E be an
expression and Ti, Ci be type and constraint, respectively,
in a theory for which we have a satisfiability solver. Then,
?
f?A?B
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
E,
where A = ocf [
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
E] (this is re-
laxed in Section 3.4), is equivalent to , and therefore can
be rewritten as,
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
?
f ? ocf [E]?B
E,
if
?
distributes over
?
and
?x?1 ? T1 . . . ?x
?
k ? Tk ?x
??
1 ? T1 . . . ?x
??
k ? Tk
?
?
C1[x1/x
?
1] ? · · · ? Ck[xk/x
?
k]
? C1[x1/x??1 ] ? · · · ? Ck[xk/x
??
k]
? (x?1, . . . , x
?
k) 6= (x
??
1 , . . . , x
??
k)
?
?
?
?
?
ocf [E][x1/x
?
1, . . . , xk/x
?
k]
?
ocf [E][x1/x
??
1 , . . . , xk/x
??
k ]
?
? = ? (2)
(that is, the set of argument tuples in applications of f in
E for different value assignments to x1, . . . , xk are always
disjoint — we refer to this as Condition (2) from now
on). Moreover, the rewritten expression is exponentially
(O(2
?
i
|{xi?Ti:Ci}|)) cheaper to evaluate than the original.
The theorem (proven in Supplementary Materials) covers
significantly more general cases than the original exam-
ples may suggest: f may have any arity; its arguments
need not be only variables or constants, but even other (in-
terpreted or uninterpreted) function applications; E may
contain quantifiers itself; and quantifiers may involve con-
straints (Ci). The following example involves all these
characteristics:
Example 3.4. Let w and g be free variables.
?
f?1..10×(1..10\{8})×{w+3}?1..5
?
x?(1+g(w))..(10+g(w))
?
y?1..10:y 6=8
?
z?1..10
f(x? g(w), y, w + 3)z
? (Inversion on x, y; see Eqn. (3) for Condition (2))
?
x?(1+g(w))..(10+g(w))
?
y?1..10:y 6=8
?
f?{(x?g(w),y,w+3)}?1..5
?
z?1..10
f(x? g(w), y, w + 3)z
? (f has a singleton domain since x, y, g and w are fixed
in the first summation, so it behaves like a variable v)
?
x?(1+g(w))..(10+g(w))
?
y?1..10:y 6=8
?
v?1..5
?
z?1..10
vz.
This transformation decreases the time complexity of the
first summation from the time to iterate over 510×9 values
of f to 5 values of v only. But, in fact, from now on SEMT
completes the calculation with no iteration at all by using
symbolic integer-specific solvers for
?
and
?
:
?
x?(1+g(w))..(10+g(w))
?
y?1..10:y 6=8
?
v?1..5
?
z?1..10
vz
?
?
x?(1+g(w))..(10+g(w))
?
y?1..10:y 6=8
?
v?1..5
55v
?
?
x?(1+g(w))..(10+g(w))
?
y?1..10:y 6=8
55×15? 82510×9 ? 82590.
In this example, Condition (2) is
?x? ? (1 + g(w))..(10 + g(w)) ?y? ? 1..10
?x?? ? (1 + g(w))..(10 + g(w)) ?y?? ? 1..10
(x?, y?) 6= (x??, y??) ? y? 6= 8 ? y?? 6= 8 ?
?
z?1..10
(x? ? g(w), y?, w + 3)
?
?
z?1..10
(x?? ? g(w), y??, w + 3) = ? (3)
which can be solved by SEMT with
?
instantiated as ?
over difference arithmetic, after including tuple and empty
set simplifiers from Definition 3.2).
Example 3.5. Consider monitoring crop growth from
satellite images to alert against famine. The growth (g) of
crops can be determined from their color (c) in the images,
and depends on whether the region was in drought (d) 3
months previously. This can be modeled as:
?m ? Months, f ? Fields :
P (c(f,m)|g(f,m)) = if g(f,m) > 2.3 then if c(f,m) . . .
P (g(f,m+ 3)|d(m)) = if d(m) then if g(f,m+ 3) . . .
The query P (d|c) requires solving the following marginal-
ization over growth:
?
g
?
m
?
f
P (c(f,m+3)|g(f,m+3))P (g(f,m+3)|d(m))
Inversion Modulo Theories applies (because each (f,m)
involves a single instance of g(f,m+ 3)) and we obtain
?
m
?
f
?
g(f,m+3)
P (c(f,m+3)|g(f,m+3))P (g(f,m+3)|d(m))
which makes the summation on growth exponentially
cheaper to compute and produces
?
m
?
f
?(c(f,m + 3), d(m)),
for ? the summation result. This can then be evaluated
directly, given the evidence on color for each m and f ,
producing a factorized representation of the marginal on
drought(m), for each month. If the number of fields is
2000, and growths domain size is 5, this cuts the cost of
exactly eliminating growth from 52000 iterations to only 5.
3.4 Dealing with Arbitrary Domains for f
Let OCf be ocf [
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
E]. Theorem
3.3 requires that A = OCf , that is, that the domain of
f coincide with its portion being used inside
?
f?A?B .
When A 6= OCf , we use function splitting (Section 3.1):
?
f?A?B
? =
?
f ??(A\OCf )?B
?
f ???(OCf\A)?B
?
f?(A?OCf)?B
??
for ?? obtained from ? after replacing each f(?) by
if ? ? A
then if? ? OCf then f(?) else f
?(?) else f ??(?).
This technique splits the original f into three different
functions f , f ?, f ??, and replaces each of its applications by
the one corresponding to its arguments. After this, the do-
main of f coincides with OCf , satisfying the correspond-
ing requirement in Theorem 3.3.
3.5 Dealing with Multiple Separate
?
Quantifiers
Theorem 3.3 requires a single nested sequence of
?
quan-
tifiers inside
?
. However, summations on products of sep-
arate
?
quantifiers can be rewritten to the required form:
?
f?A?B
(
(
?
x?Tx:Cx
Ex
)(
?
y?Ty :Cy
Ey
)
)
?
?
f?A?B
?
x?Tx:Cx
?
y?Ty :Cy
E
1
|{y?Ty :Cy}|
x E
1
|{x?Tx:Cx}|
y ,
where the exponents compensate for the extra multiplica-
tions of Ex and Ey by moving them inside
?
y and
?
x
respectively (this is sometimes called scaling). Set cardi-
nalities can be computed as a special case of summation.
In the special case in which x and y range over the same
values (that is, {x ? Tx : Cx} = {y ? Ty : Cy},
which can be evaluated by SEMT), the two
?
quanti-
fiers can be merged into a single one (or aligned), and
the original expression is rewritten instead to the cheaper
?
f?A?B
?
x?Tx:Cx
ExEy [y/x]. These transformations
can be easily generalized to cases with more than two
?
quantifications, and nested
?
expressions.
3.6 A Proof-of-concept Experiment
Since SEMT and Inversion Modulo Theories are elaborate
symbolic algorithms, two immediate questions are whether
they can be effectively implemented and how they compare
with simpler alternatives such as sampling. As a proof-
of-concept test, we used our implementation of SEMT for
computing Example 3.4 using two alternatives: one, in
which the sum-product is simplified by Inversion Mod-
ulo Theories, and another in which the sum and product
are computed by sampling. As expected, Inversion Mod-
ulo Theories vastly outperforms sampling in this example,
computing the exact answer (82590) in less than 300 ms,
whereas sampling requires 10 minutes, and 10,000 sam-
ples per quantifier, to be within an order of magnitude of
the exact answer, and 17 minutes to be within 10% error.
4 RELATED WORK AND CONCLUSION
As mentioned in the introduction, LPIMT generalizes work
in the lifted inference, probabilistic inference modulo theo-
ries, and satisfiability modulo theories literatures. It is also
related to Probabilistic Programs (PP) (Goodman et al.,
2012; Milch et al., 2005), a class of high-level repre-
sentations for probabilistic models that uses interpreted
and uninterpreted functions. The current prevalent infer-
ence method in PPs is sampling, which is approximate
and whose convergence rate depends on the size of the
grounded model, that is, on the size of the domain. LPIMT
can be applied to some fragments of PPs and is an exact
inference alternative that avoids iterating over the domain.
The closest approach to LPIMT in the PP area is Hakaru
(Carette and Shan, 2016; Narayanan et al., 2016), which
employs symbolic methods for simplification and integra-
tion of PPs, but does not include lifted inference on ran-
dom functions. Model Counting Modulo Theories (Phan,
2015) leverages SMT solvers to compute model counts,
but does not cover weighted model counting (and thus
probabilistic reasoning), and does not exploit factorization.
Weighted Model Counting and Integration (Michels et al.,
2015; Belle et al., 2015; Michels et al., 2016) and Sym-
bolic Variable Elimination (Sanner and Abbasnejad, 2012)
are similar in spirit to PIMT, but not to LPIMT; they ap-
ply to Boolean and linear real arithmetic random variables,
but not yet to random functions. (Belle, 2017) focuses
on weighted model counting (WMC) with function sym-
bols on infinite domains, reducing it to standard WMC, but
does not currently use lifting techniques. Group Inversion
(Taghipour et al., 2012) expands Inversion to cover some
extra cases, but not interpreted functions. Extending it to
do so, along with Counting, is the most immediate possi-
bility for future work.
To conclude, we have defined Inversion Modulo Theories,
an expansion of the lifted inference operation Inversion for
the case with interpreted functions, and added it to Proba-
bilistic Inference Modulo Theories framework, thus defin-
ing the first algorithm to perform exact lifted inference in
the presence of interpreted functions.
ACKNOWLEDGMENTS We gratefully acknowledge
the support of the Defense Advanced Research Projects
Agency (DARPA) Probabilistic Programming for Ad-
vanced Machine Learning Program under Air Force Re-
search Laboratory (AFRL) prime contract no. FA8750-14-
C-0005.
References
Barrett, C. W., Sebastiani, R., Seshia, S. A., and Tinelli,
C. Satisfiability Modulo Theories. In Biere, A., Heule,
M., van Maaren, H., and Walsh, T., editors, Handbook
of Satisfiability, volume 185 of Frontiers in Artificial In-
telligence and Applications, pages 825–885. IOS Press,
2009.
Belle, V., Passerini, A., and Van den Broeck, G. Proba-
bilistic inference in hybrid domains by weighted model
integration. In Proceedings of 24th International Joint
Conference on Artificial Intelligence (IJCAI), 2015.
Belle, V. Weighted model countingwith function symbols.
In Proceedings of the Conference on Uncertainty in Ar-
tificial Intelligence (UAI), 2017.
Bistarelli, S., Montanari, U., and Rossi, F. Semiring-
based constraint satisfaction and optimization. J. ACM,
44(2):201–236, March 1997.
Boutilier, C., Friedman, N., Goldszmidt, M., and Koller, D.
Context-Specific Independence in Bayesian Networks.
In Proceedings of UAI, pages 115–123, 1996.
Carette, J. and Shan, C. Simplifying probabilistic programs
using computer algebra. In Practical Aspects of Declar-
ative Languages - 18th International Symposium, PADL
2016, St. Petersburg, FL, USA, January 18-19, 2016.
Proceedings, pages 135–152, 2016.
de Moura, L., Dutertre, B., and Shankar, N. A tutorial
on satisfiability modulo theories. In Computer Aided
Verification, 19th International Conference, CAV 2007,
Berlin, Germany, July 3-7, 2007, Proceedings, volume
4590 of Lecture Notes in Computer Science, pages 20–
36. Springer, 2007.
de Salvo Braz, R., O’Reilly, C., Gogate, V., and Dechter,
R. Probabilistic Inference Modulo Theories. In Proceed-
ings of the Twenty-Fifth International Joint Conference
on Artificial Intelligence, New York, USA, 2016.
de Salvo Braz, R. Lifted First-Order Probabilistic In-
ference. PhD thesis, University of Illinois at Urbana-
Champaign, 2007.
Goodman, N. D., Mansinghka, V. K., Roy, D. M.,
Bonawitz, K., and Tarlow, D. Church: a language for
generative models. CoRR, abs/1206.3255, 2012.
Kersting, K. Lifted probabilistic inference. In European
Conference on Artificial Intelligence, 2012.
Michels, S., Hommersom, A., Lucas, P. J. F., and Velikova,
M. A new probabilistic constraint logic programming
language based on a generalised distribution semantics.
Artificial Intelligence, 228(C):1–44, November 2015.
Michels, S., Hommersom, A., and Lucas, P. J. F. Approxi-
mate probabilistic inference with bounded error. In Pro-
ceedings of the Twenty-Fifth International Joint Confer-
ence on Artificial Intelligence, New York, USA, 2016.
Milch, B., Marthi, B., Russell, S., Sontag, D., Ong, D. L.,
and Kolobov, A. BLOG: probabilistic models with un-
known objects. In IJCAI’05: Proceedings of the 19th
international joint conference on Artificial intelligence,
pages 1352–1359, San Francisco, CA, USA, 2005. Mor-
gan Kaufmann Publishers Inc.
Milch, B., Zettlemoyer, L., Kersting, K., Haimes, M.,
and Kaelbling, L. P. Lifted probabilistic inference with
counting formulas. In Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence (AAAI-2008),
Chicago, Illinois, USA, July 2008 2008.
Narayanan, P., Carette, J., Romano, W., Shan, C.-c., and
Zinkov, R. Probabilistic inference by program transfor-
mation in hakaru (system description). In Kiselyov, O.
and King, A., editors, Functional and Logic Program-
ming: 13th International Symposium, FLOPS 2016,
Kochi, Japan, March 4-6, 2016, Proceedings, pages 62–
79. Springer International Publishing, Cham, 2016.
Pearl, J. Probabilistic reasoning in intelligent systems: net-
works of plausible inference. Morgan Kaufmann, San
Mateo (Calif.), 1988.
Phan, Q.-S. Model Counting Modulo Theories. PhD thesis,
Queen Mary University of London, 2015.
Poole, D. First-order probabilistic inference. In Proceed-
ings of the 18th International Joint Conference on Arti-
ficial Intelligence, pages 985–991, 2003.
Richardson, M. and Domingos, P. Markov Logic Net-
works. Technical report, Department of Computer Sci-
ence, University of Washington, 2004.
Sanner, S. and Abbasnejad, E. Symbolic variable elimi-
nation for discrete and continuous graphical models. In
Proceedings of the Twenty-Sixth AAAI Conference on Ar-
tificial Intelligence, 2012.
Taghipour, N., Fierens, D., den Broeck, G. V., Davis, J., and
Blockeel, H. Lifted variable elimination: A novel op-
erator and completeness results. CoRR, abs/1208.3809,
2012.
Van den Broeck, G., Taghipour, N., Meert, W., Davis, J.,
and Raedt, L. D. Lifted probabilistic inference by first-
order knowledge compilation. In In Proceedings of the
22nd International Joint Conference on Artificial Intelli-
gence, pages 2178–2185, 2011.
Zhang, N. L. and Poole, D. A simple approach to Bayesian
network computations. In Proceedings of the Tenth Bien-
nial Canadian Artificial Intelligence Conference, 1994.
A SUPPLEMENTARY MATERIALS
A.1 Theorems 3.2 and 3.3 and their proofs
Theorem 3.2 (Tuple and Empty Set Simplifiers). The fol-
lowing tuple and empty set simplifiers
1. (r1, . . . , rn) = (s1, . . . , sn) ?
r1 = s1 ? · · · ? rm = sm (or its negation for 6=).
2. t ? {t1, . . . , tn} ? (t = t1) ? t ? {t2, . . . , tn}.
3. t ?
?
i?D:C ? ? ?i ? D : (C ? t ? ?).
4.
(
?
i?D:C ?
)
= ? ? ?i ? D : (¬C ? ? = ?).
5. S ? ? = ? ? TRUE.
6. S ? ? = ? ? S = ?.
7. S ? {t1, . . . , tn} = ? ?
(t1 /? S) ? (S ? {t2, . . . , tn} = ?).
8.
(
?
i?D:C ?
)
?
(
?
i??D?:C? ?
?
)
= ? ?
?i ? D : C ? ?i? ? D? : C? ? (? ? ?? = ?).
9. (S1 ? S2) ? S3 = ? ? (S1 ? S3) ? (S2 ? S3) = ?.
10. S1 ? S2 = ? ? S1 = ? ? S2 = ?.
11. {t1, . . . , tn} = ? ? FALSE if n > 0, TRUE otherwise,
when included in SEMT, rewrite ocf [E] = ? expressions to
equivalent formulas free of tuple and set expressions.
Proof. Intuitively, this theorem is analogous to an algo-
rithm that converts propositional formulas into an equiva-
lent disjunctive normal form (DNF), that is, a disjunction of
conjunctive clauses (that is, conjunctions of literals). Once
a DNF is reached, contradictory conjunctive clauses are
eliminated, and therefore the formula is satisfiable if and
only if there is at least one conjunctive clause with at least
one literal. In this analogy, conjunctions and disjunctions
correspond to intersection and union, and sets correspond
to conjunctive clauses. Empty sets are eliminated, and if
at the end we have a union of sets, and the empty ones
have been eliminated, this means that the resulting set is
not empty.
Formally, the theorem is proven by induction on the dis-
tance vector, a tuple that measures how far an expression
is from being solved. Before we define the distance vec-
tor, we need to inductive define, for any expression E, the
intersection-union nesting N??(E):
N??(E) =
?
?
?
?
?
?
i N??(Ei), if E = E1 ? · · · ?En
1 + maxiN??(Ei), if E = E1 ? · · · ? En
0, if E is any other expression.
Intuitively, N?? measures how far we are from a “flat”
union of intersections.
The distance vector of an expression E is a vector of non-
negative integers that is lexicographically ordered, with the
most significant component listed first:
1. N??(E);
2. number of intensional unions (
?
);
3. number of existential and universal quantifications;
4. number of ? applications;
5. sum of lengths of extensionally defined sets ({. . . });
6. number of ? applications;
7. number of comparisons to ?;
8. number of tuples.
In the base case, the distance vector is a tuple of zeros, and
therefore the expression is a formula without any tuple or
set operators, satisfying the theorem.
Otherwise, the distance vector contains at least one non-
zero component. If we can show that there is always at
least one applicable simplifier, and that every simplifier ap-
plication results in an expression with a smaller distance
vector with respect to the lexicographical order, then the
theorem will have been proven by induction.
There is always an applicable simplifier to expressions with
non-zero distance vector, because in that case there is at
least one tuple operator or a comparison between a set ex-
pression and ?:
• if there is a set expression, it must be one of ?, ?
applications or
?
, and there is at least one simplifier
for each of these;
• If there is a tuple anywhere, it is either inside a tuple
comparison, or inside a set; if it is in a comparison,
simplifier 1 applies; if it is in a set, one of the set sim-
plifiers applies.
Once it is established that there is always an applicable sim-
plifier, the next step is whether the distance vector is always
decreased according to its lexicographical order. Simpli-
fiers 1, 2, 4, 5, 6, 10, and 11 strictly decrease one or more
of the distance vector components without increasing any
other, so for expressions for which any of them apply, the
theorem is proven by induction on the distance vector.
The remaining simplifiers decrease a distance vector com-
ponent while increasing others, but the ones increased are
always less significant in the lexicographical order than the
one decreased:
• Simplifier 3 decreases the number of intensional
unions at the cost of the less significant number of ex-
istential quantifications;
• Simplifier 7 decreases the sum of lengths of extension-
ally defined sets at the cost of the less significant num-
ber of ? applications;
• Simplifier 8 decreases the number of intensional
unions at the cost of the less significant number of uni-
versal quantifications;
• Simplifier 9 duplicates S3 and therefore doubles all
distance vector components in S3, with the exception
the most significant one, N??, which is decreased:
N??((S1 ? S2) ? S3 = ?)
= max(N??((S1 ? S2) ? S3), 0)
= N??((S1 ? S2) ? S3)
= N??(S1 ? S2) +N??(S3)
= 1 +max(N??(S1), N??(S2)) +N??(S3)
= 1 +max(N??(S1) +N??(S3),
N??(S2) +N??(S3))
= 1 +max(N??(S1 ? S3),
N??(S2 ? S3))
= 1 +N??((S1 ? S3) ? (S2 ? S3))
> N??((S1 ? S3) ? (S2 ? S3))
= N??((S1 ? S3) ? (S2 ? S3) = ?).
To summarize, we have shown that, for every expression in
the language of interest, there is always an applicable sim-
plifier, and that all simplifiers decrease the distance vector
until it reaches the base case all-zero distance vector, which
is free of tuple and set operators.
Theorem 3.3 (Inversion Modulo Theories). Let E be an
expression and Ti, Ci be type and constraint, respectively,
in a theory for which we have a satisfiability solver. Then,
?
f?A?B
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
E,
where A = ocf [
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
E] (this is re-
laxed in Section 3.4), is equivalent to , and therefore can
be rewritten as,
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
?
f ? ocf [E]?B
E,
if
?
distributes over
?
and
?x?1 ? T1 . . . ?x
?
k ? Tk ?x
??
1 ? T1 . . . ?x
??
k ? Tk
?
?
C1[x1/x
?
1] ? · · · ? Ck[xk/x
?
k]
? C1[x1/x??1 ] ? · · · ? Ck[xk/x
??
k]
? (x?1, . . . , x
?
k) 6= (x
??
1 , . . . , x
??
k)
?
?
?
?
?
ocf [E][x1/x
?
1, . . . , xk/x
?
k]
?
ocf [E][x1/x
??
1 , . . . , xk/x
??
k ]
?
? = ? (2)
(that is, the set of argument tuples in applications of f in
E for different value assignments to x1, . . . , xk are always
disjoint — we refer to this as Condition (2) from now
on). Moreover, the rewritten expression is exponentially
(O(2
?
i
|{xi?Ti:Ci}|)) cheaper to evaluate than the original.
Proof. Let m be the number of possible assignments to
x1, . . . , xm. We prove the theorem by induction on m. If
m is 0,
?
f?A?B
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
E
=
?
f???B
1
=
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
?
f ? ??B
1
=
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
?
f ? ??B
E
=
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
?
f ? ocf [E]?B
E,
because the empty products allow the substitution of 1 by
E and ? by ocf [E] without change.
If m > 0, let x? be the first possible assignment to
x1, . . . , xk satisfying C1 ? · · · ? Ck. Then
?
f?A?B
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
E
= (separating x? from other assignments)
?
f1?ocf [E][x1,...,xk/x?]?B
?
f?(A\ocf [E][x1,...,xk/x?])?B
E1 ?
?
(x1,...,xk)?T1×···×Tk:C1?···?Ck?(x1,...,xk) 6=x?
E,
where E1 = E[f/f1]
= (E1 has no occurrences of f )
(
?
f1?ocf [E][x1,...,xk/x?]?B
E1
)
?
?
f?(A\ocf [E][x1,...,xk/x?])?B
?
(x1,...,xk)?T1×···×Tk:C1?···?Ck?(x1,...,xk) 6=x?
E
= (by induction on m)
(
?
f1?ocf [E][x1,...,xk/x?]?B
E1
)
?
?
(x1,...,xk)?T1×···×Tk:C1?···?Ck?(x1,...,xk) 6=x?
?
f?ocf [E]?B
E
= (renaming f1 to f and using the fact that E1 = E[f/f1])
(
?
f?ocf [E][x1,...,xk/x?]?B
E
)
?
?
(x1,...,xk)?T1×···×Tk:C1?···?Ck?(x1,...,xk) 6=x?
?
f?ocf [E]?B
E
= (introducing intensional products on x1, . . . , xk bound to x?)
?
(x1,...,xk)?T1×···×Tk:C1?···?Ck?(x1,...,xk)=x?
(
?
f?ocf [E][x1,...,xk/x?]?B
E
)
?
?
(x1,...,xk)?T1×···×Tk:C1?···?Ck?(x1,...,xk) 6=x?
?
f?ocf [E]?B
E
= (merging
?
by disjuncting their constraints)
?
(x1,...,xk)?T1×···×Tk:C1?···?Ck?
(
(x1,...,xk)=x??(x1,...,xk) 6=x?
)
?
f?ocf [E]?B
E
= (eliminating tautology on x? and separating
?
per index)
?
x1?T1:C1
· · ·
?
xk?Tk:Ck
?
f ? ocf [E]?B
E
The final expression is O(2
?
i
|{xi?Ti:Ci}|) cheaper to eval-
uate because the final f has all xi bound to a single value,
mapping each of their
?
i |{xi ? Ti : Ci}| assignments to
a single one and thus dividing the total size of the domain
over which one must iterate.
