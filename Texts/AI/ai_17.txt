“I can assure you [. . . ] that it’s going to be all right”?
A definition, case for, and survey of algorithmic assurances in human-autonomy trust
relationships
BRETT ISRAELSEN†, University of Colorado, BoulderDepartment of Computer Science, RECUV, and C-UAS
As technology become more advanced, those who design, use and are otherwise a?ected by it want to know that it will perform
correctly, and understand why it does what it does, and how to use it appropriately. In essence they want to be able to trust the
systems that are being designed. In this survey we present assurances that are the method by which users can understand how to trust
this technology. Trust between humans and autonomy is reviewed, and the implications for the design of assurances are highlighted.
A survey of existing research regarding assurances is presented, and several key ideas are extracted in order to re?ne the de?nition of
assurances. Several directions for future research are identi?ed and discussed.
Additional Key Words and Phrases: Human Computer Interaction, Trust
ACM Reference format:
Bre? Israelsen. 2017. “I can assure you [. . . ] that it’s going to be all right”. ACM Comput. Surv. 01, 01, Article 01 (July 2017), 44 pages.
DOI: 0000001.0000001
1 INTRODUCTION
As technology becomes more advanced, those who design, use, and are a?ected by it in other ways want to know that
it will perform correctly, and understand why it does what is does, and how to use it appropriately. In essence, people
who interact with advanced technology want to be able to trust it appropriately, and then act on that trust.
In interpersonal relationships, and otherwise, humans act largely based on trust. For example, a supervisor asks a
subordinate to accomplish a task based on several factors that indicate they can trust them to accomplish that task.
When consumers make purchases, they do so with trust that the product will perform as promised. Likewise, when
using something like an autonomous vehicle, the user must be able to trust it appropriately in order to use it properly.
With the rapid advancement of the capabilities of intelligent computing technology to do tasks that were previously
assumed to be too complicated for computers, there has been much recent discussion regarding how humans can trust
this technology – although the connection to trust is not always made explicit, per se. ?is discussion has taken place
both in public [15, 23, 26, 29, 117, 131], business [7, 10, 63, 92, 111, 122], and academic [16, 35, 41, 43, 46, 81] se?ings.
?ose who discuss how to trust a speci?c technology are really referring to the need for some indication of the
appropriate level of trust to give said technology. In other words, it is desirable to design capabilities and methods
?HAL 9000, 2001 A Space Odyssey, full quote: “I know everything hasn’t been quite right with me, but I can assure you now, very con?dently, that it’s
going to be all right again.
†?e corresponding author
?is work is supported by C-UAS and Northrop-Grumman Aerospace Systems.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for pro?t or commercial advantage and that copies bear this notice and the full citation on the ?rst page. Copyrights for third-party
components of this work must be honored. For all other uses, contact the owner/author(s).
© 2017 Copyright held by the owner/author(s). Manuscript submi?ed to ACM
Manuscript submi?ed to ACM 1
ar
X
iv
:1
70
8.
00
49
5v
2 
 [
cs
.C
Y
] 
 4
 S
ep
 2
01
7
2 Bre? Israelsen
Artificially 
Intelligent 
Agent (AIA)
User
User 
Trust-Related 
Behaviors 
(TRBs)
AIA 
Assurances
Actions
Fig. 1. Diagram depicting the simple one-way trust development relationship between a human user and an AIA. Based on a user’s
level of trust they take certain actions (e.g. give AIA commands), these commands can lead the AIA to certain actions and/or to
provide assurances to the user in order to a?ect their trust.
for intelligent technology which help us achieve appropriate levels of trust in that technology. ?ese capabilities and
methods are collectively referred to as assurances.
Speci?cally, this survey investigates what assurances an Arti?cially Intelligent Agent (AIA) can provide to a human
user in order to a?ect their trust. ?e colloquial de?nitions of ‘appropriate use’, ‘assurance’, ‘AIA’, and ‘trust’ should
su?ce for now to give the reader a general idea of the motivation; more formal de?nitions will be presented in Section 2.
It is the author’s position that there are many researchers, from di?erent disciplines, who will potentially be interested
in this work. ?is group includes those who are interested in working with, trusting, interpreting, understanding,
and/or regulating AIAs.
Figure 1 is a simple diagram of the trust cycle that exists between a human user and an AIA (justi?cation for the
existence of this cycle will be presented later). Simply, the user’s trust is a?ected by assurances that in turn a?ect the
user’s behaviors in interacting with the AIA (e.g. to trust AIA with responsibilities, or not). To fully understand and
appreciate the importance of assurances, one must have a more formal understanding of each component in Figure 1.
?is paper provides an overview of the components of Figure 1. It then turns a more focused a?ention to assurances,
and investigates some of the related research that has been done to date. From this survey of literature, properties and
classi?cations of assurances are created, and directions and considerations for further research are presented.
Some of the novel contributions of this paper include: identifying several di?erent bodies of research that contribute
to this topic; a detailed description and de?nition of assurances in general human-AIA relationships; and a detailed
breakdown of the di?erent components of assurances, and design considerations for implementing them. To this end,
Section 2 provides de?nitions for each of the terms. In Section 3 we discuss the methodology used when compiling this
survey. A?erwards, Section 4 will discuss the current landscape of assurances that exist in the literature. Section 5
draws important insights and conclusions from the literature, and then uses those to present a more detailed version of
Figure 1, that helps inform considerations when classifying and designing assurances. Finally, conclusions are presented
in Section 6.
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 3
2 BACKGROUND ANDMOTIVATION
What do people who talk about ‘comprehensible systems’, ‘interpretable learning’, ‘opaque systems’, and ‘explainable
AI’ really care about? ?is document investigates, in a formal and comprehensive manner, which dimensions of a user’s
trust should be in?uenced by the assurances of an AIA, as well as how this might be practically accomplished.
Motivations and a grounding application are presented is Sections 2.1 and 2.2, along with some application-speci?c
de?nitions of the trust-cycle terms. A discussion of related work is in Section 2.3. Assurances are presented and de?ned
in Section 2.4. Section 2.5 de?nes the term AIA in a general sense, and how they relate to assurances. Section 2.6
discusses trust, and provides a model of human-AIA trust that can be used in designing assurances. Finally, Section 2.7
discusses trust-related behaviors and how they should be considered when designing assurances.
2.1 Motivation
Generally, humans have always wanted to trust the tools and systems that they create. To this end many metrics and
methods have been created to help assure the designers and users that the tools and systems are in fact capable of being
applied in certain ways, and/or will behave as expected. Of course, at this point in the document ‘trust’ is quite an
imprecise term, and needs to be more formally de?ned; this will be done in Section 2.6
?e situation has grown more complicated in recent years because the advanced capabilities of the systems being
created can at times be di?cult for even those who designed them to comprehend and predict. ?ere are, for instance,
systems that have been designed to learn from extremely large amounts of data and expected to regularly perform
on never before seen data. In some cases, such systems have been designed to perform tasks that might take humans
entire lifetimes to complete. Below is a small sample of some application areas that exist and a possible reason why
they – perhaps unknowingly – have an interest in creating trustworthy systems.
Interpretable Science: Scientists need to be able to trust that the models created using data analysis, and be
able to draw insights from them. Scienti?c discoveries cannot depend on methods that are not understood.
Reliable Machine Learning: It is critical to have safety guarantees for AIAs that have been deployed in real-
world environments. Failing to do so can result in serious accidents that could cause loss of life, or signi?cant
damage.
Arti?cial Intelligence/Machine Learning: ?ere is a need to interpret how and why theoretical AIA models
function. ?is is due to the need to know they are being applied correctly, but also to be able to design new
methods to overcome weaknesses in the existing methods.
Government: Governments are beginning to enforce regulations on the interpretability of certain algorithms in
order to ensure that citizens can understand why many services make the decisions and predictions that they
do. A speci?c example are the algorithms deployed by credit agencies to approve/reject loans.
Medicine: Medical professionals need to understand why data-driven models give predictions so that they can
choose whether or not to follow the recommendations. While AIAs can be a very useful tool, ultimately doctors
are liable for the decisions they make and treatments they administer.
Cognitive Assistance: Systems are being designed as aids for humans to make complex decisions, such as
searching and assimilating information from databases of legal proceedings. When an AIA presents perspectives
and conclusions as summaries of this data, it must be able to also present evidence and logic to justify them.
?e interests of the author lie speci?cally in the design of unmanned vehicles that operate in concert with human
operators in uncertain environments. In this se?ing, it is desirable for the unmanned vehicle to be able to communicate
Manuscript submi?ed to ACM
4 Bre? Israelsen
with a human in some way in order to help them properly use the vehicle. ?e hope is that, in doing so, the performance
of the team can be improved by appropriately utilizing the strengths of both the human and unmanned vehicle. ?is
application is explained in more detail below in relation to Figure 1.
Whether formally acknowledged or not humans want to design assurances to help them appropriately trust AIAs
(a point already mentioned in the introduction). ?ere are a few research ?elds that have formally and explicitly
considered trust between humans and technology. Some examples are: e-commerce, automation, and human-robot
interaction. However, due to their main goals these research e?orts have mainly focused on implicit properties of the
systems that a?ect trust. Conversely, there are other research ?elds that have informally considered how to a?ect the
trust of designers and users via explicitly designed assurances. However, due to their informal treatment of trust, it is
unknown and unclear how e?ective these designed assurances might be in practice, or what principles ought to be
considered when designing assurances for general AIAs. A key objective of this paper is to survey these areas and,
in doing so, help bridge the gap between them by identifying common goals and approaches, as well as highlighting
where the di?erent groups might bene?t from the research of others.
2.2 Motivating Application and Basic Definitions
It is useful to have a concrete example on which we can refer for grounding examples. As previously mentioned
the speci?c interests of the author lie in the design of unmanned vehicles that can work in cooperation with human
operators.
Speci?cally, consider an unmanned ground vehicle (UGV) in a road network with unmanned ground sensors (UGSs).
?e road network also contains a pursuer that the UGV is trying to evade while exiting the road network. A human
operator monitors and interfaces with the UGV during operation. ?e operator does not have a detailed knowledge
of how the UGV functions or makes decisions, but can interrogate the system, modify the decision making stance
(such as ‘aggressive’ or ‘conservative’), and provide information and advice to the UGV. In this situation the operator
could bene?t from the UGVs ability to express con?dence in its ability to escape given the current sensor information,
and work with the AIA to modify behavior if necessary. ?is application serves as a useful analog for many di?erent
autonomous system applications, e.g. autonomous VIP escort problems and intelligence, surveillance and reconnaissance
(ISR)/counter-ISR operations for defense and security [64].
In this scenario the trust-cycle terms can be simply de?ned as follows:
Arti?cially Intelligent Agent: ?e UGV, this problem is di?cult because the pursuer is only observed spo-
radically, and does not follow a known course. ?e UGV must make decisions under uncertainty, with li?le
information.
Trust: ?e operator’s willingness to rely on the UGV when the outcome of the mission is at stake, such as in a
scenario where the UGV is carrying a valuable payload.
Trust-Related Behaviors: ?e operator’s behaviors towards the UGV, including the information provided, and
the commands given. ?is might take the form of approving/rejecting the UGV plan upfront, or the possibility
of involving real-time communication and adjustments to new information that the operator receives.
Assurances: Implicit and explicit communication from the UGV that has an e?ect on the operator’s trust. An
assurance might be in the form of communicating the probability of success for a given plan, or communicating
that the mission is not proceeding as expected.
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 5
Fig. 2. Application example of unmanned ground vehicle (UGV) in a road network, trying to evade a pursuer. The UGV has access to
unmanned ground sensors (UGSs) (also an unmanned aerial vehicle (UAV) that can be used as a mobile sensor), as well as information
and decision making advice from a non-expert human operator. The operator’s actions towards the UGV are trust-based.
2.3 Related Work
?ere has been a fair amount of work recently in the AI community that considers the importance of interpretability,
explainability, and transparency (i.e. Doshi-Velez and Kim [30], Weller [135], and Lipton [79], and Gunning [48]).
?is work has many interesting and important insights regarding the need for transparency, but does not formally
acknowledge the role of trust in human decision making or how interpretability and transparency a?ect the trust of
those who use AIAs. However, this work is extremely bene?cial because it draws the a?ention of researchers to this
critical area, and it is the beginning of the formalization of the problem from those who are currently doing a majority
of the design of assurances.
Lillard et al. [78] addressed the role of assurances in the relationship between humans and AIAs, and provides much
of the foundation for describing the relationships between assurances and trust in human-AIA interaction. Here, the
framework for analyzing assurances is presented in a way that is both more general and more detailed, albeit with
the same end goal of being applied in a very similar end application. For instance, we consider the full trust model
presented by McKnight and Chervany [88], whereas Lillard et al. only consider a subset of the trust model.
Regarding the relationship with the work of McKnight and Chervany [88], who constructed a typology of inter-
personal trust, we adopt the position that besides being applied to the e-commerce industry their trust model also
applies to relationships between humans and AIAs (as in Lillard et al. [78]). ?ey refer to something called ‘vendor
interventions’ that are related to what we call assurances in this paper. One small, but important distinction between
vendor interventions and assurances is that assurances cannot directly have an a?ect on the user trust-related behaviors
(TRBs). ?e premise is that no e?ect on a user’s trust can force a TRB. ?is is an important point because we are
considering a scenario (arguably more realistic, and de?nitely more humane) in which the human and AIA are working
together, and not where the human is a puppet cont
Corritore et al. [22] refer to a concept similar to assurances which they call ‘trust cues’. ?ey review some of the
literature that existed in the domain of e-commerce at that time. ?e wrap these trust cues into what they call ‘external
factors’ that a?ect the trust of a user of the internet. ?e way they present trust cues is as properties of websites, and
the e?ects that those properties had on customers.
Manuscript submi?ed to ACM
6 Bre? Israelsen
While assurances are de?ned by Lillard et al., and mentioned by McKnight and Chervany, and Corritore et al., we
investigate in detail how assurances ?t within the trust cycle (from Figure 1), survey what methods of assurance have
been and are currently being used, then present a re?ned de?nition and classi?cation of assurances. In essence others
have noted the existence of assurances, and we now directly consider the question: What, exactly, are assurances, and
how can they be designed? To that end we perform a survey of literature that formally considers trust between humans
and AIAs, as well as literature that informally investigates trust through topics like transparency, explainability, and
interpretability, and begin to distill ideas for practically designing assurances in human-AIA trust relationships.
2.4 Assurances
?e term assurances was introduced in the previous section as the name by which feedback will be known in a
human-AIA trust relationship. As assurances are the main topic of this paper, and have received li?le a?ention in trust
literature, a more detailed de?nition and discussion is merited.
McKnight and Chervany [88] allude to this kind of feedback in an e-commerce relationship as ‘Web Vendor In-
terventions’ and mention some possible actions that might be used in that speci?c application. ?ey go as far as
making a diagram that indicates that these interventions could a?ect the ‘Trusting Beliefs’, ‘Trusting Intentions’, and
‘Trust-Related Behaviors’ (see Figure 5) of an online human user. Corritore et al. [22] refer to assurances as ‘trust
cues’ that can in?uence how online users trust vendors in an e-commerce se?ing. Lee and See [73] discuss ‘display
characteristics’, which are methods by which an autonomous systems can communicate information to an operator.
?e term assurances is perhaps earliest used in the context of human-AIA relationships by Sheridan and Hennessy
[115]. More recently, and formally, Lillard et al. [78] de?ned the term ‘assurance’, we extend this de?nition here to be
more general:
Assurance: A property or behavior of an AIA that a?ects a user’s trust. ?is a?ect can increase or decrease trust.
Most researchers familiar with the ?elds of AI, ML, data science, and robotics will recognize terms like interpretable,
comprehensible, transparent, veri?ed and validated (V&V), certi?ed, and explainable AI, with respect to the models or
performance of a designed system. A key claim of this paper is that from a high level all of these approaches
have the same aim: for a user to be able to trust an AIA to operate in a certain way, and based on that trust
behave appropriately towards the AIA. ?ese methods are therefore means of assuring or calculating assurances to
be given to a human user.
Re?ned concepts about assurances are presented in Section 5 which elaborates on Figure 1. ?ere, important concepts
and considerations for designing assurances are reviewed.
2.5 Artificially Intelligent Agents
As noted by Tripp et al. [126] technology spans a wide spectrum of capabilities. With regards to autonomous systems
one might consider anything from a thermostat, to HAL 9000. While the main interest of the author is geared towards
the capability of humans to trust ‘advanced’ technology, for the purposes of this survey we will take a more holistic
view and use the term Arti?cially Intelligent Agent (AIA) to encompass a broad range of technologies that can be
considered automatic by some sense of the word. ?is is done in order to provide generally applicable de?nitions.
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 7
It is generally accepted that an arti?cial intelligence needs to possess the capabilities shown in Figure 3 [83, 99, 113]1.
?e following simple de?nitions will help to ground further discussion in the paper:
Reasoning: ?e ability to solve problems, and make conclusions.
Knowledge Representation: ?e ability to internally represent knowledge of information that has been learned.
Planning: ?e ability to make a plan in order to accomplish a goal within an environment.
Learning: ?e ability to learn from experience and data.
Perception: ?e ability to use di?erent sensors to perceive the surrounding environment.
Motion/Manipulation: ?e ability to move within an environment and manipulate parts of it.
Interaction: ?e ability to interact with other intelligent agents. For communicating with humans this could
involve some type of natural language interface.
General Capabilities of Artificially Intelligent Agents
Restricting the scope: why do WE care? Our 
primary interest is autonomous robotic 
vehicles
Russell and Norvig:
? NLP
? Knowledge Rep
? Reasoning
? ML
? Vision
? Robotics
Poole and Mackworth
? Representation and Reasoning
? Learning and Planning
Nilsson
? Reasoning and decision making
? Knowledge representation
? Mobility
? NLP
?
Reasoning Knowledge Representation Planning Learning
ExpressionMotion/ ManipulationPerception
Fig. 3. List of the capabilities of an artificial general intelligence. In this paper an AI is defined as a system that possesses at least one
of the capabilities illustrated. AIA capabilities are the sources of assurances.
It should be noted that the categories are not clearly separable; for instance where does the capability to plan end,
and reasoning begin? Similar questions could be asked of the other capabilities. Regardless, the concept of a separation
is useful in de?ning an AIA:
Arti?cially Intelligent Agent: an agent that acts on a goal (internally or externally generated), and possesses,
to some extent, at least one of the capabilities shown in Figure 3.
Of course, with such a broad de?nition, there will also be a broad range of di?erent AIAs. It is argued here that
it is most useful to classify that range along two axes: scope, and adaptability. ?e term scope refers to the range
of possible applications of the AIA: does it have a certain speci?c application, or can it be used in many di?erent
applications? Agents with narrow scope are more specialized, while those with a broad scope can be applied in more
diverse applications. Adaptability refers to the ability of the AIA to become be?er at executing its goal over time. Low
adaptability has been termed ‘weak’ and high adaptability ‘strong’. Figure 4 is a depiction of these two axes, and where
some typical AIAs might fall in that space.
Arguably we might just use the term arti?cial intelligence (AI) instead of AIA, however the term AI carries too much
ambiguity (in its fullest meaning it would possess all capabilities from Figure 3, and more). Using AIA allows the broad
inclusion of any system in the adaptability/scope plane. Figure 4 shows some examples of AIAs and their places on the
plane. To make sure the point is clear, the research discipline called machine learning (ML) is a subset of the AI research
1?is group of capabilities would generally be accepted in the AI community, although some argue that it is necessary to add other categories like
imagination, creativity, and social interaction. Of course this set of a?ributes is not universally accepted, and is still being re?ned.
Manuscript submi?ed to ACM
8 Bre? Israelsen
Weak AI:
Executes tasks within a 
rules-based programmed domain
Strong AI:
Dynamically Improves 
through experience
Broad AI:
Apply in many, diverse, 
contexts
Narrow AI:
Apply to limited, 
specialized, tasks
Be more careful with definitions
Inspired by: 
https://www.slideshare.net/ampnewventures/artificial-intelligen
ce-2016-amp-new-ventures#likes-panel
Image 
Classifier
Estimator
HAL 9000
Unmanned 
Systems
Recommender 
System
K-means 
Clustering
Personal 
Assistant
Automation
Speech to Text
AlphaGo
IBM Watson
Adaptability
Capability
Fig. 4. Illustration of the range of systems encompassed by the AIA definition. Horizontal axis reflects the scope of the AIA, the
vertical axis reflects the adaptability of the AIA.
landscape. Individual ML algorithms might be thought of as being a narrowly scoped AI that is contained within only
one of the AIA capabilities. As some examples, consider the following systems and the capabilities that they possess.
(1) An autonomous bo?le capping machine might perceive bo?les, and move mechanical parts to place caps on
them
(2) An unmanned aerial vehicle (UAV) might possess the ability to plan missions, perceive its location, and
execute motion of its components to carry out a plan
(3) A virtual personal assistant might be capable of interacting with a user, learning the user’s preferences, and
reasoning about what assistance the user needs
(4) An image classi?er might possess the capability to learn image classes from labeled examples and predict the
class of never-before-seen new images.
One might also question the need to de?ne AIAs in the ?rst place. ?is is to aid in the search for and understanding
of assurances. As will be shown later, di?erent methods of assurance can be found over the entire range of AIAs, so that
an automation system such as a bo?le capping machine might be able to use similar assurances – or more generally,
similar principles of assurance – as might a self-driving car, and vice-versa. ?e capabilities of AIAs (shown in Figure 3)
are the sources of assurances, in other words, assurances cannot exist without some source AIA capability.
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 9
?is de?nition, while broad, is still useful because it encompasses many of the systems that are typically described as
‘arti?cially intelligent’. More importantly, many of the assurances that exist for the simplest AIAs (e.g. a simple k-means
classi?er) can be extended for use in more advanced AIAs. In other words, the de?nition of AIAs sets an appropriate
scope for the bodies of research that are likely to have investigated assurances and assurance principles that can be
generalized/extended to any intelligent computing system. ?e de?nition of AIAs and their range of capabilities also
helps to understand and establish what kinds of assurances might be needed in future systems. For example, assurances
from an AIA that can only carry out planning tasks will probably di?er in design and/or application from assurances
from an AIA that can only carry out perception tasks.
2.6 User Trust
In designing assurances that a?ect trust-based user behaviors, it is critical to know what drives those behaviors. Because
of this, some time must be spent to understand what trust is.
Trust is critical in interpersonal relationships, and it a?ects the dynamics of intelligent multi-agent systems as
simple as one-on-one personal interactions [75], to more complicated ones such as ?nancial markets and governments
[38]. Consequently, researchers in psychology, sociology, and economics have historically sought to understand the
fundamental principles of trust, each with the aim of understanding their ?eld be?er [39]. Moral philosophers have
also thought intently about the topic [5].
Due to wide interest spanning many disciplines it is di?cult, if not impossible, to write a succinct de?nition of
trust that would appease all interested parties. Besides that, trust is actually a very broad concept that evades precise
de?nitions at a high level. However the following de?nition, adapted from [90], is broad enough to avoid too much
contention:
Trust: a psychological state in which an agent willingly and securely becomes vulnerable, or depends on, a
trustee (e.g., another person, institution, or an AIA), having taken into consideration the characteristics (e.g.,
benevolence, integrity, competence) of the trustee.
2.6.1 Trust between AIAs and humans? Trust is generally understood to exist between people. Is it possible for
a human to enter into a trusting relationship with an AIA? ?at humans actually do feel trust towards machines
has been experimentally con?rmed several times in research using common subjective psychological questionnaires.
Some examples include: Bainbridge et al. [6], Desai et al. [28], Freedy et al. [36], Groom and Nass [46], Inagaki et al.
[56], Kaniarasu et al. [61, 62], Mcknight et al. [85], Muir and Moray [96], Reeves and Nass [106], Riley [109], Salem et al.
[114], and Wang et al. [134].
Several academic experiments have investigated the possibility of trust existing between humans and (according
to the terminology of this survey) AIAs. All found that some level of trust can be formed in such relationships. For
instance, Lacher et al. [69] points out that people trust an AIA at di?erent levels. As an example, an operator would
have di?erent perspectives on trust based on their level of interaction with the AIA. ?e designer of an AIA would also
trust the AIA di?erently than an end user, due to the di?ering nature of the trust relationship from one to the other.
Tripp et al. [126] investigate the variation of trust between humans and di?erent levels of technology. ?ey run
experiments with three di?erent levels of technology: Microso? Access, a recommender system, and Facebook. ?ey
found that ‘human-like’ trust applied more to Facebook, while ‘system-like’ trust applied more to MS Access. ?ey
conclude that if the system is ‘human enough’, then a human trust model is appropriate. While there is technically a
di?erence between their de?nitions of ‘human’ and ‘system’ trust, we argue that for practitioners the di?erences are
Manuscript submi?ed to ACM
10 Bre? Israelsen
negligible. As an example they suggest replacing the competence dimension of the human trust model with a dimension
called functionality that has, in essence, the same de?nition as competence except it accounts for reduced complexity of
the system.
?erefore, in the interest of reducing complexity we will use a human trust model as a basis for human-AIA trust –
with the understanding that the de?nitions of the model must correspondingly vary with the complexity of the AIA.
2.6.2 A Model of Human-AIA Trust. We now present a model of human trust, which will cast insights on the
necessary e?ects of assurances. It should be noted that this model is being presented as one possible model that can be
helpful in understanding assurances – it is neither the only model, or a perfect model (as discussed above). As research
advances, such models will likely continue to evolve, and the ideas of assurances will naturally evolve as well.
In work relating to business management, McKnight et al. [89], and later McKnight and Chervany [88], performed
what is, arguably, the ?rst multi-disciplinary survey and uni?cation of trust literature, which also condensed it into a
single typology. ?e resulting model is shown, with some minor adaptations, in Figure 5. ?e ?gure illustrates the
three categories that make up a human’s trust. ?ere are causal arrows that connect the di?erent components. ?e
‘Dispositional Trust’ block is generally considered by psychologists, and deals with long-term psychological traits that
develop in a person from childhood. ?e ‘Institutional Trust’ block is generally studied by sociologists, and represents
the level to which a person trusts social/commercial structures. Finally, the ‘Interpersonal Trust’ block is deals directly
with one-on-one relationships and can generally ?uctuate more quickly than the other two.
User Trust
Interpersonal Trust (social psychology and economics)Institutional Trust 
(sociology)
Dispositional Trust 
(psychology/economics)
Disposition to 
Trust
Institution-Based 
Trust Trusting Beliefs
Trusting 
Intentions
Slower Time Scale Faster Time Scale
Fig. 5. Interdisciplinary trust model proposed by McKnight and Chervany [88]. The three main categories are delineated, and
corresponding disciplines that are interested are listed within parentheses. Connections indicate a causal relationship. The suggestion
regarding time scales of the development of trust is the author’s addition; trust development is discussed more in [76], and [75]
In the context of AIAs the components of the three categories from Figure 5 are de?ned as follows:
Disposition to Trust: ?e extent to which one displays a consistent tendency to be willing to depend on AIAs
in general across a broad spectrum of situations and persons
Institution-Based Trust: One believes that regulations are in place that are conducive to situational success in
an endeavor
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 11
Trusting Beliefs: One believes that the AIA has one or more characteristics bene?cial to oneself
Trusting Intentions: One is willing to depend on, or intends to depend on, the AIA even though one cannot
control its every action
Each of these main categories of trust has components de?ned in Figure 6. ?ese components were de?ned through
the compilation of many research studies across research disciplines, and because of this represent the most accurate
notion of the components of trust available. ?ese categories comprise the principal drivers of TRBs, and as such are
the targets at which assurances must be directed.
2.7 Trust-Related Behaviors
Something that is well accepted among researchers of all disciplines is that trust ultimately leads to some kind of behavior
or action; this idea was highlighted by Lewis and Weigert [77]. McKnight and Chervany [88] call these ‘trust-related
behaviors‘ (TRBs), which is the term that will be used in this survey. In the case of a human-AIA relationship that the
author is concerned with, TRBs could include the kinds of tasks the human user assigns to the UGV such as accepting
and following through on its plan, or directing that a new plan be made.
2.7.1 Calibration of Trust-Related Behaviors. Trust is not a quantity that can be objectively measured. Rather, its
relative magnitude must be observed through changes in TRBs, or qualitative self-reports reported in surveys [96]. It
comes as no surprise that TRBs are the more objective measure due to the fact that people are not always consistent in
their ratings, and may sincerely feel di?erent levels of trust while performing similar TRBs. Parasuraman and Riley
[101] were interested in understanding the use of automation by humans, and de?ned terms to describe that use. Here
it is proposed that, by extension, those terms also apply to the behaviors of humans towards more advanced AIAs.
Within this scope the de?nitions are as follows:
Misuse: ?e over-reliance on an AIA (which could manifest itself in expecting too much accuracy from and AIA)
Disuse: ?e under-utilization of and AIA (which could be manifest in a user turning o? the AIA, or failing to use
all of its capabilities)
Abuse: Inappropriate application of automation (where application in this case means the choice to deploy an
AIA in a certain context, such as the choice to use a quad-copter underwater).
Recall the diagram in Figure 1; the AIA has in?uence on the user’s TRBs by way of assurances. We propose that the
AIA’s goal should be that the user should not misuse, disuse, or abuse it. Consider a space of all TRBs towards an AIA,
this space would include misuse, disuse, abuse, and appropriate use (all TRBs not in misuse, disuse, or abuse). In order
to ensure that humans use AIAs appropriately, it is critical that the user TRBs be calibrated to elicit behaviors that are
within the set of appropriate behaviors. ?is can only be done by in?uencing the user trust. ?is is a point that, to
some extent, has been informally mentioned in Hutchins et al. [54], Lee and See [73], Lillard et al. [78], Muir [94, 95].
A critical oversight of other researchers who mention ‘calibration’ (or other synonymous concepts) is that they
suggest calibrating trust as opposed to TRBs. Dzindolet et al. [32] studied the e?ect of performance feedback on user’s
self-reported trust, and found that it increased; however the appropriate TRBs toward the system did not re?ect the
level of self-reported trust. ?is shows the danger of calibrating ‘trust’, as opposed to calibrating the TRBs.
Calibrating TRBs focuses on concrete and measurable behaviors that are universally applicable. In contrast, calibrating
trust involves in?uencing a quantity that is directly immeasurable, and that, when measured indirectly, is subject to the
Manuscript submi?ed to ACM
12 Bre? Israelsen
A
ss
ur
an
ce
 T
ar
ge
ts
A
ss
ur
an
ce
s
D
is
po
si
tio
na
l
B
el
ie
f
In
te
nt
io
n
Th
is
 la
yo
ut
 is
 a
da
pt
ed
 fr
om
 fi
gu
re
 3
 o
f M
cK
ni
gh
t 
20
01
. E
xc
ep
t w
e 
(L
ill
ar
d)
 a
dd
 a
 c
on
ne
ct
io
n 
to
 
D
is
po
si
tio
n…
 th
is
 m
ay
 o
r m
ay
 n
ot
 b
el
on
g 
(I 
th
in
k 
it 
sh
ou
ld
). 
B
as
ed
 o
n 
Li
lla
rd
 2
01
6 
I a
dd
 c
on
ne
ct
io
ns
 to
 
In
te
nt
io
n 
an
d 
B
eh
av
io
r, 
w
hi
ch
 I 
th
in
k 
is
 th
e 
rig
ht
 
th
in
g 
to
 d
o.
C
ol
or
s 
--
["#
ffa
16
df
f",
"#
7f
7f
df
bd
",
"#
75
af
52
ff"
,
"#
78
cc
ea
ff"
,
"#
00
ba
a6
ff"
]
In
st
itu
tio
na
l
Fa
ith
 in
 A
ut
on
om
y 
--
U
se
r a
ss
um
es
 th
ey
 c
an
 ty
pi
ca
lly
 
tru
st
 a
n 
au
to
no
m
ou
s 
sy
st
em
Tr
us
tin
g 
St
an
ce
 --
D
ec
is
io
n 
to
 tr
us
t b
as
ed
 o
n 
ut
ili
ty
 
(e
ve
n 
if 
th
er
e 
is
 e
vi
de
nc
e 
no
t t
o)
St
ru
ct
ur
al
 A
ss
ur
an
ce
 --
U
se
r a
ss
um
es
 th
at
 “p
ro
te
ct
iv
e 
st
ru
ct
ur
es
” s
uc
h 
as
 c
on
tra
ct
s,
 a
nd
 
re
gu
la
tio
ns
 a
re
 in
 p
la
ce
 a
nd
 a
re
 
co
nd
uc
iv
e 
to
 s
uc
ce
ss
 in
 a
 g
iv
en
 
si
tu
at
io
n.
Si
tu
at
io
na
l N
or
m
al
ity
--
U
se
r b
el
ie
ve
s 
th
at
 th
e 
si
tu
at
io
n 
is
 
no
rm
al
, f
av
or
ab
le
, o
r c
on
du
ci
ve
 to
 
su
cc
es
s
C
om
pe
te
nc
e 
--
U
se
r b
el
ie
ve
s 
th
at
 th
e 
au
to
no
m
ou
s 
sy
st
em
 h
as
 th
e 
ab
ili
ty
 o
r p
ow
er
 to
 
do
 w
ha
t i
s 
ne
ed
ed
B
en
ev
ol
en
ce
/In
te
gr
ity
 --
U
se
r b
el
ie
ve
s 
th
at
 th
e 
au
to
no
m
y 
w
ill
 a
ct
 in
 th
ei
r i
nt
er
es
t, 
is
 n
ot
 
de
ce
pt
iv
e,
 a
nd
 fu
lfi
lls
 c
om
m
itm
en
ts
Pr
ed
ic
ta
bi
lit
y 
--
U
se
r b
el
ie
ve
s 
th
at
 th
e 
ac
tio
ns
 o
f 
th
e 
au
to
no
m
y 
ar
e 
co
ns
is
te
nt
 
en
ou
gh
 th
at
 th
ey
 c
an
 b
e 
fo
re
ca
st
 
in
 a
 g
iv
en
 s
itu
at
io
n.
W
ill
in
gn
es
s 
to
 D
ep
en
d 
--
U
se
r i
s 
pr
ep
ar
ed
 to
 m
ak
e 
th
em
se
lv
es
 v
ul
ne
ra
bl
e 
to
 th
e 
au
to
no
m
y 
by
 re
ly
in
g 
on
 it
Su
bj
ec
tiv
e 
pr
ob
ab
ili
ty
 o
f 
de
pe
nd
in
g 
--
Th
e 
ex
te
nt
 to
 w
hi
ch
 th
e 
us
er
 
pr
ed
ic
ts
 th
ey
 w
ill
 d
ep
en
d 
on
 th
e 
au
to
no
m
y
N
or
m
al
ity
 e
m
ph
as
iz
es
 th
e 
si
tu
at
io
n.
 T
he
 
ch
ar
ac
te
ris
tic
s 
of
 th
e 
ta
sk
/s
ce
na
rio
 w
ith
 a
ll 
el
se
 
be
in
g 
fix
ed
. -
- p
ro
je
ct
in
g 
fro
m
 a
n 
ex
pe
ct
ed
 s
et
 
of
 c
om
pe
te
nc
ie
s
C
om
pe
te
nc
e 
se
em
s 
to
 b
e 
ba
se
d 
on
 a
 g
iv
en
 
si
tu
at
io
n.
 It
 m
ay
 n
ot
 b
e 
im
m
ed
ia
te
ly
 c
le
ar
 th
at
 
th
e 
si
tu
at
io
n 
is
 n
or
m
al
/a
bn
or
m
al
. 
V
er
y 
di
ffi
cu
lt 
to
, i
n 
pr
ac
tic
e,
 to
 s
ep
ar
at
e 
th
e 
di
m
en
si
on
s 
of
 tr
us
t. 
 
In
te
rr
el
at
io
ns
hi
ps
 a
re
 in
te
rd
ep
en
de
nt
. E
ac
h 
us
er
 c
an
 b
e 
di
ffe
re
nt
. 
Tu
to
rin
g?
Fi
g.
6.
A
ss
ur
an
ce
ta
rg
et
s
ba
se
d
on
th
e
co
m
po
ne
nt
de
fin
it
io
ns
of
th
e
m
ai
n
ca
te
go
ri
es
of
tr
us
t:
‘D
is
po
si
ti
on
to
Tr
us
t’,
‘In
st
it
ut
io
n-
B
as
ed
Tr
us
t’,
‘T
ru
st
in
g
B
el
ie
fs
’,
an
d
‘T
ru
st
in
g
In
te
nt
io
ns
’
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 13
biases and uncertainties of humans, along with inherent di?erences between di?erent users. Viewing the task from this
point of view, the ?ndings of Dzindolet et al. are not surprising.
It is desirable for AIAs to be designed in order to encourage appropriate TRBs, as opposed to the alternative of
purposefully misleading users misuse or abuse. ?ere is a valid argument that many of today’s AIAs that ignore (or
whose designers ignored) TRBs and assurances can be ‘unwi?ingly malicious’ in that they do not actively a?empt to
guide user’s TRBs to lay within the space of appropriate TRBs.
3 METHODOLOGY
?is survey examines research of those who are formally and informally investigating human-AIA trust. In particular,
a?ention is devoted to ideas that are applicable to the trust relationship between a single human user and a single
AIA. While theoretically a two-way trust model could be considered (i.e. in which the AIA also has trust in the user),
a?ention is restricted here to a one-way trust relationship that considers only how user trust (and TRBs) evolves in
response to assurances from the AIA.
It should be noted that it is almost impossible to perform a fully comprehensive survey of all AIA assurances, due
to the broad spectrum of possible assurances, and AIAs in general. One could rightly argue that control engineers
treat metrics like gain and phase margins as assurances for automatic feedback control systems, in much the same
way that machine learning practitioners treat training and test accuracy as assurances for learning algorithms – and
hence concepts related to robustness, stability, etc. for feedback control systems ought also be included in this survey.
While assurances can be used in both the most simple ‘automatic’ systems (like a thermostat), this survey will focus on
assurances in more advanced AIAs that make decisions under uncertainty; however, the admi?edly narrow scope of
this survey does not impede the development of fundamental insights and principles in designing assurances.
Initially, in order to ?nd applicable research, papers that formally addressed trust, and tried to create models of it
were investigated; this with the aim of trying to understand how trust might be in?uenced. Secondly, we researched
some historical literature regarding trust between humans and some form of non-human entity (typically technological
in nature). ?is mainly led to ?elds like e-commerce, automation, and human-robot interaction. ?ird, we looked at
work regarding ‘interpretable’, ‘comprehensible’, ‘transparent’, ‘explainable’, and other similar types of learning and
modeling methods. Finally, with that literature as a background, we searched for research disciplines investigating
computational methods that would be useful as assurances, but in which trust itself is not the main focus.
?is information is then used to construct an informed de?nition and classi?cation of assurances based o? of
empirical information of methods that are currently in use, or being investigated. In doing so several ideas for future
research are identi?ed.
4 A SURVEY OF ASSURANCES
Now that AIA, trust, TRBs, and assurances have been de?ned we are ready to begin the survey of assurances. ?ere are
many di?erent ways in which this survey could be organized, we choose to present it based on the di?erent goals of
the main groups of researchers who have been working in the area.
Early in reading the related literature it became clear that there were two main groups: 1) those researchers who
have formally addressed the topic of trust between humans and AIAs of some form, and 2) a much larger body of those
who have informally considered trust in their work (or concepts related to trust). Here we consider formal treatment
of trust to include those who acknowledge a human trust model and who gather data from human users in order to
Manuscript submi?ed to ACM
14 Bre? Israelsen
measure the e?ect of assurances on trust. Informal treatment of trust includes those who reference the concept and/or
components of trust, but who do not gather user data to verify the e?ects of proposed assurances.
Another way that the landscape of researchers might be divided is by the kinds of assurances they investigate.
?e ?rst group consider what we call ‘implicit’ assurances. Implicit assurances embody any assurances that are not
deliberately designed into the AIA to in?uence trust or TRBs. ?e second group consider ‘explicit’ assurances, which
are assurances that were explicitly created by a designer with the intent of a?ecting a user’s trust. Implicit assurances
can be thought of as side-e?ects of the design process; for example HAL 9000 could have been designed with a circular
‘red-eye’ looking sensor because it was cost-e?ective, however it is possible that users who interact with HAL might
?nd the ‘red-eye’ sensor to suspicious, and thus lose trust in HAL. Conversely, the same ‘red-eye’ may have been
explicitly designed and selected based on several studies that indicated that users ?nd it easier to trust advanced AIAs
with ‘red-eye’ sensors instead of similarly shaped green sensors.
Much of the research that formally considers trust has focused on implicit assurances. ?is is likely due to the
focus on investigating what properties of an autonomous systems can a?ect a user’s trust. It is possible to argue that
someone who ?nds that reliability a?ects a user’s trust is investigating an explicit assurance, but for the purposes of
this paper we try to stay true to the intent of the researcher when performing the work. More recently, as seen by a
large spike in interest in ‘interpretable’, and ‘explainable’ AIAs in government, academic, and public circles, we have
seen an emergence a group who acknowledge that the concept of trust in human-AIA relationships, and who want to
design systems accordingly.
In view of these four main groups of researchers, we organize the survey by creating four quadrants shown in Figure 7.
In the remainder of this section we survey each of these quadrants separately in order to gain some understanding of
the lessons that each has to o?er when we consider the design of assurances.
Formal 
Treatment
Informal 
Treatment
Explicit Implicit
Tr
us
t
Assurance
XXXXXXXX
XXXXXXXX
XXXXXXXX
XXXXXXXX
XXXXXXXX
X
XXXXXXXX
XXXXX
XXXXXXXX
XX
XXXXXXXX
XXXXXXXX
XXXXXXXX
XX
https://research.cor
nell.edu/researcher
s/hadas-kress-gazit
Explicit - user processes 
actively communicated 
assurances
Implicit - user is based only 
on ability of user to infer
Social psychology origin, in so far as it 
can be engineered
HRI/HCI??
V&V?? Engineered, but not explicitly 
communicated by system to user
Dialogue/etiquette?? Chris miller generic 
rules of conversation
http://cynthiabreazeal.media.mit.edu/
Not-user centered
No-experiments or 
model of user trust
experiments and 
cognitive model of 
user trust
Whose frame of 
reference?
I.II.
III. IV.
Fig. 7. Figure depicting how many papers consider trust both formally and informally, as well as those who investigate explicit and
implicit assurances
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 15
• ?adrant I. (implicit assurances, formal trust treatment) – Gather user data, consider a trust model, consider
assurances that are implicit (i.e. those who care about human-AIA trust, but aren’t designing assurance
algorithms)
• ?adrant II. (explicit assurances, formal trust treatment) – Gather user data, consider a trust model, consider
assurances that are explicit (i.e. those who formally acknowledge human-AIA trust, and design assurances to
a?ect it)
• ?adrant III. (explicit assurances, informal trust treatment) – Do not gather data from users, reference trust
(or its components interpretability, etc..), consider assurances that are explicit (i.e. those who know that the
concept of ‘trust’ is important, but that only use an informal notion of it when designing assurances)
• ?adrant IV. (implicit assurances, informal trust treatment) – Not interested in a?ects on user trust, but
reference (possibly only allude to) concepts that are related to trust as de?ned in this paper. Investigate
approaches for creating AIAs with improved properties or characteristics. ?is work is subtly di?erent from
that in ?adrant III in the degree/intent to which trust concepts were considered. In ?adrant III trust
components were clearly the main focus of the research, in this quadrant the relationship to trust is only visible
to someone who knows what they are looking for (i.e. those whose work is relevant for designing assurances,
but don’t know it)
4.1 ?adrant I. (Implicit Assurances, Formal Trust Treatment)
Muir and Moray [96] performed an experiment where participants were trained to operate a simulated pasteurizer
plant. During operation they were able to intervene in the fully-autonomous system if they felt is was necessary to
obtain be?er performance. Trust was quanti?ed by self-reported questionnaire responses, as well as by the level of
reliance on the automation during the simulation. She noted that operators could learn to trust an unreliable system if
it was consistent. ?e participants were only able to observe the reliability of the pump (i.e. the performance of the
pumps over time, from which the user created a mental model of reliability).
In experiments involving thirty students and thirty-four professional pilots, Riley [109] investigated how reliability
and workload a?ected the participant’s likelihood of trusting in automation. Two simulated environments were created
to this end. First was for participants to use/not use an automated aid (with variable reliability) to classify characters
while also performing a distraction task. Interestingly, they found that pilots (those with extensive experience working
with automated systems) had a bias to use more automation, but reacted similarly to students in the face of dynamic
reliability changes. In this se?ing, the bias to use more automation would be known as ‘framing e?ects’ (where a
human’s trust is biased by the trust they have in previously encountered systems) in cognitive science. Findings also
showed that the use of automation is highly based on individual traits.
Also considering the performance of pilots, Wickens et al. [136] investigated the e?ect of semi-reliable data while
piloting a plane. ?ey also investigated semi-reliable performance of a system that highlighted important data for
the pilot to see. ?e pilots were aware that the measurements/highlighting system might be inaccurate before the
experiment. ?e reliability of the systems did have an e?ect on the outcome of the experiment, but interestingly did
not make a measurable e?ect on the pilot’s self-reported trust. ?is underscores the point that TRBs ought to be the
focus of assurances as opposed to trust itself, since trust is a subjective measure that may or may not actually change a
person’s TRBs.
Manuscript submi?ed to ACM
16 Bre? Israelsen
McKnight and collaborators have spent signi?cant time investigating trust between humans and technology. His
initial research was focused on e-commerce se?ings but later moved to trust between humans and technology. In [85]
they gather self-reported trust through a questionnaire. ?eir experiment was interested in identifying the dimensions
of trust e?ected by learning to use Excel for use in a business class. ?e results were based solely on the intrinsic
properties of excel and how each individual perceived them.
In [70] and later in [126] they investigate the di?erence in trust between humans and trust between a human and
technology. ?ey found that as the technology becomes more ‘human-like’ the self-reported trust has more similarities
to trust between humans. ?is study was performed using Microso? Access, a recommendation assistant, and Facebook.
Respondents were asked to rate how each so?ware ‘kept its commitments’ and how ‘human-like’ it was. Again, these
impressions were based solely on the intrinsic properties of each of the three AIAs used in the experiment.
Freedy et al. [36] studied how ‘mixed-initiative teams’ (MITs, their term for human-robot teams) might have
their performance measured. ?e premise of the work is that MITs can only be successful if “humans know how
to appropriately trust and hence appropriately rely on the automation”. ?ey explore this idea by using a tactical
game where human participants supervised an unmanned ground vehicle (UGV) as part of a reconnaissance platoon.
UGVs had three levels of capability (low, medium, high), and had autonomous targeting and ?ring capability which
the operator needed to monitor in case the UGV could not perform as desired. Operators were trained to recognize
signs of failure, and to only intervene if they thought the mission completion time would su?er. Trust was formally
acknowledged in this survey and was quanti?ed by using Relative Expected Loss (REL), which is the mean expected loss
of robot control over n trial runs. Operators were found to be more likely to use a ‘medium’ ability UGV if they had ?rst
encountered a ‘high’ ability UGV, as opposed to encountering a ‘low’ ability UGV ?rst, which is another manifestation
of framing e?ects like [109]. Similar to [96] the operators learned to trust a UGV with low competence as long as it
behaved consistently.
In a similar vein Desai et al. [28] investigated the e?ects of robot reliability on the trust of human operators. In this
case a human participant needed to work with an autonomous robot to search for victims in a building, while avoiding
obstacles. ?e operator had the ability to switch the robot from manual (teleoperated) mode, to semi-autonomous, or
autonomous mode depending on how they thought they could trust the system to perform. During this experiment the
reliability of the robot was changed in order to observe the e?ects on the operator’s reliance to the robot. Trust was
measured by the amount of time the robot spent in di?erent levels of autonomy (i.e. manual vs. autonomous), and it
was found that trust changed based on the levels of reliability of the robot.
Salem et al. [114] investigated the e?ects of error, task type, and personality on cooperation and trust between a
human and robot. In this case the robot was a domestic robot that performed tasks around the house. A human guest
was welcomed to the home and observed the robot operating on di?erent tasks. A?er this observation (in which the
robot implicitly showed competence by its mannerisms and successes/failures) the human participant was asked to
cooperate with the robot on certain tasks. Interestingly, it was found that self-reported trust was a?ected by faulty
operation of the robot, but that it didn’t seem to have an e?ect on whether the human would cooperate on other tasks.
?is seems to suggest that the e?ect of institutional trust (i.e. this robot may not be competent, but whoever designed it
must have known what they were doing) allowed users to continue to cooperate with a faulty system, even if they have
low levels of trust in it.
Wu et al. [138] use game theory to investigate whether a person’s decisions are a?ected by whether they believe
they are playing a game against a human or an AI. ?is idea was studied in the context of a coin entrustment game, in
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 17
which trust is measured by the number of coins a participant is willing to lose by pu?ing them at risk of the other
player. On the surface, their work is meant to be a study of the implicit di?erences in trustworthiness between humans
and robots; however in their experiment the ‘human’ was actually an AI with some programmed idiosyncrasies to
lead the human player to believe the AI was a human. ?is was done by adding a random wait time, as opposed to
an instantaneous move that the AI would make. ?ere were also prompts at the beginning of the ‘human’ version
of the experiment that suggested that the participant was waiting for another human player to join the game. ?e
experiment found that humans trust an AI more than they trust a ‘human’. ?e authors suggest that this may be due to
the perception that an AI does not have feelings and is operating in a more predictable way. Given that the ‘human’
was an algorithm as well, this experiment shows that consistency (i.e. no variable wait times) was a factor that a?ected
the trust of the participant.
Bainbridge et al. [6] investigated the di?erence in trust between a human and a robot, in cases where the robot
was physically present and where the robot was only displayed on a screen (i.e. not physically present). In this
experiment, the only method of communication from the robot was through physical gestures. Trust was measured by
the willingness of the human participants to cooperate with the robot. Among other interesting ?ndings regarding how
the participants interacted with the robot, it was found that participants were more likely to cooperate with the robot
when it was physically present. Bainbridge et al. suggest that this is due to increased trust, in this case cooperation is a
TRB.
With the aim of understanding how individuals currently trust autonomous vehicles, Munjal Desai et al. [97]
performed a survey of around 175 participants. ?e participants were asked to rate their level of comfort with six
di?erent situations. ?ese situations ranged from parking your own car, having an autonomous vehicle with manual
override park the car, and having a fully autonomous vehicle that could not be overridden park the car. ?ere were also
questions related to user comfort with autonomy in situations where they still retained control, like how comfortable
users would be with having autonomous vehicles park near their car. ?e survey found that the participants were
most comfortable with parking their own car, and least comfortable with having a fully autonomous vehicle (with no
manual override) park their car. ?ese ?ndings are supposedly related to institutional trust, as those surveyed did not
necessarily have any experience with autonomous vehicles.
4.1.1 Summary. We see that there have been several experiments that have formally shown the e?ect of implicit
assurances (assurances that were not purposefully designed to a?ect trust) on a user’s trust towards an AIA. Generally,
implicit assurances can a?ect any of the three trust dimensions highlighted in Figure 6. To use a practical example recall
that reliability (or rather the perception of reliability built over contiguous observations of performance) was frequently
investigated as an assurance in this section. A reliable AIA can seem more competent, and predictable to a human
user. Currently, it isn’t very clear how di?erent assurances a?ect the trust dimensions. Muir and Moray [96] a?empted
to identify the e?ects of reliability on the user’s perception of the AIAs competence and predictability, but only six
participants were used, so the results are questionable. ?antifying the e?ects of assurances on di?erent dimensions of
a user’s trust is still an open research question.
?roughout this work we see evidence for the idea that a user will gather assurances, whether these are implicit
or explicit, in order to execute TRBs. To restate the point, in the absence of explicitly provided assurances (such as
investigated by every research paper in this quadrant) a user will still use other perceived properties and behaviors and
gather assurances in order to inform their trust-related behaviors.
Manuscript submi?ed to ACM
18 Bre? Israelsen
Even if an AIA has the ability to calculate an assurance, it must still have a way by which to express that assurance
to a human user. ?e human user then perceives the assurance, perhaps through interaction with the system, or only
through more passive observation of the system. ?ese kinds of perceptions can be based on displayed information, or
on how the AIA ‘behaves’ (as in [114]). Once the user perceives some kind of assurances (perhaps not purposefully
communicated), those assurances are integrated into the trust of the user towards the AIA. In most cases this group of
research focuses on assurances given through visual cues.
We also see evidence that human cognitive limitations need to be taken into account when designing assurances.
?is was directly observed in [36, 109] where framing e?ects biased operator’s behaviors towards the AIAs. ?is also
suggests that other cognitive biases such as ‘recency e?ects’ (being biased based on recent experience), and others will
also apply as well. A couple of examples include well known cognitive biases such as ‘focusing e?ects’ (being biased
based on a single aspect of an event), or the ‘normalcy bias’ (refusal to consider a situation which has never occurred
before). Finally, humans naturally a?empt to construct statistical models (albeit not especially accurate ones) of the
world around them in order to predict and operate within it. In light of this designers must also consider the limitation
(time and otherwise) for humans to build statistical models, such as reliability, when only instance by instance data is
available. ?ese ideas still remain to be veri?ed by further, more focused, experimentation.
4.2 ?adrant II. (Explicit Assurances, Formal Trust Treatment)
Sheridan and Hennessy [115] were perhaps the ?rst to a?empt a formal approach to understanding the relationship
between an operator/designer and a supervisory control system, such as a control system for a petroleum re?nery.
?ey considered psychological models of the human user and asked questions about how the user could understand
how such systems worked. To this end they suggest that aspects of the control system need to be made transparent to
the user so that the user has an accurate model of the system, but they don’t o?er concrete ways in which this can be
accomplished. ?ey mention that control displays should be designed according to the designer’s level of trust in the
individual elements, but they seem to overlook the critical nature of what explicit assurances should be displayed to
the operator of the system, as opposed to what information might allow the designer to have appropriate trust in the
control system.
Muir investigated explicit assurances that automation could give to human operators in order to a?ect trust. She
began by investigating formal models of trust between humans and then extending those concepts to trust between
humans and automation. In [94] and [95] she investigated how decision-aids could be designed in order to a?ect
trust. Within the framework of a trust model she suggested that a user must be able to “perceive a decision aide’s
trustworthiness”, which could be accomplished by providing data regarding the system’s competence on a certain
task. She also suggests that summary information regarding the system’s reliability over time would help. Finally she
suggests improving the “observability” of the machine behaviors so that the user can understand it more fully.
She goes on to suggest that the user must have an accurate criteria for evaluating the system’s trustworthiness.
?is would involve understanding the domain of competence, the history of competence, and criteria for acceptable
performance of the system. She also suggests that a user must be able to allocate tasks to the system in order to feel
equal in the trust relationship. ?e idea is that a relationship in which only one entity makes decisions is not amenable
to calibrated trust. Finally she suggests that it is important to identify and ‘calibrate’ areas where a user may trust the
system inappropriately. One key shortcoming of her work is that she suggests types of explicit assurances, but does not
suggest concrete approaches to implement them, or test them by experimentation.
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 19
Kaniarasu et al. [62] examine whether or not misuse and disuse (over and under trust, in their terminology) can be
detected and then calibrated (aligned) to be within the AIA’s competence. ?ey use data from an experiment of a robot
with a con?dence indicator in a user interface. In this case the indicator was ‘high’, ‘neutral’, or ‘low’ depending on the
experimental mode the robot was operating in (which was unknown to the user except by the con?dence indicator). A
user was asked to provide trust feedback every twenty seconds while operating a robot along a speci?ed path without
hi?ing obstacles and responding to secondary tasks. ?e user trust was quanti?ed by measuring how frequently they
switched between partially autonomous and fully autonomous modes. ?e experiment found that the indicators of
con?dence did in fact reduce misuse and disuse. However, we note that at this point in time most robots are not
equipped to ‘know’ how reliable they are. ?is highlights a common shortcoming in the design of current robots: that
they cannot quantify their own reliability.
Although they don’t perform any formal experiments, Chen et al. [19] lay out a framework for agent transparency
based on formal trust models. ?is is in the se?ing of situational awareness (SA, [33]) of an autonomous squad member
in a military operation. ?e aim is to make the mission SA and agent more transparent, so that the user will be
able to trust the agent and use its assistance. ?ey propose explicit feedback that can support the three levels of an
operator’s SA. ?ey call their model the SA-based Agent Transparency (SAT) model. ?e ?rst level – Basic Information
– includes understanding the purpose, process, and performance of the system. ?e second level – Rationale – includes
being able to understand the reasoning process of the system. ?e third level – Outcomes – involves being aware
of future projections and potential limitations of the system. ?ey suggest that two main components are display of
information, and the display of uncertainty, but note that there are many considerations to take into account when
trying to communicate information to human users. For example, numerical probabilities can be confusing and may
need to be replaced by con?dence limits or ranges. ?is work indicates the importance of di?erent levels of assurances;
in some situations only a certain subset of information will be needed. A key limitation is that, generally, not all of
the elements of the SAT framework are available to AIAs at this time, this can be due to design limitations as well as
theoretical limitations (i.e. no method might exist to quantify the future performance of a model in an environment in
which it has never been deployed).
Dragan and Srinivasa [31] investigated how a robot could move in a ‘legible’ way, or in other words, make movements
that in themselves convey the intended destination. ?is kind of problem is important for situations in which a robot
and person are collaboratively working in close proximity to each other. ?ey investigate this within the context of
how quickly a human participant is able to predict the goal of a moving point, before the point actually reaches that
goal. ?ey found that legible motion does in fact improve the user’s ability to understand and predict where the robot
is trying to move. It is di?cult to classify this work because it does not directly address or consider human trust, but
it is clearly an explicit assurance making the AIA motion capability more predictable. Furthermore, they run human
experiments in order to validate that the calculated motions are in fact more interpretable to users. ?eir work focuses
mainly on the premise that some deviation from the most cost optimal trajectory makes motion more legible; this idea
can be extended in certain situations where humans rely on redundant or non-optimal behavior to predict outcomes,
although it is contrast to Wu et al. [138].
In a similar vein, Chadalavada et al. [17] investigated ways to make interaction between humans and robots more
natural (i.e. being more predictable and using common human methods for communicating especially non-verbal
communication) in se?ings where they occupy the same work space. ?eir approach was to have the robot project its
intended movements onto the ground in order to indicate where it would be moving. ?ey performed experiments in
Manuscript submi?ed to ACM
20 Bre? Israelsen
which participants were asked to answer questionnaires regarding how predictable, reliable, and transparent the robot
was when it was projecting its intentions, and when it wasn’t. ?ere was a signi?cant improvement in all measures
when the robot was projecting its intended movements. ?is is strong evidence for an assurance aimed at predictability.
Similarly, in Sza?r et al. [120, 121] they investigated using a quad-copter’s motion pa?erns, as well as signaling with
light, to help users more easily interpret the intended movements and actions.
Turning to the question of how explanations of robot reasoning can e?ect human-robot trust, Wang et al. [134]
performed an experiment in which a human and robot investigate a town; the robot has sensors that detect danger,
and will recommend that the human wear protective gear if it senses danger. However, the human can choose to not
accept the recommendation based on their trust in the robot, and the need to avoid the delay associated with using the
protective gear. ?e robot is able to pose natural-language explanations for its recommendations, as well as report
its ability. ?is was done by creating explanations generated by translating the components of the robot’s planning
model (the robot uses a partially observable Markov decision process (POMDP) planner) to natural language. In their
experiment the robot was able to translate every component of it’s POMDP which are: states, actions, conditional
transitions, rewards, observations, and conditional observation probabilities. ?is allowed it to make statements like: “I
think the doctor’s o?ce is safe”, or “My sensors have detected traces of dangerous chemicals”, or “My image processing
will fail to detect armed gunmen 30% of the time”. ?e translation process required pre-made templates to map natural
language, for example writing that observation 1 means dangerous chemicals.
During the experiment they used two levels of robot capability: ‘high’, and ‘low’; and three levels of explanations:
‘con?dence level’, ‘observation’, and ‘none’. ?ey found that when the robot’s ability was low, the explanations helped
build trust. Generally, con?dence level, and observation explanations had a similar e?ect on trust and both were an
improvement over no explanations. Whereas, when the capability was high, the explanations didn’t have a signi?cant
e?ect. ?is suggests that in some cases some kinds of assurances are overridden by other ‘stronger’ ones, speci?cally
the explanations (explicitly designed for a?ecting trust) were rendered useless by the high reliability (which can be
thought of as an implicit assurance in this case) of the AIA.
Also examining POMDP-based robot planning and actions, Aitken et al. [3] and Aitken [2] consider a formal model
of trust, and propose a metric called ‘self-con?dence’ that is an assurance for UGV that is using a POMDP planner
(in this case, for the road network application described earlier in Section 2.2). It is made of a combination of ?ve
component assurances: 1) Model Validity, 2) Expected Outcome Assessment, 3) Solver ?ality, 4) Interpretation of User
Commands, and 5) Past Performance. ?e components considered are fairly general, and applicable to most planners,
but would require new algorithms to be designed for those methods. Model validity a?empts to quantify the validity of
a model within the current situation. ?e expected outcome assessment uses the distribution over rewards to indicate
how bene?cial or detrimental the outcome is likely to be. Solver quality seeks to quantify how well a speci?c POMDP
solver is likely to perform in the given problem se?ing (i.e. how precise the solution can be given a POMDP description).
?e interpretation of commands component is meant to quantify how well the objective has been interpreted (i.e. how
sure am I that I was commanded to move forward). Finally past performance, is meant to add in empirical experience
from past missions, in order to make up for theoretical oversights.
Self-con?dence is reported as a single value between ?1 (complete lack of con?dence), and 1 (complete con?dence).
A self-con?dence value of 0 re?ects total uncertainty. Each of the component assurances would be useful on its own,
but the composite ‘sum’ of each factor is meant to distill the information from the ?ve di?erent areas, so that a (possibly
novice) user can quickly and easily evaluate the ability of the robot to perform in a given situation. Currently, only one
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 21
of the ?ve algorithms (Expected Outcome Assessment) has been developed, but there is continuing work on the other
metrics. As of the writing of this document, no human experiments have been performed to validate the usefulness of
the self-con?dence metric.
4.2.1 Summary. ?e research in this section focuses on what needs to be expressed as an assurance as well as how
to express it. One obvious take-away is that no AIA can express an explicit assurance unless it is able to ?rst calculate it,
or if it has been designed with assurances (i.e. a sticker on the outside saying ‘trust me’, although this would probably
be a weak assurance). Not all situations require the same kind of assurances, as discussed by [19], depending on the
task at hand di?erent assurances, such as high-level predictions of future performance or con?dence in executing a
motion, will be more appropriate.
As in the ?rst quadrant, we observe the critical role that human perception has on the e?ect assurances. Simply, an
assurance can only be e?ective if a user can perceive it. To this end the researchers investigate di?erent methods for
expressing assurances. ?is methods include visually displaying ‘summary data’ [96], displaying intended actions [17],
making movements legible [31], and using natural language in order to aid the users in perceiving the assurance [134].
Using explicit assurances is, unknowingly to some, an a?empt to directly in?uence the dimensions of trust from
Figure 6. Understanding how assurances a?ect the user’s ideas about the ‘competence’, ‘predictability’, and ‘situational
normality’ of an AIA in a certain situation is an important consideration, but perhaps only in certain situations. Here
researchers used metrics for tracking changes in the TRBs of a human user in lieu of a?empting to understand the
self-reported level of trust of the user, for example by measuring the frequency of a user switching control from
autonomous to semi-autonomous. Further consideration highlights the point that – unless the goal of the AIA is to
a?ect the user’s self-reported trust – TRBs are really the most appropriate metric that can be used. ?is is an idea that
began in ?adrant I, but that became more clear in this quadrant.
Most explicit assurances that have been created so far are static – they do not change in time or with context. For
example [134] and [3] consider giving calculated assurances to a user in the form of natural language feedback and
simpli?ed analysis of the task respectively. Generally, the state-of-the-art is to use assurances that are static. Given
the complexity of human-AIA trust relationships, and the diverse situations and user’s involved, the extension of
assurances to be more dynamic and adaptable in time, and in di?erent situations, is an important research direction
that is under-served at this point in time.
4.3 ?adrant III. (Explicit Assurances, Informal Trust Treatment)
4.3.1 Performance Prediction. Zhang et al. [140] are concerned with the performance of visual classi?cation systems;
particularly with the common occurrence of vision systems to fail ‘silently’, that is with no warning. ?ey suggest that
the two possible solutions are to: 1) analyze the output of the vision system to assess its con?dence, and 2) analyze the
input itself in order to assess the classi?er’s con?dence. ?ey pursue the second, with the goal of minimizing failures as
well as trying to verify the safety of a classi?er. ?ey argue that the second method is more desirable because it can be
applied to any classi?er. In essence they learn a model of the training error associated with di?erent images and then
use this to predict the error on test images. ?ey demonstrate their methods on several di?erent classi?cation tasks and
show that it predicts failure “surprisingly reliably”. Although, they make an assumption that similar inputs (by some
measure) will give similar outputs, which can be di?cult to quantify in some domains.
Gura?u et al. [49] (following from ideas in Churchill et al. [21]) consider the task of predicting the performance of a
robot’s navigation when using cameras for localization. ?eir aim was to have the robot operate autonomously only
Manuscript submi?ed to ACM
22 Bre? Israelsen
when it was con?dent that it could do so, otherwise it would request human assistance. To achieve this they make use
of GPS localized observations and ‘Experience Based Navigation’ (EBN) to identify which past experience matches the
current state of the robot. Using this approach they are able to reduce the number of mistakes that the robot makes
when navigating in a real scenario. ?is approach seems to be fairly sound in their suggested application, and they
claim that it is agnostic of classi?er, which should be bene?cial for use in other AIAs.
Kaipa et al. [60] also consider the con?dence of a visual classi?er, but with the application being a robot that is
picking an object out of a bin. To do this they utilize the ‘Iterative Closest Point’ (ICP) method to match which points in
a point cloud are likely associated with the object of interest in the bin. With that information the robot can then asses
how con?dently it can perform the task and if its con?dence is below some threshold it will request assistance from a
human. ?is method is quite similar to ‘Chi-square innovation hypothesis testing’ used for adaptive Kalman ?ltering
[8], and shares a key drawback, which is that it requires a precise model, and doesn’t necessarily indicate why the test
is failing. Regardless, if applicable they are quite useful for verifying that the AIA is ‘OK’.
Kuter and Miller [67] recognize that plans produced by hierarchical tasking networks can be fragile, and suggest that
the planner calculate its stability. In essence, how sensitive is the plan to uncertainties? ?ey suggest ‘counter planning’
and ‘plan repair’ so that the autonomous system can identify likely contingencies that might interfere with an existing
plan and then adapt the plan to account for those contingencies. If the system is able to adapt to contingencies then it
will have a higher ‘self-con?dence’. ?is allows humans to be able to understand the system be?er, and trust it more
appropriately.
Hutchins et al. [54] consider the fact that autonomous systems have components, and that those components
have ‘competency boundaries’. Which refers to the fact that di?erent sensors, planners, and actuators have di?erent
capabilities in di?erent conditions. For example a GPS has high competency in an open-air situation, but very low
competency inside of a tunnel or building. ?ey suggest that these competency boundaries be quanti?ed, and then
displayed to a user, so the user be?er understands the system’s capabilities and can trust it more. ?e main limitation
here is that they propose an expert-based Likert scale for quantifying competency boundaries, which, which lacks
formal foundations, and extensions to other domains. Both this work and that of Kuter and Miller are related to the
work of Aitken et al., but the later two do not explicitly relate their approaches to trust and TRBs. Although, Hutchins
et al. is the only paper of the three to consider the exact method by which the assurance should be communicated.
4.3.2 Interpretable Models. A well-known concept in machine learning and pa?ern recognition is the trade-o?
between accuracy and interpretability. Van Belle et al. [128] examine this issue in the context of making clinical decision
support systems for use in the medical ?eld. To this end they propose an ‘interval coded scoring’ system that imposes
a constraint such that each variable has a constant e?ect on the estimated decision-making risk (e.g. the Bayes’ risk
for classi?cation). ?ey demonstrate the method on two datasets, and show that their method can be visualized using
scoring tables and color bars. ?ey point out that their method needs to be extended in order to detect interaction
e?ects automatically, and to handle multi-class decision support systems. One reason why this approach is important is
because probabilities are notoriously di?cult for humans to interpret, and thus the reason to manipulate the output in
order to accommodate those who interact with the system.
Along these lines, Ruping [112] speci?cally asks how classi?cation results can be made more interpretable to those
who design and use classi?ers. He reviews several possible methods, and states that ratings by experts is perhaps the
most accurate way, although it is at the same time subjective. To address the accuracy-interpretability trade-o? he
investigates the use of a simpler global model, and a more complex local model (O?e [100] and Ribeiro et al. [107]
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 23
implement similar ideas as well). Figure 8 illustrates this idea on a simple example, the le? (global) model can be used
as a large scale interpretable model, while the more complex local model can be used as necessary when more precise
understanding is required. ?is allows simple interpretation of global properties, but more complex local models to
maintain accuracy. While his focus was on classi?cation, the methods could also be useful in regression as well. He
spends time investigating how the global and local models should be learned in a principled way, and demonstrates this
approach on several black-box and white-box classi?ers.
x x xx
x
x
x
x x
o
o o
o
o oo
o o
o?
x x xx
x
x
x
x x
o
o o
o
o oo
o o
o
o
x
Fig. 8. Example of simple global model on the le?, and on the right a more complex local model that can be used for interpretability
when more detail is necessary.
Later, Van Belle and Lisboa [127] address the challenge that some of the highest performing ML models are too
complicated to be interpreted. ?ey investigate di?erent ‘interpretable’ and visualization methods in order to understand
what opportunities they ?nd. ?ey suggest that there are three methods that help ascertain the level of interpretability
and potential utility of models: 1) Map to domain knowledge, 2) Ensure safe operation across the full operational range
of model inputs, and 3) Accurately model non-linear e?ects (compare to categories proposed by Lipton [79]). ?ey
analyze some of the existing methods (nomograms, rule induction, graphical models, data visualization) and point out
their weaknesses. ?ey ?nish by pointing to more recent research in sparse and interpretable models, and suggest
that it is a promising line of research. In summary, they discuss that each ‘interpretable’ method has its bene?ts and
weaknesses and there is no method that clearly out-performs any other.
Similarly, Caruana et al. [14] are interested in predicting the probability that a patient will be re-admi?ed to the
hospital within thirty days. ?ey also mention the trade-o? between accuracy and interpretability, and propose a
generalized additive model with pairwise interactions (GA2M) model. A generalized additive model (GAM) is generally
thought of as interpretable as the e?ects of individual variables can easily be quanti?ed; however GAMs su?er from low
accuracy. In a comparison with other methods, they show that adding pairwise interactions allows the GA2M model to
be as accurate as the current less interpretable methods, while still maintaining reasonable interpretability.
In a similar vein, Choi et al. [20] modify a recursive a?ention neural network to remove recurrence on the hidden
state vector, and instead add recurrence on the doctor visits and diagnoses. In this way the model is able to predict
possible diagnoses in time, and a visualization can be that that indicates the critical visits and diagnoses that lead to
that prediction. ?ese methods are promising because it restructures an advanced learning model in a way that useful
information can be extracted. ?is method seems very promising, however, their architecture is dependent on problems
Manuscript submi?ed to ACM
24 Bre? Israelsen
with similar properties, meaning that their approach would need to be re-implemented by an expert to be used in
another situation.
Abdollahi and Nasraoui [1] construct a conditional restricted Boltzmann machine (RBM) in order to create a
collaborative ?ltering algorithm, that can suggest ‘explainable’ items, while maintaining accuracy. ?e challenge is that
the RBMs are very accurate collaborative ?ltering algorithms, but di?cult to understand. ?ey make a more explainable
version by adding an additional ‘explainability layer’ which indicates the explainability of an item to a certain user.
?is method shows how adding an additional element in a model can help add explainability, and this seems to be
somewhat of a theme in this section.
Ridgeway et al. [108] recognize that boosting methods, or classi?ers with voting methods, are very accurate but
not interpretable. ?ey propose boosting by ‘weight of evidence’ (WoE), where WoE refers to having a weight that
indicates whether the observation is positively or negatively correlated to the class. Each weight can then be used
to gain some understanding about how the observation a?ects the classi?cation. ?ey demonstrate its utility using
multiple experiments that it has performance on par with AdaBoost, and with a Naive Bayes classi?er. Again, their
approach gets at the ability of adding in interpretability components into a model can yield a more interpretable model,
while losing very li?le accuracy.
Huysmans et al. [55] investigate decision trees, decision tables, propositional if-then rules, and oblique rule sets in
order to understand which is the most interpretable and perform an experiment to identify which method works most
e?ectively. ?e experiment involved interpreting a model and answering questions about what the correct classi?cation
would be. ?ey made several interesting observations. First, they con?rmed that a model with larger representation
leads to lower answer accuracy responses. ?ey found that overall decision trees and decision tables were the most
interpretable, but that di?erent tasks made the tree or table more desirable (the layout of the data in a table can be
superior for certain ‘look-up’ tasks).
One drawback with decision trees is that the rules can get very complicated in a large tree. Park et al. [102] investigate
how to make rules in decision trees more ‘intuitive’. ?ey propose a method that learns chains of decisions that together
increase the ratio of positive class. ?ey present a method that is called ?-Carving decision Chain (ACDC), and say
that it is a greedy alternative to a general framework known as RuleLists (Wang and Rudin [133]). Similarly, Jovanovic
et al. [58] use ‘Tree-Lasso’ logistic regression with domain knowledge. Speci?cally they use medical diagnostic codes to
group similar conditions and then use ‘Tree-Lasso’ regression that uses that information to make a more sparse model.
Zycinski et al. [141] also use domain knowledge to structure the data matrix before feature selection and classi?cation.
While the work of Huysmans et al. gives some indication about how to choose classi?ers for interpretability, Park
et al. points out that to gain real interpretability in complex tasks expert knowledge is still needed to make sense of
complicated features.
Faghmous and Kumar [34] argue that ‘theory guided data science’ is necessary in science applications; for instance,
when studying environmental e?ects, black-box models are of li?le use. Instead, in order to gain insight they need
a theoretical framework, to help highlight causation. Similarly, Morrison et al. [93] address the situation where an
analytical model is available but imperfect. ?ey use a chemical kinetics application where the theoretical reaction
equations are well known, and then add a ‘stochastic operator’ over the top of the known model to account for
uncertainties. One drawback for these methods is that they require a lot of domain knowledge. However, in the areas
of science and engineering where physical models are well understood this type of approach could be useful to indicate
the performance of the system with regards to the theoretical model.
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 25
4.3.3 Visualization and Dimensionality Reduction. Intuitively, it might make sense to simply show users the inputs,
raw data, and/or intermediate processing steps that led to a particular AIA behavior. In many real applications, however,
there are too many individual variables for a human to a?end to. In such situations, dimensionality reduction (DR)
and visualization are tools that can be used to help make a model or data easier to understand. Venna [130] discusses
dimensionality reduction as a tool for data visualization for ML, and reviews many linear and non-linear projection
methods. Vellido et al. [129] also discusses the importance of DR for making ML models interpretable.
In an interesting application to time series data, Kadous [59] asks how comprehensible descriptions of multivariate
time series can be learned, with the end goal of interpreting Australian sign language. He focuses on reducing the
feature space fed into a learning algorithm (i.e. the data was gathered using an instrumented glove), and does so using
‘parameterized event primitives’ (PEPs), which are commonly occurring events or pa?erns that are expected to occur.
He shows that his method reduced test error while also having more comprehensible features. It seems likely that
parameterized primitives might be automatically learned. While an interesting application, this would require expert
knowledge to extend to other domains. Although it does seem likely that PEPs could be automatically identi?ed in
some way.
4.3.4 Explanation. In the context of POMDP planning, Lomas et al. [82] recognize that in order for humans to
appropriately trust robots they must be able to predict their behavior, and the robot must be able to communicate in
order for that to happen. To this end they present the Explaining Robot Actions (ERA) system that interfaces with a
model that represents semantic and physically based information, along with other factors. In essence the ERA is a
translator between the planner/model and the human. ?is approach depends on a layered world model that includes
semantic information as well as the physical model, but is promising in the respect that the ERA is a separate system
that queries the robot.
With regards to expert systems Swartout [119] examines explanation methods. He noted that “trust in a system is
developed not only by the quality of its results, but also by clear description of how they were derived”. O?en the data
(or knowledge) used to train an expert system is not kept in the system for later use. He proposed a method called
‘XPLAIN’ that not only describes what it did, but why it did it. It does this by using description facts of the domain
and prescriptive principles simultaneously; in essence it learns how to make decisions and how to explain them at the
same time. ?is approach is meant for a structured problem de?ned by a domain model and the domain principles, in
situations where problems are based mostly in data, this approach would not be applicable.
Rouse [110] asked how computer-based decision support systems should be designed in order to help humans cope
with their complexity. He suggests that methods need to be designed so as to provide di?erent kinds of information
which include: pa?erns vs. elements, and current vs. projected outcomes or states. ?is work is important in pointing
out that assurances also depend on what the role of the human is and what information they need.
?is was also investigated to some extent by Wallace and Freuder [132] in their work concerning explaining outcomes
of constraint solvers. ?ey discuss how to distinguish between ‘good’ and ‘bad’ explanations, or those explanations
that facilitate or detract from the user’s ability to understand how the explanation actually applies to the solution
being explained. ?ey critically ask the question of how explanations should be presented to users (something that
Kuhn [65] explores more formally with regard to framing of uncertain information in decision making). An important
consideration is that constraint solvers don’t take uncertainty into account, but instead are deterministic. ?is is not
the case in many of the more adaptable and capable AIAs.
Manuscript submi?ed to ACM
26 Bre? Israelsen
Lacave and D??ez [68] revisit some of these ideas from the perspective of explaining Bayesian networks. ?ey are
concerned with how and why a system reaches a conclusion. ?ey present three properties of explanation: 1) content
(what to explain), 2) communication (how to explain), and 3) adaptation (how to adapt based on who the user is). It is
not possible to cover all of the ideas that they present in their paper, but they are key to the idea of designing assurances.
Some key points are that they highlight the di?erences between explaining evidence, the model, or the reasoning.
?ese are three key considerations in making assurances. ?ey also discuss whether an explanation is meant to be a
description or for comprehension, as well as whether explanations need to be on a macro or micro scale (as mentioned
by Ruping). ?ey also consider whether explanations should occur by two-way interaction between system and user, by
natural language interaction, or by probabilities. Finally, when considering adaptation, they hit on another key point of
assurances, which is that in general application not all users will require (or desire) the same kinds of assurances. ?is
paper points out many challenges and considerations in designing assurances, and illustrates that, as with the ‘No free
lunch’ theorem, there is no single ‘best’ assurance that will address every possible situation.
4.3.5 Model Checking. While assurances behind reasoning processes can be useful in many situations, they are
not trustworthy in and of themselves if the models or assumptions they are based on are ?awed to begin with. ?us,
there is also great interest in providing assurances that the models and assumptions underlying said AIA reasoning
processes are, in fact, sound. Laskey [71] – with the intention of helping users of ‘probability based decision aids’ by
communicating the validity of the model – notes that it is infeasible to perform a decision theoretic calculation to decide
if revision of the model is necessary. She then presents a class of theoretically justi?ed model revision indicators which
are based on the idea of constructing a computationally simple alternate model and then to initiate model revision if
the likelihood ratio of alternate model becomes too large.
Zagorecki et al. [139], discusses the ‘surprise index’ introduced by Habbema [51], which is the likelihood of a certain
measurement given a speci?c model, which applies nicely to Bayes-nets that have probabilistic descriptions. ?e
major ?aw of the surprise index is that it is computationally infeasible due to the possibility (likelihood) of being a
non-analytic distribution. Zagorecki et al. suggest an approximation by using the log-normal distribution to approximate
the distribution of values in the joint probability distribution. Another challenge is knowing what a ‘good’ value for the
surprise index is since knowing the maximum likelihood of a non-analytic distribution is not an easy problem.
Ghosh et al. [42] present a method, in the framework of a practical self-driving car problem, called Trusted Machine
Learning (TML). ?e main approach of TML is to make ML models ?t constraints (be trustable). To do this they utilize
tools from formal methods to provide theoretical proof of the functionality of the system. ?ey present ‘model repair’
and ‘data repair’ that they can utilize when the current model doesn’t match the data, at which point the model and
data can be repaired and control can be replanned in order to conform with the formal method speci?cations. One
challenge that presents itself is how to identify the ‘trustable’ constraints, this returns a lot of responsibility to the
designer to foresee all possible failures, which is a strong assumption.
4.3.6 Human-Involved Learning. Another possible way to make system assure a human user is to use the human in
the learning process. Freitas [37] addressed this point with regards to discovering ‘interesting’ knowledge from data, by
comparing two main approaches. Given such large datasets, human users require assistance from complex systems in
order to ?nd pa?erns and other ‘interesting’ insights. He mentions ‘user-driven’ methods that involve a user pointing
out ‘interesting’ templates, or in another method general impressions in the form of IF-THEN rules. He compares these
methods to other ‘data-driven’ methods that have been used, and cites other research that suggests that data-driven
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 27
approaches are not very e?ective in practice. ?is is a cautionary tale that many times engineering methods to assist
humans are not as e?ective as we would like to believe. Although, the ‘user-driven’ approach may not fair any be?er
when compared over many users, as each user will likely have di?erent preferences. Chang et al. [18] also consider a
similar, scaled up, ‘user-driven’ approach called ‘Revolt’ that crowd-sources the labeling of images. It is able to a?ain
high accuracy labeling, while also exploring new or ambiguous classes that can be ignored with traditional approaches.
4.3.7 Explicit Assurances via Formal Methods. Validation and Veri?cation (V&V) typically refers to using formal
methods to guarantee the performance of a system within in some set of speci?cations. Not all practitioners are
aware that V&V provides ways to assure users; arguably some have this idea in their mind, but only a few V&V
researchers consider how to communicate the results of analysis via formal methods to users. A prime example is given
by Raman et al. [105], who developed a way by which a user can provide natural language speci?cations to a robot and
a ‘correct-by-construction’ controller will be built if the speci?cation is valid. Otherwise, the robot will provide an
explanation about which speci?cation(s) cause failure. ?ey study this with the goal of implementing the system on
a robot assistant in a hospital. ?eir method involves parsing natural language input (such as: “Go to the kitchen”),
and converting that to linear temporal logic (LTL) that represents a task speci?cation. ?is is then used to construct
a controller if possible, otherwise the ‘unrealizable’ speci?cations need to be communicated back to the user. ?is
approach is promising in that it presents a way to communicate that a speci?cation cannot be met, although it does not
formally account for e?ects on user trust or TRBs in formulating explanations. ?e expression of assurances is also
asymmetrically limited to cases where the robot cannot meet the speci?cations.
4.3.8 Summary. Many of the approaches above focus on informing the user how the model/logic works; there is also
some focus on predicting future performance. Some AIAs do not have the capability to implement these approaches.
Or, the type of approach – such as informing the user how a deep neural network works – simply does not have a
satisfactory answer to date. As another example, methods for quantifying the amount of uncertainty in a planner have
not been developed for all planning algorithms. Likewise, there may not be satisfactory ways of visualizing certain data.
?e approach of using a more simplistic global model paired with more accurate local model is fairly prominent
([100, 107, 112]). ?is idea is similar to how human experts explain approaches to non-experts; as a non-expert desires
to know more about a speci?c concept of a complex idea, more detailed explanation can be provided. ?is is promising
because it is also extensible to di?erent types of users. In particular [107] is designed to work with any classi?er, and
[140] has the same goal as well. Of course the trade-o? of accuracy for ?exibility will always exist between more
general approaches and specialized approaches.
?e underlying hypothesis here is that many of the methods should a?ect the user’s perception of the ‘competency’,
and ‘predictability’ of the AIA, or the ‘situational normality’ of the task being performed. However, none of this has
been tested by experimentation that gathers self-reported changes in trust. Similarly, there has been no formalization
of how the e?ects of assurances on TRBs might be quanti?ed in di?erent applications. In essence this work is only
addressing at a small subset of important considerations for designing assurances, or the subset that includes the
methods by which to calculate the assurance. ?e experimental testing of the e?ects of the proposed assurances on
both self-reported trust, and TRBs remains open. Indeed, the research in this section is mainly focused on what AIA
engineers and designers think needs to be explained, or what assurance they think users should have. ?ese ideas
de?nitely have merit, but need to be tested to identify if they are e?ective, to what extent they are e?ective, and whether
there is a more e?ective, or more e?cient way to achieve similar results.
Manuscript submi?ed to ACM
28 Bre? Israelsen
4.4 ?adrant IV. (Implicit Assurances, Informal Trust Treatment)
4.4.1 Safety and Learning under nonstationarity and risk aversion. While a fairly high-level treatment, Amodei et al.
[4] are concerned with ‘AI safety’, which is in essence how to make sure that there is no “unintended and harmful
behavior that [emerges] from machine learning systems”. Given this de?nition, much of the paper discusses concepts
that are critical to AIA assurances. Among the more directly applicable topics in the scope of this paper are: safe
exploration (how to learn in a safe manner), and robustness to distributional shi? (a di?erence between training data
and test data). ?ey also discuss designing objective functions that are more appropriate. To restate more concretely,
there is a need to design objective functions that more accurately re?ect the true objective function. A popular example
(roughly summarized here) from Bostrom [12] is a robot that has an objective of making paper clips, it then decides to
take over the world in order to maximize its resources and ability to make more paper clips. ?is highlights the point
that sometimes over-simplistic objective functions can result in unintended and unsafe behaviors.
Generally, stationary data (data whose distribution does not change a?er training) is assumed in supervised ML.
Sugiyama et al. [118] considers what to do when there is ‘covariate shi?’ (or when both training and test data change
distribution, but their relation does not change between training and test phases), and ‘class-balance change’ (where
the class-prior probabilities are di?erent in training and test phases, but the input distribution of each class does not
change). ?ey design and present tools that help diagnose and treat those conditions (this is follow on work for some of
what is presented in ?in?onero-Candela [104] where they consider dataset shi?). A key approach is to use importance
sampling, which involves weighting the training loss by the ratio between the probability of the test data and that of
the training data.
Had?eld-Menell et al. [52], in considering an AI safety problem, address the problem of verifying that a robot can be
turned o?, even when it might have an objective function that indicates that disabling the ‘o?-switch’ will reduce the
chance of failure. ?is kind of scenario, or something similar, can easily occur with a su?ciently sophisticated and
capable AIA, and a complex enough set of objectives that might result in unintended consequences. ?ey propose
that one objective would be for the robot to maximize value for the human, and to not assume that it knows how to
perfectly measure that value.
Da Veiga and Marrel [25] discuss a form of safety where they are concerned with nonparametric classi?cation and
regression with constraints. More speci?cally, they are concerned about learning Gaussian process (GP) models with
inequality constraints, and present a method to do this by using conditional expectations of the truncated multivariate
normal distribution (or Tallis formulas). ?is is not the only work that references learning with constrained GPs. It is
also not the only work that considers constrained modeling, but it would take too long to review all of those papers. ?e
main claim here is that constrained models are a way to guarantee the properties of a model within some speci?cations.
Garc?a and Ferna?ndez [40] perform a survey about safe reinforcement learning (RL). Safety in RL can be critical
based on the application, such as an aerospace vehicle that can cost several thousands of dollars. ?ey state that
there are two main methods: 1) modifying the optimality criterion with a safety factor, and 2) modi?cation of the
exploration process through the incorporation of external knowledge. ?ey also present a hierarchy of approaches and
implementations. Some approaches used when modifying the optimality criterion are worst-case, risk-sensitive, and
constrained criterion. Whereas, modifying the exploration process is done through using external knowledge, and as
well as using risk-directed exploration. Safe RL is a particularly important area that requires assurances, as the systems
are designed speci?cally to evolve without supervision.
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 29
As one example, Lipton et al. [80] design a reinforcement learner that uses a deep Q-network (DQN) and a ‘supervised
danger model’. Basically, the danger model stores the likelihood of entering a catastrophe state within a ‘short number
of steps’, this model can be learned by detecting catastrophes through experience and can be improved over time. In
this way they show that their method, they call ‘intrinsic fear’, is able to overcome the sensitive nature of DQNs. ?ere
is a clear limitation of the danger model, in that it does not contain useful information until a catastrophe is detected.
Curran et al. [24], in a more speci?c application, asks how a robot can learn when a task is too risky, and then avoid
those situations, or ask for help. To do this, they use a hierarchical POMDP planner that explicitly represents failures as
additional state-action pairs. In this way the resulting policy can be averse to risk. ?ey say that this method can be
especially useful when optimal actions are not straight-forward, and they state that it can use any reward function. It
seems that this method might su?er from some of the typical problems of POMDPs, which are computational complexity
in high-dimensional state spaces.
4.4.2 Active Learning. Paul and Newman [103] is concerned with whether a robot can improve its own performance
over time, with the goal of ‘life-long learning’. ?ey use ‘perplexity’ which is a method ?rst introduced in language
models and adapted to work with images. Perplexity, in their application, is a measure that indicates the uncertainty
in predicting a single class. Over time, the most perplexing images are stored and used in expanding the sample set.
?is work is interesting for application in assurances because the ability to quantify something that is perplexing is a
predecessor to being able to communicate that to a human user.
Recently, there have been several papers that a?empt to use Gaussian processes (GPs) as a method to actively learn
and assign probabilistic classi?cations (see Berczi et al. [11], Dequaire et al. [27], Grimme? et al. [44, 45], MacKay
[84], Triebel et al. [123, 124, 125]). ?e applications surveyed here are all mainly related to image classi?cation and
robotics. As with perplexity-based classi?ers, the key insight is that if a classi?er possesses a measure of uncertainty,
then that uncertainty can be used for e?cient instance searching, comparison, and learning, as well as reporting a
measure of con?dence to users. ?e key property of GPs that makes them an a?ractive for this purpose is their ability
to produce con?dence/uncertainty estimates that grow more uncertain away from the training data. ?at is, GPs have
the inherent ability to ‘know what they don’t know’, and this information can be readily assessed and conveyed to
users, even in high-dimensional reasoning problems. ?is property of GPs has also found great use in other active
learning applications, such as Bayesian optimization (see Williams and Barber [137], Snoek et al. [116], Brochu et al.
[13], and Israelsen et al. [57]).
4.4.3 Representation learning and Feature Selection. Another promising ?eld of research is related to learning
representations of data and selecting data features. ?ese two topics are surveyed by Bengio et al. [9] and Guyon
and Elissee? [50] respectively. From some of the discussion of interpretable models in section 4.3.2 we ?nd that
representation is important for making interpretable models. Having appropriate representations (i.e. like the ones
humans use and understand) is a large step forward in making assurances for humans.
For instance, in their work related to interpreting molecular signatures Haury et al. [53] investigate the in?uence
of di?erent feature selection methods on the accuracy, stability and interpretability of molecular signatures. ?ey
compared di?erent feature selection methods such as: ?lter methods, wrapper methods, and ensemble feature selection.
?ey found that the e?ects of feature learning greatly in?uenced the results.
As another example, Mikolov et al. [91] studied how to represent words and phrases in a vector format. Using
this representation, they are able to perform simple vector operations to understand similar words, and the relative
Manuscript submi?ed to ACM
30 Bre? Israelsen
relationships learned. For example the operation airlines +German yields similar entries that include Lu f thansa. ?is
type of representation encodes information that can be checked and understood by humans.
How can human understandable features and representations be discovered? ?is is still an open question. ?e main
question in the representation learning world is how to ?nd the best representations, not necessarily the representation
and features that are most human. ?is is not surprising because human representations and features are not necessarily
optimal, and AIAs are being designed to be optimal using other objective functions (arguably more appropriate functions,
if humans don’t need to understand what is going on).
4.4.4 Summary. ?e literature surveyed in this section is not exhaustive, nor could it reasonably ever claim to be.
One thing that should be clear is that in every ?eld where designers want to ensure reliable and correct application of
an AIA, there will be assurances that are created. ?e disciplines selected in this paper are a subset that are aligned
with the author’s interests in unmanned vehicles.
?e work in this section is easily distinguished from that in ?adrant I because it does not discuss trust in any way.
However, it is only subtly di?erent from that in ?adrant III. ?e research in ?adrant III is explicitly focused on things
like interpretability and explanation by direct statement of the authors. Conversely, the research found in this quadrant
is only related to trust by those who are familiar with the underlying concepts in this paper. ?is research is created
with the intent of making the AIAs intrinsically more safe, aware of reasoning processes, having be?er representations
in some way, and others. ?is group unintentionally, creates foundations for trustworthy AIAs. Here are found the
researchers who created AIAs with properties, like reliability, that can then be investigated by those who formally
acknowledge human-AIA trust in ?adrant I. ?ese are the methods can be turned into explicit assurances by designers
who intentionally do so.
5 A SYNTHESIZED AND REFINED VIEW OF ASSURANCES
From the review of ?adrants I. through IV. of the formal/informal, explicit/implicit plane, we are able to ?nd some
insights with respect to assurances and can discuss them in a more comprehensive way. Using insights from the survey
a re?ned version of Figure 1 can be constructed. Figure 9 incorporates all details from Section 2 as well as adding some
insights from the survey that give direction about the design of assurances in human-AIA trust relationships.
Below we synthesize and discuss the design of explicit assurances in this more detailed framework – some of these
insights might also apply to implicit assurances, but implicit assurances will not be directly discussed in this paper.
5.1 Calculating, Designing, and Planning Explicit Assurances
Recall that an assurance is de?ned as any behavior or property of an AIA that a?ects a user’s trust, therefore an explicit
assurance is any assurance that was consciously implemented/applied by the designer before deployment of the AIA
with the express intention of in?uencing a user’s TRB/trust (whether or not the means for doing so conforms to a formal
trust concept). As such, it is possible to design assuring properties into the system a priori. It is likewise possible to
design assuring behaviors into an AIA. From the literature, there are some high-level ideas that surround the calculation
of assurances:
• ?antifying Uncertainty
• Reducing Complexity
• Assurance by Design
• Planning Strategies of Assurance
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 31
AIA AIA Assurances
Explicit 
Assurances
Implicit 
Assurances
Expression of 
Assurances
Intrinsic 
Properties
User
Trust
User Trust-Related 
Behaviors (TRBs)
Perception of 
Assurances
Actions
Appropriate 
Use Misuse
Disuse Abuse
AIA 
Capabilities
Mediums, Methods, 
and Efficacy
Observing 
Effects of 
Assurances
Calculating,
Designing,
and 
Planning 
Assurances
Fig. 9. Detailed extension of Figure 1. The AIA, User , and User TRBs blocks are defined as discussed in Section 2 (with the exception
of the ‘Perception’ blocks added to the AIA and User boxes). The AIA Assurances box has been filled using insights from the surveyed
material. The boxes that are greyed out will not be discussed in this section.
?antifying Uncertainty. Being able to quantify the di?erent kinds of uncertainty in the AIA is necessary before
a?empting to express that uncertainty to a human user. ?ere are several di?erent kinds of uncertainty that might be
considered such as uncertainty in sensing, uncertainty in planning, and uncertainty in locomotion. ?e general idea is
that a model or method needs to be incorporated in the AIA that will represent the di?erent kinds of uncertainty to the
human user in some way. A human user could use such information to inform their trust in the ‘situational normality’,
‘competence’, and/or ‘predictability’ of the AIA.
In the surveyed literature we have seen the following main approaches to do this. In some cases uncertainty is
already represented intrinsically by the algorithms and/or models being used in the AIA. Some use the built-in statistical
representations of transitions and observations as the basis of quantifying uncertainty in a POMDP robot. ?is is an
approach that has straightforward analogs in systems that use algorithms and/or models that inherently consider
uncertainty.
Models and methods that intrinsically contain or represent uncertainty are frequently available. However, even
when that it is the case – such as with POMDPs featuring transition probabilities and observation probabilities – there
are types of uncertainties that may still not be considered. For example, there are further metrics of uncertainty beyond
those intrinsically captured by a planner [3, 66]. Using the UGV road-network problem as an example, if the UGV
calculates a distribution over possible outcomes, how favorable is that distribution? Or, given a certain road-network
what kind of accuracy can be expected from the POMDP solver that the UGV is equipped with? Generally these
considerations might be described as addressing ‘uncertainties in application’, which are those uncertainties that arise
when trying to apply certain algorithms and models in various environments.
Perhaps the most obvious (but not necessarily simple) approach is to quantify the uncertainty of a classi?er, i.e. its
probability of error in making a decision (regression methods have analogous approaches). Some have approached this
problem by using a GP model for classi?cation [49]. In this way they, in essence, construct a model from empirical
Manuscript submi?ed to ACM
32 Bre? Israelsen
training data and used that model to quantify uncertainty in di?erent test scenarios. In contrast, we might quantify the
uncertainty of a classi?er based solely on the input itself [140]. Of course these methods share common drawbacks of
being solely supervised learning approaches, however even in the unsupervised domain such as reinforcement learning,
methods for avoiding highly uncertain (un-safe) rewards use some form of external expert knowledge (see [40, 80]).
Uncertainty can be easier to assess if some kind of oracle, or reference is available for comparison. ?antifying the
similarity between the empirical experience and the available reference can be a measure of uncertainty [60]. Of course
this approach loses its appeal when a truth model isn’t available. ?is shouldn’t detract from the intent of ?nding some
kind of reference (truth or otherwise) in which the reasoning, sensing, and other processes of an AIA can be compared
to evaluate uncertainty. ?e evaluation of statistical models involves a very similar concept: given a statistical model
as a reference does current empirical experience support or detract from the hypothesis that the model is still valid
[42, 72]? ?ese approaches can quantify the degree to which the statistical models are still true, and this measurement
can be used as an indication of uncertainty.
Generally, the capability of quantifying uncertainty enables an AIA to be able to express assurances related to the
‘situational normality’, ‘competence’, and ‘predictability’ of the system in a given situation. One might imagine that,
in the UGV road-network problem, the UGV expressing high uncertainty in its plan would in?uence the competence
component of the user’s trust. Conversely, if an uncertainty measure is not available the user might take this as an
implicit assurance that the AIA is perfectly con?dent, or based on the user’s experience they might conclude that since
all AIA plans have been ?awed in the past, the plan of this AIA must be ?awed as well.
Reducing Complexity. Many researchers have a?empted to remove complexity from the models and logic of the AIA
to make the methods more interpretable (or comprehensible, or explainable, . . . ) to a human user. As with quantifying
uncertainty, making an AIA more interpretable can also inform a user’s trust regarding the ‘situational normality’,
‘competence’, and ‘predictability’ of it. Of course this presupposes that many of the methods used by AIAs are ‘complex’
by some measure, we claim that the fact that experts are required to understand some of the methods (and even then it
may not be totally possible) proves this supposition. Complexity only exists in the presence of some reference frame,
which is the designer’s in this case. Generally complexity is said to increase with the number of variables, steps of
reasoning, the size of data, etcetera.
In practice, reduction of complexity has been addressed by approaches as simple as ?nding summary statistics, or
calculating averages [95, 96]. ?is can also be accomplished by computing heuristic measures which reduce many
complex goals into more manageable pieces[3]. Creating variable ?delity models is another way by which complexity
can be reduced (and increased when necessary). A?empts should be made to make/discover/learn models with scalable
interpretability based on given criteria like required depth of understanding, level of expertise, and time to gain
understanding.
One might focus on creating and using models that are inherently more interpretable to humans (i.e. [14] and
others). ?is could include constraining the feature space to be more simple, reducing dimensionality, learning more
understandable features, and theoretically founding the models (i.e. interpretable science).
While it is possible that there are inherently interpretable models that can be designed that can compete with
other non-interpretable models, we believe that this is not the best long-term approach to reducing complexity; this is
primarily due to the lack of scalability for engineers and scientists to frequently design new algorithms. Investigating
methods that generate explanations from non-interpretable models is a more promising direction. ?e main reason is
that the idea of interpretable models is not well-de?ned and, in reality, doesn’t exist as a single tangible goal. Instead
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 33
there is a continuum of interpretability that is based on the complexity of the problem, the time required for a user to
interpret (i.e. a few seconds or months of study), the expertise of the user, and others. Investigating the generation of
interpretations and explanations that are user speci?c and model agnostic would be the best of both worlds. ?ese
ideas are much more aligned with the e?orts of [112] and others who seek to use models with scalable resolution and
accuracy.
Assurance by Design. No ma?er how much engineers like to think about automating everything, realistically a
human will need to be involved at some level of assurance design for the foreseeable future, if only because the main
pursuit of human-AIA trust directly involves a human. ?e above two approaches alone (quantifying uncertainty, and
reducing complexity) can largely use existing methods, however some researchers directly engineered their methods
and models in the AIA to be more meaningful to humans.
One approach is pu?ing a human in the learning process, which essentially modi?es the objective function of the
learning algorithm [37]. In essence the objective function would then be based on a large set of human preferences
(and biases). ?is kind of approach is promising, in that it can be used to encode many human qualities that cannot
be easily quanti?ed, or even explained. However, there are trade-o?s that can be undesirable in many situations as
well. We o?en use designed, objective, learning algorithms to avoid human biases. It is interesting to note that using a
human in the loop can o?er more interpretability to the result of a learning process, while at the same time making the
learning process itself less procedural.
One source of discord between what humans expect and what AIAs actually do (i.e. less predictable AIAs) is a poorly
designed objective function [4]. ?is might be referred to as myopic objectives, when an AIA focuses on a speci?c
objective to the extent that a human can no longer relate to the objective of the AIA (and will be correspondingly
surprised by its actions). ?is suggests that signi?cant time may be required to design objectives that align with those
of humans, this alignment will automatically make the AIA more predictable, and competent in the user’s eyes.
It is also possible to modify standard learning approaches (like the ones discussed in the previous section), in such a
way as to make the methods inherently more assuring to a human. For example one might restructure a neural net
architecture, or a decision tree with the sole purpose of making it more interpretable [1, 20, 58]. It is di?cult however,
because it is not always clear how this can/should be done.
Planning Explicit Assurances. Planning assurances is critical when trying to a?ain desired TRBs from a human user.
In this context when we say ‘planning explicit assurances’ we mean formulating a plan for the expression of assurances
over time, with the goal of more e?ectively and appropriately expressing assurances to a human user. Having said this
we recognize that planning is not a capability available to all AIAs. In cases where AIAs don’t have the ability to plan,
they may be designed beforehand with some kind of static plan of assurance. Otherwise, more advanced AIAs might
take into account TRBs to plan an assurance strategy to assist the human to use it appropriately.
When planning assurances the AIA must be able to account for limitations of users, and its own limitations in
expressing assurances. For example a user may not be able to understand the necessary information needed to use the
AIA more appropriately. Also, the AIA may need to take a long-term strategy to teach the user, as opposed to only
presenting the user with canned, static, assurances. Some of the important user considerations will be discussed further
in Section 5.2.
One must ask whether the human user can correctly process the information received. ?is is perhaps most easily
illustrated by considering a non-expert user who cannot understand highly technical assurances regarding the AIA.
Manuscript submi?ed to ACM
34 Bre? Israelsen
However, less trivial manifestations may be troubling, such as the existence of bias in the perception of assurances.
?is will be addressed further in Section 5.2.
?is topic is nearly unexplored in the context of human-AIA trust relationships. However, there are several fairly new
programs that are interested in this question (i.e. explainable arti?cial intelligence (XAI) [47], and assured autonomy
[98]). Assuming an AIA can give assurances there are important questions like: what is the best way to present them?
How can they be adapted for di?erent kinds of users? How can the AIA teach or tutor the human over time? ?is is a
large gap in the current assurances landscape, answers to these questions are critical to designing more robust and
e?ective assurances.
5.2 Expression and Perception of Assurances
Expression and Perception of assurances have been combined in this section because they share several critical aspects.
?e key points to be considered in design of assurances are:
• Mediums
• Methods
• E?cacy
For an explicit assurance the medium, and method of expression must be selected taking into consideration the
limitations of the AIA. Here medium denotes the means by which an assurances is expressed, this could be through
any of the senses by which humans perceive, such as sight, sound, touch, smell, and taste. ?e method of assurance
is the way by which the assurance is expressed. An example may help: a plot may be conveyed through sight in the
typical way, or through spoken language (for example when communicating to a blind person); in this case the plot
is the method, and sight or sound are the di?erent mediums through which it can be communicated. An AIA might
be limited in methods of expression because it does not have a display, or a speaker. In that situation how is the user
supposed to receive an assurance?
A designer must also consider whether a human can perceive the assurances being given. If so, to what extent is
the information from the assurance transfered, or how e?cacious was the assurance? A few examples include: an
AIA giving an auditory assurance in a noisy room and the user not hearing it (such as an alert bell in a factory where
the workers use ear-plugs), or an AIA a?empting to display an assurance to a user that has obstructed vision. If an
assurance is not expressed, or not perceived by the user, it is useless and has no e?ect. For example, an AIA may
have the ability to store data about its performance, and compute a statistic regarding its reliability, but if it cannot
successfully express (or communicate) that information in some way, the information is useless.
A user will always have some kind of TRB towards an AIA (if only to choose to ignore the AIA). In the absence of
explicit assurances the human will instead use implicit assurances to inform their TRBs. However, the general human
user will not have knowledge regarding which assurances are implicit or explicit – humans participating in research
from ?adrants I and II were not aware which assurances were designed by the researchers and which weren’t. Recall
from Section 2.4 that to a user all assurances are the same; that is to say that any property or behavior of an AIA that
a?ects trust is an assurance to a user, and it doesn’t ma?er whether the assurance was designed for that purpose or not
(is explicit or implicit).
Mediums: One might use sight to express an assurance. For example an AIA might give visual feedback, or display
di?erent performance characteristics [17, 96]. ?is can also be accomplished by using natural language [134] – it is also
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 35
a simple ma?er to convert wri?en natural language output to spoken natural language now. Other typical mediums are
blinking lights, colored boxes, ringing bells, buzzers, recorded voice alerts and others.
?e other senses (touch, smell, and taste) are not well explored in literature related to human-AIA trust. Generally,
any human sense could be used as a medium. Besides sight, and sound, one of of tactile feedback has been used
extensively in robotics where it is called ‘haptic feedback’ (where the user receives mechanical feedback through the
robot controls). ?is medium is use to create a more immersive user interface in robotics, to help users feel more
connected to the robot. One can imagine smell and taste having an obvious application in the assurances of a cooking
robot, other applications certainly exist as well and are open to further research.
Methods: One of the main methods by which to express an assurance is by displaying a value, such as a ?ow-rate
[96]. While this may sound banal it actually involves some nuanced points. ?e interesting part is that a value such
as a ?ow-rate actually conveys no assurance to a human user without the human user then creating a corresponding
mental model of the trustworthiness of an AIA capability. ?e user’s trust dimensions (‘competence’, ’predictability’,
etcetera) are then a?ected by this perception. ?is approach (also used by [54, 115, 136] and others) is e?ective, but
relies heavily on the assumption that the user will create a model that is ‘good enough’ out of sequentially presenting
certain information.
Similarly one might train a user to recognize signs of failure in di?erent interactions with an AIA [28, 36, 114].
Still this approach relies on human users to make models that are ‘good enough’ in order to correctly decide how to
appropriately use the AIA. ?e main drawback of this work, and of that above is the blind reliance on users being able
to make correct statistical models (of things like reliability) from noisy observations. A more ideal approach would be
to design assurances that remove chances for misinterpretation because of inconsistent human models.
More direct methods of expressing assurances include displaying the intended movements through visual projection
of a planned path [17] – this is subtly, but signi?cantly di?erent from making the user infer the intended intention.
Analogously, natural language expressions (wri?en or otherwise) a?empt a more active method of expressing an
assurance (such as [134]). One might also display plans and logic in di?erent formats such as tables or trees, bar charts
[54, 55, 127]. ?ese approaches a?empt to remove some uncertainty regarding the human’s ability to create the correct
model. As humans are fond of saying “You can’t assume that I can read your mind!”, in essence more passive expressions
from AIAs are relying on humans to read AIA’s ‘minds’ (we can’t even do that with other humans).
Any of these methods can be more or less e?ective based on the task, or situation in which they are used. How should
uncertainly be displayed (i.e. as a distribution, summary statistics, fractions or decimals)? Unsurprisingly we ?nd that
the answer is ‘it depends’ [19, 65, 68, 132]. ?ings such as the experience of the user, or the nature of the information
being displayed a?ect the user’s ability to interpret the assurance. In the absence of that information the best that can
be done is to select the method that will work for the largest group of typical users of the AIA. A su?ciently advanced
AIA might also be able to learn the best methods to communicate to di?erent users on a one-on-one basis.
It is generally presumed that making something more human-like will make an AIA more trustworthy. An algorithm
may be human-like when it represents knowledge in a way that a human would understand, or executes logic in a
way that a human can follow. A robot that is humanoid becomes more human-like in appearance [6], a system that
uses natural language becomes more human-like in communication [68]. ?e human-AIA trust relationship depends
on assurances that, in essence, are conversions from AIA capabilities to human-perceptible behaviors and properties.
Assurances are the method of communication upon which humans can learn to trust AIAs. Because of this it is expected
Manuscript submi?ed to ACM
36 Bre? Israelsen
that at some point all assurances have to at least be made human-understandable in some way, otherwise the AIA is
essentially speaking a di?erent language, and assurances cannot be understood.
Making something ‘human-like’ doesn’t always increase it’s trustworthiness. In [31] the AIA is made more trust-
worthy by making the robot motions more human-like, whereas in [138] making the AIA more human-like resulted
in a decrease of trustworthiness. In this case the di?erence came from the type of task, in the ?rst case the robot
was physically working in proximity to a human, in the other case the user was playing a competitive game against
the AIA. It has been observed that humans trust more ‘human-like’ AIAs in more human-like ways Tripp et al. [126].
Perhaps ‘human-like’ applies to how di?cult it is to relate to the AIA. Following on this idea the bene?ts or drawbacks
of human-like characteristics are in?uenced by a user’s general impressions and feelings of how trustworthy humans
are in similar situations. ?is would also involve aspects of psychology, sociology, and is very di?cult to control and
account for. ?is is an open research question that is important to answer in order to design be?er assurances.
It is worth considering, in more detail, what implications the existence of implicit and explicit assurances has on the
designer. It is unrealistic to think that a designer can take all possible assurances into account, and thus will need to
focus e?orts on some of the most important. ?e foremost consideration is that an analysis of the interaction between
the human and user needs to be made in order to identify the critical assurances for a given scenario. For example, in
the road network problem, an analysis might ?nd that the most critical assurances are about the competence of the
UGV’s planner. In this case the designer must take time to design an assurance that is directed at the user’s perception
of the AIA’s competence – let’s call this a planning-competence assurance.
One di?culty arises from this approach is that there doesn’t seem to be a way to determine what passive assurances
might drown out active assurances. Following from the example above, the designer may have built an excellent
planning-competence assurance, but failed to consider the e?ect of how the UGV appears – it may be old, have loose
panels, and rust holes. Generally, designers overlook implicit assurances (i.e. do not consider them explicitly in design)
because they assume that they will have no e?ect (i.e. why does it ma?er if there are rust-holes if the UGV works?).
?is can stem from either: 1) ignorance of human-AIA trust dynamics, or 2) lack of identifying which assurances are
most important to a human user.
While it might be nice, it seems unreasonable, ine?cient, and unwise to a?empt a study of every possible assurance
from an AIA to a human and then select the most important. Perhaps one way a designer might try to identify which
assurances are important is to perform human studies where feedback about which characteristics of the AIA most
a?ected the trust of the user. An approach like this would help to point out if explicit assurances are being noticed, and
if there are implicit assurances that are overly in?uential, or that overwhelm the explicitly designed assurances. With
such feedback designers would have a realistic idea about whether their explicit assurances are having the desired e?ect.
We use the UGV road-network problem to illustrate. A?er designing an explicit assurance (or more) the operator/UGV
team could work together, a?erwards the operator could rank the di?erent behaviors/properties of the AIA a?ected
their trust in it. In this way the critical implicit and explicit assurances will be identi?ed. If the explicit assurance is
near the top of the list of in?uencing assurances then it is working, if not a re-design may need to occur.
One ?nal point is that there are several potential sources of explicit assurance that lack appropriate expressions,
and thus cannot be e?ectively utilized as assurances. For example, it is unclear the best way for an AIA to express
that it has been validated and veri?ed on situations similar to the current one. Similarly, what other methods exist
for communicating statistical distributions besides showing a plot (only useful for 1 or 2 dimensional distributions) or
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 37
showing su?cient statistics? Investigating how assurances can be expressed in e?ective, and e?cient ways is critical to
human-AIA trust relationships.
E?cacy: Some kinds of expression are very ‘one-dimensional’ in that they only use one medium, or method. ?is,
again, has been seen in practice by the utilization of plo?ing a certain value over time. Because of this, much of
the research to date involves assurances that are not robust to loss in transfer, meaning that the approaches count
everything on a speci?c medium and method to work, and if not the whole assurance is rendered useless. Hence,
exploring ways in which assurances can be robustly communicated is a clear opportunity for those trying to design
assurances. ?is is akin to a human speaking with their voice, making facial expressions, and gestures with their hands
as well; simultaneously utilizing several mediums/methods helps to ensure an assurance will be e?ective. Of course,
repeating the same message over a thousand times is wasteful, and so enters the idea of e?ciency in expression.
Perhaps less obvious is a situation in which the user has to supplement an incomplete assurance. A user can supposedly
create a mental model of the trustworthiness of an AIA capability – based on repeated observations over time. Creating
this mental model takes time, and the model is prone to cognitive biases. In this case the assurance is communicated
slowly and indirectly. Generally, a highly e?ective assurance would have precise information communicated in a way
that is easy for the user to perceive, with li?le loss. Whereas, an ine?cient, and ine?ective assurance may be more
vague, wasteful (i.e. repeating the same thing a thousand times), and susceptible to loss in communication. ?e solutions
to e?cacy lay in selecting appropriate methods, and mediums for expression of the assurance, and by designing for
appropriate levels of redundancy to ensure that the assurance is received.
5.3 Observing E?ects of Assurances
Since the purpose of assurances is to a?ect TRBs, it is important to be able to quantify those e?ect. ?ere are two
di?erent instances in which this is useful: 1) when the designer needs an understanding of how e?ective the assurances
are; and 2) When the AIA needs to observe the e?cacy of it’s own assurances.
To our knowledge there has not been any work regarding the second situation that enables an AIA to observe
responses to assurances and then adapt behaviors appropriately (at least not in the trust cycle se?ing), however this is
arguably the ultimate goal so that AIAs can themselves modify assurances to meet di?erent needs. What does this
mean practically? ?eoretically, any method that is made for the designer to measure the e?ects of assurances could
also be deployed into an AIA. ?e surveyed literature gives some insights into how that has been done to date.
When it comes time to measure the e?ect of assurances on a human’s trust there are two main approaches:
• Gather self-reported changes in trust from human users
• Measure changes in user’s TRBs
Self-Reported Changes. Gathering self-reported changes in trust involves asking questions like: ‘how trustworthy do
you feel the system is?’; or ‘to what extent do you ?nd the system to be interpretable?’ [62, 85, 96, 114, 136]. ?ese kinds
of questions can be useful in verifying whether the assurances are having the expected e?ect. It is not unreasonable to
imagine that an AIA might be equipped with a method by which it can ask the user questions about their trust, process
those responses, and modify assurances appropriately.
However, sometimes changes is self-reported trust do not result in changes in TRBs [32]. From the AIAs perspective
this means that — unless the object of the assurances is to make the person’s level of self-reported trust change — the
assurances are not providing any bene?t. As previously discussed, the goal of assurances is to elicit appropriate TRBs
Manuscript submi?ed to ACM
38 Bre? Israelsen
from the human user. From this perspective, measuring changes in TRBs is the more direct, and objective, approach to
measure the e?ect of assurances.
Self-reports are the most useful when trying to understand the true e?ects of an assurance. Does a certain assurance,
assumed to a?ect ‘situational normality’, actually do that? ?ere is space for quite a lot of research in this realm. Does
displaying a speci?c plot actually convey information about ‘predictability’? ?is information can be used to inform
the selection of the methods of assurance.
Measuring Changes in TRBs. Generally researchers in the ?eld have measured, in some way, how frequently the AIA
was able to run in autonomous mode before being turned o? [28, 36]. Other researchers calculated whether the user
was willing to cooperate with the AIA or not [6, 114, 138]. A be?er de?ned metric is: the likelihood of appropriate use
of a certain capability by the user; albeit more di?cult to formally de?ne/calculate in di?erent situations. As a concrete
example, in the UGV road-network problem there isn’t really an option to ‘turn o?’ the UGV. Instead the remote
operator can make decisions such as accepting a plan designed by the UGV. In this situation the e?ect of assurances
might be measured by how likely the operator is to accept a generated plan instead of overriding it (recall that the
goal may not be to have the generated plan accepted 100% of the time, rather that it be accepted with respect to how
appropriate it is in a given situation).
In practical application (such as in the UGV road-network problem), the user, and the human-AIA team, care more
about whether TRBs are appropriate or not. It doesn’t help if an assurance helps the user feel that the AIA is more
competent, if the user doesn’t treat the AIA any di?erently than before the assurance. ?is assumes that it is possible
for appropriate TRBs to be measured in the ?rst place. For example, if appropriate behavior is for the user to verify
a sensor reading, can the AIA perceive that happening? In that situation perhaps the easiest approach would be to
ask the user, but what if the user is dishonest? Is there a way to verify the user behavior is actually appropriate? ?is
is something that has gained notoriety with the current generation of autonomous cars, where the car is capable of
autonomous operation, but the user still needs to sit in the driver’s seat and be a?entive just in case the vehicle cannot
perform correctly. ?ese are some arguments for the importance of designing methods for perceiving appropriate (and
inappropriate) TRBs when designing assurances.
6 CONCLUSIONS
Now, more than ever, there is a great need for humans to be able to trust the AIAs that we are creating. Assurances are
the method by which AIAs can encourage humans to trust them appropriately, and to then use them appropriately. We
have presented here a de?nition, case for, and survey of assurances in the context of human-AIA trust relationships.
?is survey was performed, to some extent, from a standpoint of designing an unmanned ground vehicle that is
working in concert with a human supervisor. However, the theoretical framework, and classi?cation of assurances
is meant to be general in order to apply to a broad range of AIAs. One of the main motivations of this survey was
the insight that there is an extremely large community of researchers working on human-AIA assurances (perhaps
unknowingly). It is important to recognize this so that we can start to organize our e?orts and begin methodically
answering the open questions of this important ?eld. Arguably, the ultimate goal is to develop a su?cient set of
assurances that can be located in ?adrant III (i.e. those that have been designed and experimentally veri?ed), of course
this requires cooperation among the community.
?e most surprising insight from compiling this survey was the absence of a detailed de?nition and classi?cation of
assurances. Assurances have been, by far, the most ignored component of human-AIA trust relationships. ?ere have
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 39
been many researchers who have recognized the existence of assurances (usually by other names), but there has been
no detailed de?nition until now. We have drawn from multiple bodies of research in order to ‘?ll in’ the details of the
human-AIA trust cycle (Figure 1) (a novel contribution in itself). ?is led to our main contribution: to formally de?ne
and classify assurances within the human-AIA trust cycle. We have introduced the idea that assurances must stem from
di?erent AIA capabilities, that they can be implicit or explicit, that the way in which they are expressed to a user can
be just as important as how they are calculated/designed. We have also highlighted that methods for measuring the
e?ects of assurances must also exist. And, we have shown how each of these ?ts into the larger trust cycle.
?ere is a fairly large body of research that is focused, in some way, on in?uencing trust in human-AIA relationships.
However, there is a larger portion of research that deals with techniques that would be useful in designing assurances
but that, to date, has not directly or knowingly considered a?ecting human-AIA trust through assurances as a formal
design goal. Research from these areas (such as V&V, active learning, and safety) should provide a rich collection of
methods to be studied and formally applied to human-AIA trust relationships.
While the basic de?nition of assurances (i.e. feedback to user trust, in the human-AIA trust cycle) is simple from a
theoretical standpoint, the exercise of gathering related literature helped to illuminate some important considerations
and details regarding the design of assurances. In Section 5 we present Figure 9 which is an original synthesis of
the surveyed literature. We show that designers must be able to design ways for an AIA to calculate, design, and
plan explicit assurances. Designers must also account for the expression and perception of assurances, this involves
considering how e?ective a given method/medium might be in conveying an assurance to a user. Finally, the whole
purpose of assurances is lost if there is no way to measure/quantify the e?ects of the assurances; e?ort must be spent in
creating appropriate metrics and experiments by which to do this.
A sobering reminder is that there is not a single assurance that will perform the best in all situations. It is almost
certain that given time highly specialized assurances can be designed for many situations. Even so we warn that, for
the research and design of assurances to be sustainable in the current environment of fast-paced development of new
technology, it is important to consider approaches that are as general as possible in order to be more easily used with
newly developed methods for implementing AIA capabilities.
?e treatment of assurances in this survey are based, in part, on a model of trust. For completeness it is important to
mention distrust. As reviewed and discussed by Lewicki et al. [74], and formalized in [86, 87]. Low trust is not the same
as distrust, neither is low distrust the same as trust. McKnight and Chervany [86] suggest that “the emotional intensity
of distrust distinguishes it from trust”, and they explain that distrust comes from emotions like: wariness, caution, and
fear. Whereas, trust stems from emotions like: hope, safety, and con?dence. Trust and distrust are orthogonal elements
that de?ne a person’s TRB towards a trustee. In this survey distrust was not considered, however it must be made
clear that any complete treatment of trust relationships, and for our purposes, designed assurances, must consider the
dimensions of distrust as well as those of trust. For now, this investigation is le? as an avenue for future research.
We hope that researchers can begin to reach across perceived lines in order to ?nd more tools to appropriately design
and test assurances. We hope that the material is Section 2 will provide a common foundation on which researchers
from all quadrants can build on in order to unify research e?orts. More speci?cally, we hope that those in the ?elds of
CS, ML, and AI can begin to use the principles outlined in this survey to help guide their search for more ‘interpretable’,
‘explainable’, and ‘comprehensible’ systems. It is important for them to understand the existence of a trust model, and
the human-AIA trust cycle. If they consider these points, they will be able to identify be?er methods, and design more
e?ective assurances.
Manuscript submi?ed to ACM
40 Bre? Israelsen
Likewise, those who formally consider trust (i.e. researchers in HRI, e-commerce, UI) should now be able to identify
more methods and approaches for designing assurances, so that they can perform experiments to validate them. ?ey
will also have a be?er idea of what kinds of experiments have been performed, and possible new areas to investigate.
We have identi?ed many opportunities for further research in how AIAs can in?uence human trust through
assurances. ?e framework found herein will help other researchers to see the ?eld from a larger perspective, to classify
the type of research they are performing, and help them to consider the greater implications of their work.
REFERENCES
[1] Behnoush Abdollahi and Olfa Nasraoui. 2016. Explainable Restricted Boltzmann Machines for Collaborative Filtering. (22 June 2016).
arXiv:stat.ML/1606.07129
[2] Ma?hew Aitken. 2016. Assured Human-Autonomy Interaction through Machine Self-Con?dence. Master’s thesis. UNIVERSITY OF COLORADO AT
BOULDER.
[3] Ma?hew Aitken, Nisar Ahmed, Dale Lawrence, Brian Argrow, and Eric Frew. 2016. Assurances and machine self-con?dence for enhanced trust in
autonomous systems. In RSS 2016 Workshop on Social Trust in Autonomous Systems. qav.comlab.ox.ac.uk.
[4] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane?. 2016. Concrete Problems in AI Safety. (21 June 2016).
arXiv:cs.AI/1606.06565
[5] Anne?e Baier. 1986. Trust and Antitrust. Ethics 96, 2 (1986), 231–260.
[6] Wilma A Bainbridge, Justin W Hart, Elizabeth S Kim, and Brian Scassellati. 2011. ?e Bene?ts of Interactions with Physically Present Robots over
Video-Displayed Agents. Adv. Robot. 3, 1 (1 Jan. 2011), 41–52.
[7] Guru Banavar. 2016. What It Will Take for Us to Trust AI. h?ps://hbr.org/2016/11/what-it-will-take-for-us-to-trust-ai. (29 Nov. 2016). Accessed:
2017-2-16.
[8] Yaakov Bar-Shalom, X Rong Li, and ?iagalingam Kirubarajan. 2001. Estimation with Applications to Tracking and Navigation: ?eory Algorithms
and So?ware. Vol. 9. Wiley. 584 pages.
[9] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: a review and new perspectives. IEEE Trans. Pa?ern Anal. Mach.
Intell. 35, 8 (Aug. 2013), 1798–1828.
[10] Mark Benio?. 2016. ?e AI revolution is coming fast. But without a revolution in trust, it will fail. h?ps://www.weforum.org/agenda/2016/08/
the-digital-revolution-is-here-but-without-a-revolution-in-trust-it-will-fail/. (26 Aug. 2016). Accessed: 2017-6-14.
[11] L P Berczi, I Posner, and T D Barfoot. 2015. Learning to assess terrain from human demonstration using an introspective Gaussian-process classi?er.
In 2015 IEEE International Conference on Robotics and Automation (ICRA). 3178–3185.
[12] Nick Bostrom. 2014. Superintelligence: Paths, Dangers, Strategies. OUP Oxford.
[13] Eric Brochu, Vlad M Cora, and Nando de Freitas. 2010. A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to
Active User Modeling and Hierarchical Reinforcement Learning. (12 Dec. 2010). arXiv:cs.LG/1012.2599
[14] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible Models for HealthCare: Predicting
Pneumonia Risk and Hospital 30-day Readmission. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD ’15). ACM, New York, NY, USA, 1721–1730.
[15] David Cassel. 2017. ?e Big ?estion from 2016: Can We Trust Our Technologies? - ?e New Stack. h?ps://thenewstack.io/
big-question-2016-can-trust-technologies/. (1 Jan. 2017). Accessed: 2017-6-14.
[16] Davide Castelvecchi. 2016. Can we open the black box of AI? Nature 538, 7623 (6 Oct. 2016), 20–23.
[17] R T Chadalavada, H Andreasson, R Krug, and A J Lilienthal. 2015. ?at’s on my mind! robot to human intention communication through on-board
projection on shared ?oor space. In 2015 European Conference on Mobile Robots (ECMR). 1–6.
[18] Joseph Chee Chang, Saleema Amershi, and Ece Kamar. 2017. Revolt: Collaborative Crowdsourcing for Labeling Machine Learning Datasets. In
Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New York, NY, USA, 2334–2346.
[19] Jessie Y Chen, Katelyn Procci, Michael Boyce, Julia Wright, Andre Garcia, and Michael Barnes. 2014. Situation awareness-based agent transparency.
Technical Report. DTIC Document.
[20] Edward Choi, Mohammad Taha Bahadori, Joshua A Kulas, Andy Schuetz, Walter F Stewart, and Jimeng Sun. 2016. RETAIN: An Interpretable
Predictive Model for Healthcare using Reverse Time A?ention Mechanism. (19 Aug. 2016). arXiv:cs.LG/1608.05745
[21] W Churchill, Chi Hay Tong, C Gura?u, I Posner, and P Newman. 2015. Know your limits: Embedding localiser performance models in teach and
repeat maps. In 2015 IEEE International Conference on Robotics and Automation (ICRA). 4238–4244.
[22] Cynthia L Corritore, Beverly Kracher, and Susan Wiedenbeck. 2003. On-line trust: concepts, evolving themes, a model. Int. J. Hum. Comput. Stud.
58, 6 (2003), 737–758.
[23] Alex Cranz and Beth Elderkin. 2017. Why We Don’t Fully Trust Technology. h?ps://www.gizmodo.com.au/2015/10/
why-we-dont-fully-trust-technology/. (14 June 2017). Accessed: 2017-6-14.
[24] William Curran, Cameron Bowie, and William D Smart. 2016. POMDPs for Risk-Aware Autonomy. In 2016 AAAI Fall Symposium Series.
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 41
[25] Se?bastien Da Veiga and Amandine Marrel. 2012. Gaussian process modeling with inequality constraints. In Annales de la Faculte? des Sciences de
Toulouse, Vol. 21. 529–555.
[26] David Danks. 2017. Can we trust self-driving cars? h?p://www.salon.com/2017/01/08/can-we-trust-self-driving-cars partner/. (8 Jan. 2017).
Accessed: 2017-6-14.
[27] J Dequaire, C H Tong, W Churchill, and I Posner. 2016. O? the beaten track: Predicting localisation performance in visual teach and repeat. In 2016
IEEE International Conference on Robotics and Automation (ICRA). 795–800.
[28] M Desai, M Medvedev, M Va?zquez, S McSheehy, S Gadea-Omelchenko, C Bruggeman, A Steinfeld, and H Yanco. 2012. E?ects of changing reliability
on trust of robot systems. In 2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI). 73–80.
[29] David DeSteno. 2014. Can You Trust Technology? h?p://www.hu?ngtonpost.com/david-desteno/can-you-trust-technology b 4683614.html.
(29 Jan. 2014). Accessed: 2017-6-14.
[30] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. (28 Feb. 2017). arXiv:stat.ML/1702.08608
[31] Anca Dragan and Siddhartha Srinivasa. 2013. Generating Legible Motion. In Proceedings of Robotics: Science and Systems.
[32] Mary T Dzindolet, Sco? A Peterson, Regina A Pomranky, Linda G Pierce, and Hall P Beck. 2003. ?e role of trust in automation reliance. Int. J.
Hum. Comput. Stud. 58, 6 (2003), 697–718.
[33] Mica R Endsley. 1995. Toward a ?eory of Situation Awareness in Dynamic Systems. Hum. Factors 37, 1 (1 March 1995), 32–64.
[34] James H Faghmous and Vipin Kumar. 2014. A Big Data Guide to Understanding Climate Change: ?e Case for ?eory-Guided Data Science. Big
Data 2, 3 (1 Sept. 2014), 155–163.
[35] Katherine Ellen Foley. 2017. A pioneering computer scientist wants algorithms to be regulated like cars, banks, and drugs. h?ps://qz.com/998131.
(3 June 2017). Accessed: 2017-6-5.
[36] A Freedy, E DeVisser, G Weltman, and N Coeyman. 2007. Measurement of trust in human-robot collaboration. In 2007 International Symposium on
Collaborative Technologies and Systems. 106–114.
[37] Alex A Freitas. 2006. Are we really discovering interesting knowledge from data. Expert Update (the BCS-SGAI magazine) 9, 1 (2006), 41–47.
[38] F Fukuyama. 1995. Trust: ?e social virtues and the creation of prosperity. mpls.frb.org.
[39] Diego Gambe?a. 1988. Trust: Making and Breaking Cooperative Relations. Blackwell.
[40] J Garc?a and F Ferna?ndez. 2015. A comprehensive survey on safe reinforcement learning. J. Mach. Learn. Res. (2015).
[41] Zoubin Ghahramani. 2015. Probabilistic machine learning and arti?cial intelligence. Nature 521, 7553 (28 May 2015), 452–459.
[42] Shalini Ghosh, Patrick Lincoln, Ashish Tiwari, Xiaojin Zhu, and Wisc Edu. 2016. Trusted Machine Learning for Probabilistic Models. Reliable
Machine Learning in the Wild at ICML (2016).
[43] Will Goodrum. 2016. Finding Balance: Model Accuracy vs. Interpretability in Regulated Environments. h?p://www.elderresearch.com/company/
blog/predictive-model-accuracy-versus-interpretability. (4 Nov. 2016). Accessed: 2017-3-16.
[44] H Grimme?, R Paul, R Triebel, and I Posner. 2013. Knowing when we don’t know: Introspective classi?cation for mission-critical decision making.
In 2013 IEEE International Conference on Robotics and Automation. 4531–4538.
[45] Hugo Grimme?, Rudolph Triebel, Rohan Paul, and Ingmar Posner. 2016. Introspective classi?cation for robot perception. Int. J. Rob. Res. 35, 7
(2016), 743–762.
[46] Victoria Groom and Cli?ord Nass. 2007. Can robots be teammates?: Benchmarks in human–robot teams. Interact. Stud. 8, 3 (1 Jan. 2007), 483–500.
[47] David Gunning. 2016. Explainable Arti?cial Intelligence. h?ps://www.darpa.mil/program/explainable-arti?cial-intelligence. (2016). Accessed:
2017-9-1.
[48] David Gunning. 2017. Explainable arti?cial intelligence (xai). Defense Advanced Research Projects Agency (DARPA), nd Web (2017).
[49] Corina Gura?u, Chi Hay Tong, and Ingmar Posner. 2016. Fit for Purpose? Predicting Perception Performance Based on Past Experience. In 2016
International Symposium on Experimental Robotics. Springer, Cham, 454–464.
[50] Isabelle Guyon and Andre? Elissee?. 2003. An Introduction to Variable and Feature Selection. J. Mach. Learn. Res. 3, Mar (2003), 1157–1182.
[51] Jdf Habbema. 1976. Models for diagnosis and detection of combinations of diseases. Decision making and medical care : can information science help?
: Proceedings of the IFIP Working Conference on Decision Making and Medical Care (1976), 399–411.
[52] Dylan Had?eld-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell. 2016. ?e O?-Switch Game. (24 Nov. 2016). arXiv:cs.AI/1611.08219
[53] Anne-Claire Haury, Pierre Gestraud, and Jean-Philippe Vert. 2011. ?e in?uence of feature selection methods on accuracy, stability and interpretability
of molecular signatures. PLoS One 6, 12 (21 Dec. 2011), e28210.
[54] Andrew R Hutchins, M L Cummings, Mark Draper, and ?omas Hughes. 2015. Representing Autonomous Systems’ Self-Con?dence through
Competency Boundaries, Vol. 59. SAGE Publications, 279–283.
[55] Johan Huysmans, Karel Dejaeger, Christophe Mues, Jan Vanthienen, and Bart Baesens. 2011. An empirical evaluation of the comprehensibility of
decision table, tree and rule based predictive models. Decis. Support Syst. 51, 1 (1 April 2011), 141–154.
[56] T Inagaki, N Moray, and M Itoh. 1998. Trust self-con?dence and authority in human-machine systems. Proceedings of the IFAC man-machine systems
(1998), 431–436.
[57] Bre? W Israelsen, Nisar Ahmed, Kenneth Center, Roderick Green, and Winston Benne? Jr. 2017. Towards Adaptive Training of Agent-based
Sparring Partners for Fighter Pilots. In InfoTech@Aerospace Conference. Grapevine, TX, 1–15.
Manuscript submi?ed to ACM
42 Bre? Israelsen
[58] Milos Jovanovic, Sandro Radovanovic, Milan Vukicevic, Sven Van Poucke, and Boris Delibasic. 2016. Building interpretable predictive models for
pediatric hospital readmission using Tree-Lasso logistic regression. Artif. Intell. Med. 72 (2016), 12–21.
[59] Mohammed Waleed Kadous. 1999. Learning Comprehensible Descriptions of Multivariate Time Series. In In Proceedings of the 16 th International
Conference of Machine Learning (ICML-99, Saso Dzeroski Ivan Bratko (Ed.). 454–463.
[60] Krishnanand N Kaipa, Akshaya S Kankanhalli-Nagendra, and Satyandra K Gupta. 2015. Toward Estimating Task Execution Con?dence for Robotic
Bin-Picking Applications. In 2015 AAAI Fall Symposium Series.
[61] Poornima Kaniarasu, Aaron Steinfeld, Munjal Desai, and Holly Yanco. 2012. Potential Measures for Detecting Trust Changes. In Proceedings of the
Seventh Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI ’12). ACM, New York, NY, USA, 241–242.
[62] Poornima Kaniarasu, Aaron Steinfeld, Munjal Desai, and Holly Yanco. 2013. Robot Con?dence and Trust Alignment. In Proceedings of the 8th
ACM/IEEE International Conference on Human-robot Interaction (HRI ’13). IEEE Press, Piscataway, NJ, USA, 155–156.
[63] Bijan Khosravi. 2016. Will You Trust AI To Be Your New Doctor? h?p://www.forbes.com/sites/bijankhosravi/2016/03/24/
will-you-trust-ai-to-be-your-new-doctor-a-?ve-year-outcome/. (24 March 2016). Accessed: 2017-2-16.
[64] D Kingston. 2012. Intruder tracking using uav teams and ground sensor networks. In German Aviation and Aerospace Congress (DLRK 2012).
[65] Kristine M Kuhn. 1997. Communicating Uncertainty: Framing E?ects on Responses to Vague Probabilities. Organ. Behav. Hum. Decis. Process. 71, 1
(1 July 1997), 55–83.
[66] Ugur Kuter and Chris Miller. 2012. Computational Mechanisms to Support Reporting of Self Con?dence of Automated / Autonomous Systems. In
AAAI 2015 Fall Symposium. 18–21.
[67] Ugur Kuter and Chris Miller. 2015. Computational Mechanisms to Support Reporting of Self Con?dence of Automated/Autonomous Systems. In
2015 AAAI Fall Symposium Series.
[68] Carmen Lacave and Francisco J D??ez. 2002. A review of explanation methods for Bayesian networks. Knowl. Eng. Rev. 17, 02 (1 June 2002), 107–127.
[69] A Lacher, R Grabowski, and S Cook. 2014. Autonomy, trust, and transportation. 2014 AAAI Spring Symposium Series (2014).
[70] Nancy K Lankton and D Harrison McKnight. 2008. Do People Trust Facebook as a Technology or as a“ Person”? Distinguishing Technology Trust
from Interpersonal Trust. AMCIS 2008 Proceedings (2008), 375.
[71] Kathryn Blackmond Laskey. 1991. Con?ict and Surprise: Heuristics for Model Revision. In Proceedings of the Seventh Conference on Uncertainty in
Arti?cial Intelligence (UAI’91). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 197–204.
[72] K B Laskey. 1995. Sensitivity analysis for probability assessments in Bayesiannetworks. IEEE Trans. Syst. Man Cybern. 25, 6 (1995), 901–909.
[73] J D Lee and K A See. 2004. Trust in Automation: Designing for Appropriate Reliance. (2004).
[74] Roy J Lewicki, Daniel J McAllister, and Robert J Bies. 1998. Trust and Distrust: New Relationships and Realities. Acad. Manage. Rev. 23, 3 (1998),
438–458.
[75] Roy J Lewicki, Edward C Tomlinson, and Nicole Gillespie. 2006. Models of Interpersonal Trust Development: ?eoretical Approaches, Empirical
Evidence, and Future Directions. J. Manage. 32, 6 (1 Dec. 2006), 991–1022.
[76] Roy J Lewicki and Carolyn Wietho?. 2006. Trust, trust development, and trust repair. ?e handbook of con?ict resolution: ?eory and practice (2006),
92–119.
[77] J David Lewis and Andrew Weigert. 1985. Trust as a Social Reality. Soc. Forces 63, 4 (1 June 1985), 967–985.
[78] Austin Lillard, Eric W Frew, Brian Argrow, Dale Lawrence, and Nisar Ahmed. 2016. Assurances for Enhanced Trust in Autonomous Systems. In
Submi?ed to IROS.
[79] Zachary C Lipton. 2016. ?e Mythos of Model Interpretability. (10 June 2016). arXiv:cs.LG/1606.03490
[80] Zachary C Lipton, Jianfeng Gao, Lihong Li, Jianshu Chen, and Li Deng. 2016. Combating Reinforcement Learning’s Sisyphean Curse with Intrinsic
Fear. (3 Nov. 2016). arXiv:cs.LG/1611.01211
[81] James Robert Lloyd. 2014. Automatic Statistician. h?ps://www.automaticstatistician.com/examples/. (1 March 2014). Accessed: 2017-2-16.
[82] Meghann Lomas, Robert Chevalier, Ernest Vincent Cross, II, Robert Christopher Garre?, John Hoare, and Michael Kopack. 2012. Explaining Robot
Actions. In Proceedings of the Seventh Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI ’12). ACM, New York, NY, USA,
187–188.
[83] George F Luger. 2008. Arti?cial Intelligence: Structures and Strategies for Complex Problem Solving (6th ed.). Addison-Wesley Publishing Company,
USA.
[84] D J C MacKay. 1992. Information-Based Objective Functions for Active Data Selection. Neural Comput. 4, 4 (July 1992), 590–604.
[85] D Harrison Mcknight, Michelle Carter, Jason Benne? ?atcher, and Paul F Clay. 2011. Trust in a Speci?c Technology: An Investigation of Its
Components and Measures. ACM Trans. Manage. Inf. Syst. 2, 2 (July 2011), 12:1–12:25.
[86] D Harrison McKnight and Norman Chervany. 2001. While trust is cool and collected, distrust is ?ery and frenzied: A model of distrust concepts.
Amcis 2001 Proceedings (2001), 171.
[87] D H McKnight and Norman L Chervany. 2001. Trust and Distrust De?nitions: One Bite at a Time. In Trust in Cyber-societies. Springer, Berlin,
Heidelberg, 27–54.
[88] D H McKnight and N L Chervany. 2001. What Trust Means in E-Commerce Customer Relationships: An Interdisciplinary Conceptual Typology.
International Journal of Electronic Commerce 6, 2 (2001), 35–59.
Manuscript submi?ed to ACM
“I can assure you [. . . ] that it’s going to be all right” 43
[89] D Harrison McKnight, Larry L Cummings, and Norman L Chervany. 1998. Initial Trust Formation in New Organizational Relationships. Acad.
Manage. Rev. 23, 3 (1 July 1998), 473–490.
[90] D H McKnight, Charles J Kacmar, and Vivek Choudhury. 2004. Dispositional Trust And Distrust Distinctions in Predicting High- and Low-Risk
Internet Expert Advice Site Perceptions. e-Service Journal 3, 2 (2004), 35–55.
[91] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je? Dean. 2013. Distributed Representations of Words and Phrases and their
Compositionality. In Advances in Neural Information Processing Systems 26, C J C Burges, L Bo?ou, M Welling, Z Ghahramani, and K Q Weinberger
(Eds.). Curran Associates, Inc., 3111–3119.
[92] Kathryn Moody. 2017. How CHROs can handle emerging AI and build company trust. h?p://www.hrdive.com/news/
how-chros-can-handle-emerging-ai-and-build-company-trust/438602/. (22 March 2017). Accessed: 2017-6-14.
[93] Rebecca E Morrison, Todd A Oliver, and Robert D Moser. 2016. Representing model inadequacy: A stochastic operator approach. (6 April 2016).
arXiv:cs.CE/1604.01651
[94] Bonnie M Muir. 1987. Trust between humans and machines, and the design of decision aids. Int. J. Man. Mach. Stud. 27, 5 (12 Nov. 1987), 527–539.
[95] Bonnie M Muir. 1994. Trust in automation: Part I. ?eoretical issues in the study of trust and human intervention in automated systems. Ergonomics
37, 11 (1994), 1905–1922.
[96] B M Muir and N Moray. 1996. Trust in automation. Part II. Experimental studies of trust and human intervention in a process control simulation.
Ergonomics 39, 3 (March 1996), 429–460.
[97] University of Massachuse?s-Lowell Munjal Desai, University of Massachuse?s-Lowell Kristen Stubbs, Carnegie Mellon University Aaron Steinfeld,
University of Massachuse?s-Lowell Holly Yanco, and Authors. 2009. Creating Trustworthy Robots: Lessons and Inspirations from Automated
Systems. (2009).
[98] Sandeep Neema. 2017. Assured Autonomy. h?ps://www.darpa.mil/program/assured-autonomy. (2017). Accessed: 2017-9-1.
[99] Nils J Nilsson. 2009. ?e ?est for Arti?cial Intelligence. Cambridge University Press.
[100] Clemens O?e. 2013. Safe and Interpretable Machine Learning: A Methodological Review. In Computational Intelligence in Intelligent Data Analysis,
Christian Moewes and Andreas Nu?rnberger (Eds.). Springer Berlin Heidelberg, 111–122.
[101] Raja Parasuraman and Victor Riley. 1997. Humans and automation: Use, misuse, disuse, abuse. Human Factors: ?e Journal of the Human Factors
and Ergonomics Society 39, 2 (1997), 230–253.
[102] Yubin Park, Joyce Ho, and Joydeep Ghosh. 2016. ACDC: ? -Carving Decision Chain for Risk Strati?cation. (16 June 2016). arXiv:stat.ML/1606.05325
[103] R Paul and P Newman. 2011. Self help: Seeking out perplexing images for ever improving navigation. In 2011 IEEE International Conference on
Robotics and Automation. 445–451.
[104] Joaquin ?in?onero-Candela. 2009. Dataset Shi? in Machine Learning. MIT Press.
[105] Vasumathi Raman, Constantine Lignos, Cameron Finucane, Kenton C Lee, Mitch Marcus, and Hadas Kress-Gazit. 2013. Sorry dave, i’m afraid I
can’t do that: Explaining unachievable robot tasks using natural language. Technical Report. University of Pennsylvania Philadelphia United States.
[106] B Reeves and C Nass. 1997. ?e Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places. Comput.
Math. Appl. 5, 33 (1997), 128.
[107] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “Why Should I Trust You?”: Explaining the Predictions of Any Classi?er. (16 Feb.
2016). arXiv:cs.LG/1602.04938
[108] G Ridgeway, D Madigan, T Richardson, and J O’Kane. 1998. Interpretable Boosted Na??ve Bayes Classi?cation. KDD (1998).
[109] Victor Riley. 1996. Operator reliance on automation: ?eory and data. Lawrence Erlbaum Associates, Inc.
[110] William B Rouse. 1986. Design and evaluation of computer-based decision support systems. Microcomputer decision support systems (1986).
[111] Fade Rudnitsky. 2017. How Can We Trust AI Companies? – Contextual Wisdom — ?e O?cial Neura Blog – Medium. h?ps://medium.com/
the-o?cial-neura-blog/how-can-we-trust-ai-companies-28a6ac635e45. (2 May 2017). Accessed: 2017-6-14.
[112] Stephan Ruping. 2006. Learning Interpretable Models. Ph.D. Dissertation. University of Darmstadt.
[113] Stuart Jonathan Russell and Peter Norvig. 2010. Arti?cial Intelligence: A Modern Approach (third ed.). Prentice Hall.
[114] Maha Salem, Gabriella Lakatos, Farshid Amirabdollahian, and Kerstin Dautenhahn. 2015. Would You Trust a (Faulty) Robot?: E?ects of Error,
Task Type and Personality on Human-Robot Cooperation and Trust. In Proceedings of the Tenth Annual ACM/IEEE International Conference on
Human-Robot Interaction (HRI ’15). ACM, New York, NY, USA, 141–148.
[115] ?omas B Sheridan and Robert T Hennessy. 1984. Research and modeling of supervisory control behavior. Report of a workshop. Technical Report.
DTIC Document.
[116] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical Bayesian Optimization of Machine Learning Algorithms. In Adv. Neural Inf.
Process. Syst. 25. 2951–2959.
[117] Ieee Spectrum. 2016. Can We Trust Robots? h?p://spectrum.ieee.org/robotics/arti?cial-intelligence/can-we-trust-robots. (31 May 2016). Accessed:
2017-6-14.
[118] Masashi Sugiyama, Makoto Yamada, and Marthinus Christo?el du Plessis. 2013. Learning under nonstationarity: covariate shi? and class-balance
change. Wiley Interdiscip. Rev. Comput. Stat. 5, 6 (2013), 465–477.
[119] William R Swartout. 1983. XPLAIN: a system for creating and explaining expert consulting programs. Artif. Intell. 21, 3 (1 Sept. 1983), 285–325.
Manuscript submi?ed to ACM
44 Bre? Israelsen
[120] Daniel Sza?r, Bilge Mutlu, and Terrence Fong. 2014. Communication of Intent in Assistive Free Flyers. In Proceedings of the 2014 ACM/IEEE
International Conference on Human-robot Interaction (HRI ’14). ACM, New York, NY, USA, 358–365.
[121] Daniel Sza?r, Bilge Mutlu, and Terry Fong. 2015. Communicating Directionality in Flying Robots. In Proceedings of the Tenth Annual ACM/IEEE
International Conference on Human-Robot Interaction (HRI ’15). ACM, New York, NY, USA, 19–26.
[122] Colin Tankard. 2016. What the GDPR means for businesses. Network Security 2016, 6 (1 June 2016), 5–8.
[123] Rudolph Triebel, Hugo Grimme?, Rohan Paul, and Ingmar Posner. 2013. Introspective active learning for scalable semantic mapping. In Workshop.
Robotics Science and Systems (RSS). 809–816.
[124] Rudolph Triebel, Hugo Grimme?, Rohan Paul, and Ingmar Posner. 2016. Driven Learning for Driving: How Introspection Improves Semantic
Mapping. In Robotics Research, Masayuki Inaba and Peter Corke (Eds.). Springer International Publishing, 449–465.
[125] Rudolph Triebel, Hugo Grimme?, and Ingmar Posner. 2013. Con?dence Boosting: Improving the Introspectiveness of a Boosted Classi?er for
E?cient Learning. In Workshop on Autonomous Learing.
[126] J Tripp, D H McKnight, and N K Lankton. 2011. Degrees of Humanness in Technology: What Type of Trust Ma?ers? AMCIS (2011).
[127] Vanya Van Belle and Paulo Lisboa. 2013. Research directions in interpretable machine learning models. In ESANN.
[128] Vanya M C A Van Belle, Ben Van Calster, Dirk Timmerman, Tom Bourne, Cecilia Bo?omley, Lil Valentin, Patrick Neven, Sabine Van Hu?el, Johan
A K Suykens, and Stephen Boyd. 2012. A mathematical model for interpretable clinical decision support with applications in gynecology. PLoS One
7, 3 (29 March 2012), e34312.
[129] A Vellido, J D Mart??n-Guerrero, and Pjg Lisboa. 2012. Making machine learning models interpretable. ESANN (2012).
[130] Jarkko Venna. 2007. Dimensionality reduction for visual exploration of similarity structures. Helsinki University of Technology.
[131] Julian Wagner and Alexander Benecke. 2016. National Legislation within the Framework of the GDPR. Eur. Data Prot. L. Rev. 2 (2016), 353.
[132] Richard J Wallace and Eugene C Freuder. 2001. Explanations for whom. In CP01 Workshop on User-Interaction in Constraint Satisfaction.
[133] Fulton Wang and Cynthia Rudin. 2015. Falling Rule Lists. In Arti?cial Intelligence and Statistics. 1013–1022.
[134] Ning Wang, David V Pynadath, and Susan G Hill. 2016. Trust Calibration Within a Human-Robot Team: Comparing Automatically Generated
Explanations. In ?e Eleventh ACM/IEEE International Conference on Human Robot Interaction (HRI ’16). IEEE Press, Piscataway, NJ, USA, 109–116.
[135] Adrian Weller. 2017. Challenges for Transparency. (2017).
[136] Christopher D Wickens, Rena Conejo, and Keith Gempler. 1999. Unreliable Automated A?ention Cueing for Air-Ground Targeting and Tra?c
Maneuvering. Proc. Hum. Fact. Ergon. Soc. Annu. Meet. 43, 1 (1 Sept. 1999), 21–25.
[137] C K I Williams and D Barber. 1998. Bayesian classi?cation with Gaussian processes. IEEE Trans. Pa?ern Anal. Mach. Intell. 20, 12 (Dec. 1998),
1342–1351.
[138] Jane Wu, Erin Paeng, Kari Linder, Piercarlo Valdesolo, and James C Boerkoel. 2016. Trust and Cooperation in Human-Robot Decision Making. In
2016 AAAI Fall Symposium Series.
[139] Adam Zagorecki, Marcin Kozniewski, and Marek Druzdzel. 2015. An Approximation of Surprise Index as a Measure of Con?dence. In 2015 AAAI
Fall Symposium Series.
[140] Peng Zhang, Jiuling Wang, Ali Farhadi, Martial Hebert, and Devi Parikh. 2014. Predicting failures of vision systems. In Proceedings of the IEEE
Conference on Computer Vision and Pa?ern Recognition. 3566–3573.
[141] Grzegorz Zycinski, Margherita Squillario, Annalisa Barla, Tiziana Sanavia, Alessandro Verri, and Barbara Di Camillo. 2012. Discriminant functional
gene groups identi?cation with machine learning and prior knowledge. In ESANN.
Manuscript submi?ed to ACM
