Recovery analysis for weighted mixed `2/`p minimization
with 0 < p ? 1
Zhiyong Zhou?, Jun Yu
Department of Mathematics and Mathematical Statistics, Ume?a University,
Ume?a, 901 87, Sweden
September 4, 2017
Abstract: We study the recovery conditions of weighted mixed `2/`p (0 < p ? 1) minimization for block
sparse signals reconstruction from compressed measurements when partial block support information is
available. We show that the block p-restricted isometry property can ensure the robust recovery. Moreover,
we present the sufficient and necessary condition for the recovery by using weighted block p-null space
property. The relationship between the block p-RIP and the weighted block p-null space property has been
established at the same time. Finally, we illustrate our results with a series of numerical experiments.
Keywords: Compressive sensing; Prior support information; Block sparse; Non-convex minimization.
1 Introduction
Since its advent [3–5, 10], Compressive Sensing (CS) has attracted considerable attentions (see the
monographs [13, 18] for a comprehensive view). CS aims to recover an unknown signal with underde-
termined linear measurements. Specifically, the standard CS problem consists in the reconstruction of a
N -dimensional sparse or compressible signal x from a significantly fewer number of linear measurements
y = Ax+e ? Rm with m N and the noise e satisfying ?e?2 ? ? for some known constant ? > 0. Then, if
the measurement matrix A satisfies the restricted isometry property (RIP) condition, the robust recovery
can be guaranteed by using the `1 minimization
min
z
?z?1, subject to ?y ?Az?2 ? ?. (1)
To obtain better recovery performance, the structures and some prior information of signals are in-
corporated in the recovery algorithms. In this paper, we consider both cases. As for the structure, we
assume that the unknown signal x is block sparse or nearly block sparse [11, 12, 14–16], which means that
the nonzero entries of x occur in clusters. Block sparse model appears in many practical scenarios, such
as when dealing with multi-band signals [27], in measurements of gene expression levels [30], and in colour
imaging [24]. Moreover, block sparse model can be used to treat the problems of multiple measurement
vector (MMV) [8,14,26] and sampling signals that lie in a union of subspaces [2, 14,27].
?Corresponding author, zhiyong.zhou@umu.se. This work is supported by the Swedish Research Council grant (Reg.No.
340-2013-5342).
1
ar
X
iv
:1
70
9.
00
25
7v
1 
 [
cs
.I
T
] 
 1
 S
ep
 2
01
7
With N =
?n
i=1 di, we define the i-th block x[i] of a length-N vector x over I = {d1, · · · , dn}. The
i-th block is of length di, and the blocks are formed sequentially so that
x = (x1 · · ·xd1? ?? ?
xT [1]
xd1+1 · · ·xd1+d2? ?? ?
xT [2]
· · ·xN?dn+1 · · ·xN? ?? ?
xT [n]
)T . (2)
Without loss of generality, we assume that d1 = d2 = · · · = dn = d, implying that N = nd. A vector
x ? RN is called block k-sparse over I = {d, · · · , d} if x[i] is nonzero for at most k indices i. In other
words, by denoting ?x?2,0 =
?n
i=1 I(?x[i]?2 > 0), a block k-sparse vector x can be defined by ?x?2,0 ? k.
For any p > 0, we define the mixed `2/`p norm ?x?2,p = (
?n
i=1?x[i]?
p
2)
1/p. To make explicit use of the
block structure to achieve better sparse recovery performance, the corresponding extended version of sparse
representation algorithm has been developed, namely the mixed `2/`1 minimization,
min
z
?z?2,1, subject to ?y ?Az?2 ? ?. (3)
It was shown in [14] that if the measurement matrix A satisfies the block RIP condition which generalizes
the conventional RIP notion, then the mixed `2/`1-norm recovery algorithm is guaranteed to recover any
block sparse signal, irrespectively of the locations of the nonzero blocks. Furthermore, recovery will be
robust in the presence of noise and modeling errors (i.e., when the vector is not exactly block sparse).
Other existing recovery algorithms include group lasso [39], adaptive lasso [23], iterative reweighted `2/`1
recovery algorithms [40], block version of the CoSaMP algorithm [12], and the extensions of the CoSaMP
algorithm and of the Iterative Hard Thresholding algorithm [1].
On the other hand, we also consider the case that an estimate of the block support of the signal is
available. The related literatures on signal recovery using partial support or block support information
include [19–22, 28, 29, 31–33, 38]. For an arbitrary signal x ? RN defined as (2), let xk be its best approx-
imation with k nonzero blocks, so that xk minimizes ?x ? g?2,1 over all block k-sparse vectors g. Let T0
be the block support of xk, where T0 ? {1, · · · , n} and |T0| ? k. Let T? , the block support estimate, be a
subset of {1, 2 · · · , n} with cardinality |T? | = ?k, where 0 ? ? ? a for some a > 1 and |T? ? T0| = ??k (for
interpretation of ? and ? see Remark 1 in Section 2). To incorporate prior block support information T? ,
the weighted mixed `2/`1 minimization
min
z
n?
i=1
wi?z[i]?2, subject to ?y ?Az?2 ? ?, where wi =
???? ? [0, 1], i ? T?1, i ? T? c (4)
is adopted. The main idea is to choose ? such that the `2 norm of the blocks of x that are ”expected” to
be large are penalized less in the weighted objective function.
Moreover, as is shown in many literatures [6, 17, 37], `p-minimization with 0 < p < 1 allows exact
recovery with fewer measurements than that by `1-minimization. Thus, it is natural to adopt the nonconvex
minimization to the setting of block sparse signal reconstruction with prior block support information.
Specifically, we consider the weighted mixed `2/`p (0 < p ? 1) minimization problem:
min
z
n?
i=1
wi?z[i]?p2, subject to ?y ?Az?2 ? ?, where wi =
???? ? [0, 1], i ? T?1, i ? T? c . (5)
2
When there is no prior block support information (? = 1), the mixed `2/`p(0 < p ? 1) minimization
problem has been studied in [34, 35]. And the case that there is partially known signal block support but
with ? = 0 was considered in [20]. We generalize the existing results to incorporating the prior known
block support information with a weight ? ? [0, 1]. In summary, the main contributions of our work include
the following aspects. First, we provide the recovery analysis for the weighted mixed `2/`p (0 < p ? 1)
minimization by using block p-RIP condition. This result extends the existing literatures [19, 20, 33–35].
Second, we propose the weighted block p-null space property and present the sufficient and necessary
condition for the weighted mixed `2/`p (0 < p ? 1) minimization by this new tool. Third, we establish the
relationship between block p-RIP condition and weighted block p-null space property. Finally, we illustrate
our results via a series of simulations.
The paper is organized as follows. In Section 2, we present the sufficient recovery condition by using
block p-RIP. In Section 3, we introduce the notion of weighted block p-null space property (NSP) and
establish the sufficient and necessary condition with this new tool. In Section 4, we establish the relationship
between these two conditions. In Section 5, we conduct some simulations to illustrate the theoretical results.
Section 6 is devoted to the conclusion.
2 Block p-RIP
As for the weighted mixed `2/`p (0 < p ? 1) minimization, we can obtain the reconstruction guarantees
by using block p-RIP. We begin with introducing the definition of block restricted p-isometry constant,
which is a natural extension of the conventional restricted p-isometry constant.
Definition 1 ( [7, 20,34,35]) Given a measurement matrix A ? Rm×N and 0 < p ? 1. Then the block
p-restricted isometry constant (RIC) ?k|I over I = {d1, · · · , dn} of order k is defined to be the smallest
positive number such that
(1? ?k|I)?x?
p
2 ? ?Ax?
p
p ? (1 + ?k|I)?x?
p
2 (6)
for all x ? RN that are block k-sparse over I.
For convenience, we write ?k for the block p-RIC ?k|I whenever there is no confusion. Then, we have
the sufficient condition for the robust recovery result by using (5) with the block p-RIC ?k.
Theorem 1 Let x ? RN , and xk be its best approximation with k nonzero blocks, supported on block
index set T0. Let T? ? {1, 2, · · · , n} be an arbitrary set. Define ? and ? as before such that |T? | = ?k and
|T? ?T0| = ??k. Suppose that there exists an a ? Z, with a ? (1??)?, a > 1, and the measurement matrix
A satisfies
?ak +
a1?p/2
?
?(a+1)k <
a1?p/2
?
? 1, (7)
3
where ? = ? + (1? ?)(1 + ?? 2??)1?p/2 for some given 0 ? ? ? 1. Then the solutions x] of problem (5)
obeys
?x] ? x?2 ? C1
(
??x? xk?p2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)1/p
k1/p?1/2
+ C2?, (8)
for some positive constants C1 and C2.
Remark 1. Note that this theorem involves two important ratios: ? determines the ratio of the estimated
block support size to the actual block support of xk (or the block support of x if x is block k-sparse),
while ? determines the ratio of the number of block indices in block support of xk that were accurately
estimated in T? to the size of T? . Specifically, ? = |T??T0||T? | .
Remark 2. The constants C1 and C2 are explicitly given by the following expressions:
C1 =
22/p?1a1/2?1/p
[
(1 + ?ak)
1/p + (1? ?(a+1)k)1/p
]
[(1? ?(a+1)k)? ap/2?1(1 + ?ak)?]1/p
,
C2 =
21/pm1/p?1/2(1 + a1/2?1/p?1/p)
[(1? ?(a+1)k)? ap/2?1(1 + ?ak)?]1/p
.
Remark 3. For Theorem 1 to be held, it is sufficient that A satisfies
?(a+1)k < ? :=
a1?p/2 ? [? + (1? ?)(1 + ?? 2??)1?p/2]
a1?p/2 + [? + (1? ?)(1 + ?? 2??)1?p/2]
. (9)
Next, we illustrate how the slighted stronger sufficient conditions vary with ? and ? for p = 0.01, 0.5, 1,
respectively. In Figure 1, for each p, we plot, for different values of ?, ? versus ?, where we set the pa-
rameters a = 3 and ? = 1. We observe that as ? increases the sufficient condition on the block p-RIC
becomes weaker, allowing for a wider class of measurement matrices A. And for each p with ? > 0.5, see
? = 0.7 and 0.9 in our setting, as ? decreases, the sufficient condition becomes weaker. For instance, when
? = 0.7, p = 0.5, with ? = 0.2 it suffices to have ? = 0.5072, compared with ? = 0.3902 for ? = 1. The
opposite conclusion holds for the case ? < 0.5. When ? = 0.5, the sufficient condition remains the same
for different ?. From another point of view, as p decreases, the sufficient condition becomes weaker, which
reflects the benefits of using nonconvex minimization.
Remark 4. By setting ? = 1 and a = b2/(2?p) with b > 1, this theorem reduces to Theorem 2.1 in [35].
In addition, by setting ? = 0, ? = 0.5 and a = ? = b2/(2?p) with b > 1, then this theorem goes to Theorem
2 in [20] with s = ak.
Proof. Let x] = x+ h be a solution of problem (5), where x is the unknown true signal. Throughout the
paper, xT will denote the vector equal to x on the block index set T and zero elsewhere. Then, we have
??x]
T?
?p2,p + ?x
]
T? c
?p2,p ? ??xT? ?
p
2,p + ?xT? c?
p
2,p.
4
Figure 1: Comparison of the sufficient conditions for the block restricted p-isometry constants with various of ? for
p = 0.01, 0.5, 1. We set a = 3 and ? = 1 for all the figures.
That is,
??xT? + hT? ?
p
2,p + ?xT? c + hT? c?
p
2,p ? ??xT? ?
p
2,p + ?xT? c?
p
2,p.
Consequently,
??xT??T0 + hT??T0?
p
2,p + ??xT??T c0 + hT??T c0 ?
p
2,p + ?xT? c?T0 + hT? c?T0?
p
2,p + ?xT? c?T c0 + hT? c?T c0 ?
p
2,p
? ??xT??T0?
p
2,p + ??xT??T c0 ?
p
2,p + ?xT? c?T0?
p
2,p + ?xT? c?T c0 ?
p
2,p.
The forward and reverse triangle inequalities implies
??hT??T c0 ?
p
2,p + ?hT? c?T c0 ?
p
2,p ? ?hT? c?T0?
p
2,p + ??hT??T0?
p
2,p + 2
(
?xT? c?T c0 ?
p
2,p + ??xT??T c0 ?
p
2,p
)
.
Adding and subtracting ??hT? c?T c0 ?
p
2,p on the left hand side, and ??hT? c?T0?
p
2,p on the right, we obtain
??hT c0 ?
p
2,p + (1? ?)?hT? c?T c0 ?
p
2,p ? ??hT0?
p
2,p + (1? ?)?hT? c?T0?
p
2,p + 2
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)
.
But we can also write
?hT c0 ?
p
2,p = ??hT c0 ?
p
2,p + (1? ?)?hT? c?T c0 ?
p
2,p + (1? ?)?hT??T c0 ?
p
2,p.
Therefore, we have
?hT c0 ?
p
2,p ? ??hT0?
p
2,p + (1? ?)?hT??T c0 ?
p
2,p + (1? ?)?hT? c?T0?
p
2,p + 2
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)
.
5
Let the set T?? = T0 ? T? , then we can write ?hT??T c0 ?
p
2,p + ?hT? c?T0?
p
2,p = ?h(T0?T? )\T???
p
2,p and simplify the
bound on
?hT c0 ?
p
2,p ? ??hT0?
p
2,p + (1? ?)?h(T0?T? )\T???
p
2,p + 2
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)
. (10)
Next, we decompose hT c0 into disjoint block index set Tj , each of Tj (j ? 1) consists of ak blocks, where
a > 1. That is, T1 indexes the ak blocks with largest `2 norm of hT c0 , T2 indexes the second ak blocks with
largest `2 norm of h(T0?T1)c , and so on. Then this gives hT c0 =
?
j?1
hTj . For each i ? Tj (j ? 2), it is easy
to see that
?hTj [i]?
p
2 ?
?hTj?1 [i]?
p
2 + · · ·+ ?hTj?1 [ak]?
p
2
ak
=
?hTj?1?
p
2,p
ak
.
Then
?hTj [i]?22 ?
?hTj?1?22,p
(ak)2/p
, ?hTj?22 ?
ak?hTj?1?22,p
(ak)2/p
,
?hTj?
p
2 ?
?hTj?1?
p
2,p
(ak)1?p/2
.
Thus,
?h(T0?T1)c?2 ? ?
?
j?2
hTj?2 ?
?
j?2
?hTj?2 ? (ak)1/2?1/p
?
j?2
?hTj?1?2,p = (ak)1/2?1/p
?
j?1
?hTj?2,p.
Hence,
?h(T0?T1)c?
p
2 ? (ak)
p/2?1
???
j?1
?hTj?2,p
??p ? (ak)p/2?1?
j?1
?hTj?
p
2,p = (ak)
p/2?1?hT c0 ?
p
2,p.
Combining the above expression with (10), we get
?h(T0?T1)c?
p
2 ? (ak)
p/2?1
[
??hT0?
p
2,p + (1? ?)?h(T0?T? )\T???
p
2,p + 2
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)]
.
(11)
By Ho?lder’s inequality it follows that
?Ah?pp ?
(
m?
i=1
(|(Ah)i|p)2/p
)p/2
·
(
m?
i=1
1
)1?p/2
= m1?p/2?Ah?p2.
Since x] is a solution of problem (5) and ?y ?Ax?2 = ?e?2 ? ?, so we have
?Ah?2 = ?A(x] ? x)?2 ? ?Ax? y?2 + ?Ax] ? y?2 ? 2?.
Thus, ?Ah?pp ? m1?p/2?Ah?p2 ? m1?p/2(2?)p. Therefore, we obtain
?AhT0?T1?pp ? ?Ah?pp + ?Ah(T0?T1)c?
p
p ? m1?p/2(2?)p +
?
j?2
?AhTj?pp
? m1?p/2(2?)p + (1 + ?ak)
?
j?2
?hTj?
p
2.
6
Moreover, we have ?
j?2
?hTj?
p
2 ? (ak)
p/2?1
?
j?2
?hTj?1?
p
2,p ? (ak)
p/2?1?hT c0 ?
p
2,p.
Thus, we have
?AhT0?T1?pp ? m1?p/2(2?)p + (ak)p/2?1(1 + ?ak)?hT c0 ?
p
2,p
? m1?p/2(2?)p + 2(ak)p/2?1(1 + ?ak)
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)
+ ?(ak)p/2?1(1 + ?ak)?hT0?
p
2,p + (1? ?)(ak)
p/2?1(1 + ?ak)?h(T0?T? )\T???
p
2,p.
Noting that |(T0 ? T? ) \ T??| = (1 + ? ? 2??)k, then ?h(T0?T? )\T???
p
2,p ? [(1 + ?? 2??)k]
1?p/2 ?h(T0?T? )\T???
p
2.
Since the set T1 contains the ak blocks with largest `2 norm of hT c0 with a > 1, and |T? \T??| = (1??)?k ? ak,
then ?h(T0?T? )\T???
p
2 ? ?hT0?T1?
p
2, which implies that ?h(T0?T? )\T???
p
2,p ? [(1 + ?? 2??)k]
1?p/2 ?hT0?T1?
p
2. In
addition, ?hT0?
p
2,p ? k1?p/2?hT0?
p
2 ? k1?p/2?hT0?T1?
p
2. Thus,
(1? ?(a+1)k)?hT0?T1?
p
2 ? ?AhT0?T1?
p
p
? m1?p/2(2?)p + 2(ak)p/2?1(1 + ?ak)
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)
+ ?ap/2?1(1 + ?ak)?hT0?T1?
p
2 + (1? ?)a
p/2?1(1 + ?? 2??)1?p/2(1 + ?ak)?hT0?T1?
p
2.
Therefore, if (1? ?(a+1)k)? ap/2?1(1 + ?ak)? > 0, that is ?ak + a
1?p/2
? ?(a+1)k <
a1?p/2
? ? 1, then we have
?hT0?T1?
p
2 ?
m1?p/2(2?)p + 2(ak)p/2?1(1 + ?ak)
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)
(1? ?(a+1)k)? ap/2?1(1 + ?ak)?
, (12)
where ? = ? + (1? ?)(1 + ?? 2??)1?p/2. According to (11), we also have
?h(T0?T1)c?
p
2 ? a
p/2?1??hT0?T1?
p
2 + 2(ak)
p/2?1
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)
?
ap/2?1?m1?p/2(2?)p + 2(ak)p/2?1(1? ?(a+1)k)
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)
(1? ?(a+1)k)? ap/2?1(1 + ?ak)?
.
Therefore, since ?v?p ? 21/p?1?v?1 for all v ? R2, we have
?h?2 ? ?hT0?T1?2 + ?h(T0?T1)c?2
? 21/p?1
???2m1/p?1/2?+ 21/p(ak)1/2?1/p(1 + ?ak)1/p
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)1/p
[(1? ?(a+1)k)? ap/2?1(1 + ?ak)?]1/p
???
+ 21/p?1
???2a1/2?1/p?1/pm1/p?1/2?+ 21/p(ak)1/2?1/p(1? ?(a+1)k)1/p
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)1/p
[(1? ?(a+1)k)? ap/2?1(1 + ?ak)?]1/p
???
?
22/p?1(ak)1/2?1/p
[
(1 + ?ak)
1/p + (1? ?(a+1)k)1/p
] (
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)1/p
[(1? ?(a+1)k)? ap/2?1(1 + ?ak)?]1/p
7
+
21/pm1/p?1/2(1 + a1/2?1/p?1/p)?
[(1? ?(a+1)k)? ap/2?1(1 + ?ak)?]1/p
= C1
(
??xT c0 ?
p
2,p + (1? ?)?xT? c?T c0 ?
p
2,p
)1/p
k1/p?1/2
+ C2?,
which completes the proof.
Corollary 1 Let A ? Rm×N be a measurement matrix, x ? RN be a block k-sparse signal supported on
block index set T0 with y = Ax, and 0 < p ? 1. Let T? ? {1, 2, · · · , n} be an arbitrary set. Define ? and ?
as before such that |T? | = ?k and |T? ? T0| = ??k. Suppose that there exists an a ? Z, with a ? (1 ? ?)?,
a > 1, and the measurement matrix A satisfies
?ak +
a1?p/2
?
?(a+1)k <
a1?p/2
?
? 1, (13)
where ? = ? + (1? ?)(1 + ?? 2??)1?p/2 for some given 0 ? ? ? 1. Then the unique solution of problem
(5) with ? = 0 is exactly x.
Remark 5. According to the arguments in [20, 35], if we let A be an m × N matrix whose entries are
i.i.d Gaussian random variables with mean zero, then there exist C3(p) and C4(p) such that whenever
0 < p ? 1 and m ? C3(p)kd+ pC4(p)k ln(n/k), the block p-RIP (7) will hold and the block sparse signal x
can be exactly recovered with high probability. For a given p ? (0, 1], C3(p) and C4(p) are finite constants,
and the second term of the above expression has the dominant impact on the number of measurements
in an asymptotic sense. When p ? 0, the condition reduces to m ? C3(0)kd. While, when p = 1,
the required number of measurements m ? C3(1)kd + C4(1)k ln(n/k), which implies fewer measurements
are required with smaller p for exact recovery via weighted mixed `2/`p minimization than the case that
p = 1. This is largely consistent with what we have mentioned in Remark 3, namely, as p decreases, the
sufficient condition for the exact recovery becomes weaker. Though we have only considered the case that
d1 = d2 = · · · = dn = d, the results here can be adapted to the case in which di are not equal via replacing
d by the maximal block length max
1?i?n
di.
3 Weighted Block p-NSP
In this section, we focus on the noise free case, that is e = 0, and the signal x is exactly block k-sparse,
supported on block index set T0. We obtain the exact recovery condition for the problem (5) with ? = 0
by using weighted block p-null space property. For an index set V ? {1, · · · , n}, we define
?s(V ) := {U ? {1, · · · , n} : |(V ? U c) ? (V c ? U)| ? s}.
Definition 2 Let T ? {1, · · · , n} with |T | ? k and T? ? ?s(T ). Assume the block size equals d. A matrix
A ? Rm×N is said to have the weighted nonuniform block p-null space property with parameters T and T? ,
and constant C if for any vector h : Ah = 0, we have
??hT ?p2,p + (1? ?)?hS?
p
2,p ? C?hT c?
p
2,p, (14)
where S = (T? ? T c) ? (T? c ? T ). In this case, we say A satisfies ?-d-p-NSP(T, T? , C).
8
Next, we define a weighted uniform block p-null space property that leads to a sufficient and necessary
condition for the exact recovery of all block k-sparse signals from compressive measurements using weighted
mixed `2/`p minimization problem.
Definition 3 Assume the block size equals d. A matrix A ? Rm×N is said to have the weighted block
p-null space property with parameters k and s, and constant C if for any vector h : Ah = 0, and for every
block index set T ? {1, · · · , n} with |T | ? k and S ? {1, · · · , n} with |S| ? s, we have
??hT ?p2,p + (1? ?)?hS?
p
2,p ? C?hT c?
p
2,p. (15)
In this case, we say A satisfies ?-d-p-NSP(k, s, C).
Remark 6. Our notations ?-d-p-NSP(T, T? , C) and ?-d-p-NSP(k, s, C) are direct extensions of the recent
notations ?-NSP(T, T? , C) and ?-NSP(k, s, C) proposed by [25], which corresponds to the special case that
p = 1 and the block size d = 1. That’s to say ?-1-1-NSP(k, s, C) is equivalent to ?-NSP(k, s, C). Conse-
quently, 1-1-1-NSP(k, k, C) is the standard null space property of order k, i.e, NSP(k,C).
The following theorem presents the sufficient and necessary condition for weighted mixed `2/`p mini-
mization problem to recover all block k-sparse signals when the error in the block support estimate is of
size s or less.
Theorem 2 Assume the block size equals d. Given a matrix A ? Rm×N , every block k-sparse signal
x ? RN , supported on block index set T0, is the unique solution of the weighted mixed `2/`p, 0 < p ? 1
norm minimization problem (5) with ? = 0 and T? = ?s(T0), if and only if A satisfies ?-d-p-NSP(k, s, C)
for some positive constant C < 1.
Proof. a) ” ? ”. Assume if A does not satisfy ?-d-p-NSP(k, s, C) for any constant C > 1, then there
exists a vector h : Ah = 0 and block index set T with |T | ? k and block index set S with |S| ? s, such
that AhT = ?AhT c and
??hT ?p2,p + (1? ?)?hS?
p
2,p ? ?hT c?
p
2,p.
Define T? = (T c ? S) ? (T ? Sc) so that S = (T ? T? c) ? (T c ? T? ). Substituting for S and splitting the set
T , we obtain
?
(
?hT?T? c?
p
2,p + ?hT?T? ?
p
2,p
)
+ (1? ?)
(
?hT?T? c?
p
2,p + ?hT c?T? ?
p
2,p
)
= ?hT?T? c?
p
2,p + ??hT?T? ?
p
2,p + (1? ?)?hT c?T? ?
p
2,p ? ?hT c?
p
2,p.
Then, we have
??hT?T? ?
p
2,p + ?hT?T? c?
p
2,p ? ??hT c?T? ?
p
2,p + ?hT c?T? c?
p
2,p.
In other words, the weighted mixed `2/`p norm of the vector hT equals or exceeds that of ?hT c . So hT is
not the unique minimizer, which is contradictory to our condition. Thus, the necessity is verified.
9
b) ” ? ”. Let x] be a minimizer of weighted mixed `2/`p, 0 < p ? 1 norm minimization problem (5)
with ? = 0 and T? = ?s(T0) and define h = x
] ? x. Then, by the optimality of x], we have
??xT? + hT? ?
p
2,p + ?xT? c + hT? c?
p
2,p ? ??xT? ?
p
2,p + ?xT? c?
p
2,p,
which is equivalent to
??xT??T0 + hT??T0?
p
2,p + ??xT??T c0 + hT??T c0 ?
p
2,p + ?xT? c?T0 + hT? c?T0?
p
2,p + ?xT? c?T c0 + hT? c?T c0 ?
p
2,p
? ??xT??T0?
p
2,p + ??xT??T c0 ?
p
2,p + ?xT? c?T0?
p
2,p + ?xT? c?T c0 ?
p
2,p.
Since x is strictly block k-sparse and supported on the block index set T0, thus xT c0 = 0. Then we have
??hT??T c0 ?
p
2,p + ?hT? c?T c0 ?
p
2,p ? ?hT? c?T0?
p
2,p + ??hT??T0?
p
2,p.
Adding and subtracting ??hT? c?T c0 ?
p
2,p on the left hand side, and ??hT? c?T0?
p
2,p on the right, we obtain
??hT??T c0 ?
p
2,p + ??hT? c?T c0 ?
p
2,p + ?hT? c?T c0 ?
p
2,p ? ??hT? c?T c0 ?
p
2,p
? ??hT??T0?
p
2,p + ??hT? c?T0?
p
2,p + ?hT? c?T0?
p
2,p ? ??hT? c?T0?
p
2,p.
Therefore,
??hT c0 ?
p
2,p + (1? ?)?hT? c?T c0 ?
p
2,p ? ??hT0?
p
2,p + (1? ?)?hT? c?T0?
p
2,p.
Finally, by adding (1? ?)?hT??T c0 ?
p
2,p to both sides, we have
?hT c0 ?
p
2,p ? ??hT0?
p
2,p + (1? ?)?hT??T c0 ?
p
2,p + (1? ?)?hT? c?T c0 ?
p
2,p
= ??hT0?
p
2,p + (1? ?)?hS?
p
2,p, (16)
by setting S = (T? ? T c0 )? (T? c ? T0). Note that when |S| ? s, the above inequality is in contradiction with
that A satisfies ?-d-p-NSP(k, s, C) for some constant C < 1, unless h = 0. Hence, we have x] = x, and the
sufficiency holds.
4 From Block p-RIP to Weighted Block p-NSP
In this part, we establish the relationship between the block p-RIP and weighted block p-NSP, that is
block p-RIP can directly imply the weighted block p-NSP, then we can obtain the exact recovery for every
block k-sparse signal via the weighted mixed `2/`p, 0 < p ? 1 norm minimization problem (5) with ? = 0,
as presented in the following theorem.
Theorem 3 Assume the block size equals d and let x ? RN be block k-sparse, supported on block index
set T0. Let T? ? {1, 2, · · · , n} be an arbitrary set and define ? and ? as before such that |T? | = ?k and
|T? ? T0| = ??k. Let s = (1 + ? ? 2??)k, then we have T? = ?s(T0). Suppose that there exists an a ? Z,
with a ? (1? ?)?, a > 1, and the measurement matrix A satisfies
?ak +
a1?p/2
?
?(a+1)k <
a1?p/2
?
? 1, (17)
where ? = ? + (1? ?)(s/k)1?p/2 for some given 0 ? ? ? 1. Then A satisfies ?-d-p-NSP(k, s, C) for some
constant C < 1.
10
Consequently, according to Theorem 2, we have x is the unique solution of problem (5) with ? = 0.
Proof. To show for any vector h : Ah = 0, and for every block index set T ? {1, · · · , n} with |T | ? k, and
S ? {1, · · · , n} with |S| ? s, we have
??hT ?p2,p + (1? ?)?hS?
p
2,p ? C?hT c?
p
2,p, (18)
for some constant C < 1, we only need to show (18) holds for T = G0 and S = H0, where G0 is the block
index set over the k blocks with largest `2 norm of h, H0 is the block index set over the s blocks with
largest `2 norm of h.
Next, we decompose hGc0 into disjoint block index set Gj , each of Gj (j ? 1) consists of ak blocks. That
is, G1 indexes the ak blocks with largest `2 norm of hGc0 . G2 indexes the second ak blocks with largest `2
norm of h(G0?G1)c , and so on.
Since Ah = 0, then we have
?AhG0?G1?pp ? ?Ah(G0?G1)c?
p
p ?
?
j?2
?AhGj?pp ? (1 + ?ak)
?
j?2
?hGj?
p
2.
Moreover, we have ?
j?2
?hGj?
p
2 ? (ak)
p/2?1
?
j?2
?hGj?1?
p
2,p ? (ak)
p/2?1?hGc0?
p
2,p.
Thus,
(1? ?(a+1)k)?hG0?G1?
p
2 ? ?AhG0?G1?
p
p ? (1 + ?ak)(ak)p/2?1?hGc0?
p
2,p.
It implies that
?hG0?G1?
p
2 ?
(ak)p/2?1(1 + ?ak)
1? ?(a+1)k
?hGc0?
p
2,p. (19)
In addition, ?hG0?
p
2,p ? k1?p/2?hG0?
p
2 ? k1?p/2?hG0?G1?
p
2. Hence,
?hG0?
p
2,p ? k
1?p/2?hG0?G1?
p
2 ?
ap/2?1(1 + ?ak)
1? ?(a+1)k
?hGc0?
p
2,p. (20)
For the term ?hH0?
p
2,p, we have ?hH0?
p
2,p ? s1?p/2?hH0?
p
2. Moreover, if s ? k, we have H0 ? G0
and ?hH0?
p
2 ? ?hG0?
p
2 ? ?hG0?G1?
p
2. If s > k, then G0 ? H0. However, since |H0 \ G0| = s ? k =
(1 + ? ? 2??)k ? k = (1 ? 2?)?k ? ak, then we also have ?hH0?
p
2 ? ?hG0?G1?
p
2 as the block index set G1
contains the ak blocks with largest `2 norm of hGc0 . As a consequence,
?hH0?
p
2,p ? s
1?p/2?hH0?
p
2 ? s
1?p/2?hG0?G1?
p
2 ?
(ak/s)p/2?1(1 + ?ak)
1? ?(a+1)k
?hGc0?
p
2,p. (21)
Therefore, we obtain
??hG0?
p
2,p + (1? ?)?hH0?
p
2,p ? ?
ap/2?1(1 + ?ak)
1? ?(a+1)k
?hGc0?
p
2,p + (1? ?)
(ak/s)p/2?1(1 + ?ak)
1? ?(a+1)k
?hGc0?
p
2,p
=
ap/2?1?(1 + ?ak)
1? ?(a+1)k
?hGc0?
p
2,p.
Let C = a
p/2?1?(1+?ak)
1??(a+1)k
, we have C < 1 under the condition of block p-RIP. The proof is completed.
11
5 Simulation
In this section, we conduct several simulations to illustrate our presented theoretical results. We adopt
the iteratively reweighted least squares (IRLS) approach to solve the nonconvex optimization problem. We
begin with x(0) = arg min ?y ?Ax?22, and set ?0 = 1. Then, let x(t+1) be the solution of
min
x
1
2?
?y ?Ax?22 +
1
2
?W (t)x?22, (22)
where ? > 0 is a regularization parameter, and the weight matrix W (t) is defined as W
(t)
i = diag((?t +
?
4
p(p?2)
i ??
1/p
i x
(t)[i]?22)p/4?1/2) for the i-th block. Then, x(t+1) can be given explicitly as
x(t+1) = (W (t))?1
(
[A(W (t))?1]T [A(W (t))?1] + ?I
)?1
[A(W (t))?1]T y.
The value of ? is decreased according to the rule ?t+1 = 0.1?t, and the iteration is continued until ?x(t+1)?
x(t)?2 ? 10?5 or the iteration times is larger than 2500. In our experiments, the measurement matrix
A is generated as an m × N matrix with entries drawing from i.i.d standard normal distribution. For
a generated block sparse or nearly block sparse signal x, the measurements y = Ax + ?z with z being
the standard Gaussian white noise. We consider several different parameter values to demonstrate the
theoretical results. In each experiment, we report the average results over 20 replications. For IRLS, we
set ? = 10?6 in the noise free case (? = 0), and ? = 10?2 in the noisy case (? = 0.01). The recovery
performance is assessed by the signal to noise ratio (SNR), which is given by
SNR = 20 log10
(
?x?2
?x? x]?2
)
.
5.1 Exactly block sparse case
We first consider the case that x is exactly block k-sparse with k = 20. In this set of experiments,
the signal is of length N = 400, which is generated by choosing k blocks uniformly at random, and then
choosing the nonzero values from the standard normal distribution for these k blocks.
Figure 2 illustrates the recovery performance of the weighted `2/`p minimization problem with p = 0.5
and d = 2 in both with and without noise cases. It can be observed that when ? = 0.7, the best recovery is
achieved for very small ? whereas a ? = 1 results in the worst SNR. On the other hand, when ? ? 0.5, the
performance of the recovery algorithms is better for large ? than that for small ?. The case ? = 0 results
in the worst SNR. Figure 3 shows the averaged SNR using weighted mixed `2/`p minimization for different
values of the ratio parameter ? with fixed p = 0.5 and d = 2. It shows that when ? = 0.7, using a larger
support estimate results in better reconstruction. However, in both the noise free and noisy measurements
cases, the recovery performance is more sensitive to the accuracy ? of the block support estimate than the
ratio parameter ?.
In addition, we illustrate the impacts of p and d in Figure 4 for both the noise free and noisy mea-
surements cases. We fix k = 20, ? = 0.7 and ? = 0.5. It is evident that as p decreases, the recovery
performance becomes better. And as d increases, more number of measurements are required to obtain
good reconstructions. These are largely consistent with the theoretical results in Section 2.
12
Figure 2: Performance of weighted mixed `2/`p recovery with p = 0.5 in terms of SNR for exactly block sparse signal
x depending on ? with k = 20, d = 2, while varying the number of measurements m.
Figure 3: Performance of weighted mixed `2/`p recovery with p = 0.5 in terms of SNR for exactly block sparse signal
x depending on ? with k = 20, d = 2, while varying the size of the block support estimate ? as a proportion of k.
13
Figure 4: Performance of weighted mixed `2/`p recovery in terms of SNR for exactly block sparse signal x depending
on p with k = 20, ? = 0.7, ? = 0.5, while varying the number of measurements m.
5.2 Nearly block sparse case
Next, we generate x with the `2 norm of its blocks decay like i
?? where i ? {1, · · · , n} and ? > 1. In
Figure 5, we illustrate the averaged SNR versus the size of the block support estimate ? as a proportion
of k for ? = 1.5. To calculate ?, we set k = 20, i.e., we are interested in the best 20-term block sparse
approximation. It shows that mediate values of ? (e.g., ? = 0.2 or 0.5) results in the best recovery.
Generally, larger block support estimate favours better reconstruction result. Finally, we illustrate the
impacts of ? and d for different p in Figure 6. For ? = 1.5 we set k = 20, while for ? = 2, we consider
k = 10. It is evident that the recovery performance improves as ? increases, i.e., the block sparsity of
signal increases. Moreover, decreasing p and d improves the recovery performance when other parameters
are fixed.
6 Conclusion
In this paper, we presented the recovery analysis of weighted mixed `2/`p (0 < p ? 1) minimization
by using both block p-RIP and weighted block p-null space property. In addition, we established the rela-
tionship between these two conditions. A series of simulations were conducted to illustrate our theoretical
results.
There are some interesting issues left for future work. One is that it maybe possible to generalize
`2-constrained minimization problem considered in this paper to the `q-constrained minimization problem
with 2 ? q ? ? (see [9, 32]) and with 0 ? q < 2 ( [36]). Another issue that is not addressed here is to
14
Figure 5: Performance of weighted mixed `2/`p recovery in terms of SNR for nearly block sparse signal x with
p = 0.5,m = 80, ? = 1.5, k = 20, ? = 0.7, ? = 0.5, while varying the size of the block support estimate ? as a
proportion of k.
Figure 6: Performance of weighted mixed `2/`p recovery in terms of SNR for nearly block sparse signal x for different
? and d depending on p with fixed ? = 0.7, ? = 0.5, ? = 0.01 for all the cases, while varying the number of
measurements m.
15
obtain the minimum required number of measurements for perfect recovery directly via weighted block
p-null space property.
References
[1] Baraniuk, R.G., Cevher, V., Duarte, M.F. and Hegde, C. (2010). Model-based compressive sensing.
IEEE Transactions on Information Theory 56(4) 1982–2001.
[2] Blumensath, T. and Davies, M.E. (2009). Sampling theorems for signals from the union of finite-
dimensional linear subspaces. IEEE Transactions on Information Theory 55(4) 1872–1882.
[3] Candes, E.J., Romberg, J.K. and Tao, T. (2006). Stable signal recovery from incomplete and inaccurate
measurements. Communications on Pure and Applied Mathematics 59(8) 1207–1223.
[4] Candes, E.J. and Tao, T. (2005). Decoding by linear programming. IEEE Transactions on Information
Theory 51(12) 4203–4215.
[5] Candes, E.J. and Tao, T. (2006). Near-optimal signal recovery from random projections: Universal
encoding strategies?. IEEE Transactions on Information Theory 52(12) 5406–5425.
[6] Chartrand, R. (2007). Exact reconstruction of sparse signals via nonconvex minimization. IEEE Signal
Processing Letters 14(10) 707–710.
[7] Chartrand, R. and Staneva, V. (2008). Restricted isometry properties and nonconvex compressive
sensing. Inverse Problems 24 1–14.
[8] Chen, J. and Huo, X. (2006). Theoretical results on sparse representations of multiple-measurement
vectors. IEEE Transactions on Signal Processing 54(12) 4634–4643.
[9] Dirksen, S., Lecue?, G. and Rauhut, H. (2016). On the gap between restricted isometry
properties and sparse recovery conditions. IEEE Transactions on Information Theory, (DOI:
10.1109/TIT.2016.2570244).
[10] Donoho, D.L. (2006). Compressed sensing. IEEE Transactions on Information Theory 52(4) 1289–
1306.
[11] Duarte, M.F. and Eldar, Y.C. (2011). Structured compressed sensing: From theory to applications.
IEEE Transactions on Signal Processing 59(9) 4053–4085.
[12] Eldar, Y.C., Kuppinger, P. and Bolcskei, H. (2010). Block-sparse signals: Uncertainty relations and
efficient recovery. IEEE Transactions on Signal Processing 58(6) 3042–3054.
[13] Eldar, Y.C. and Kutyniok, G. (2012). Compressed Sensing: Theory and Applications. Cambridge
University Press.
[14] Eldar, Y.C. and Mishali, M. (2009). Robust recovery of signals from a structured union of subspaces.
IEEE Transactions on Information Theory 55(11) 5302–5316.
[15] Eldar, Y. C. and Rauhut, H. (2010). Average case analysis of multichannel sparse recovery using
convex relaxation. IEEE Transactions on Information Theory 56(1) 505–519.
[16] Elhamifar, E. and Vidal, R. (2012). Block-sparse recovery via convex optimization. IEEE Transactions
on Signal Processing 60(8) 4094–4107.
16
[17] Foucart, S. and Lai, M.J. (2009). Sparsest solutions of underdetermined linear systems via `q-
minimization for 0 < q ? 1. Applied and Computational Harmonic Analysis 26(3) 395-407.
[18] Foucart, S. and Rauhut, H. (2013). A Mathematical Introduction to Compressive Sensing. New York:
Springer-Verlag.
[19] Friedlander, M.P., Mansour, H., Saab, R. and Yilmaz, O. (2012). Recovering compressively sampled
signals using partial support information. IEEE Transactions on Information Theory 58(2) 1122–1134.
[20] He, S., Wang, Y., Wang, J. and Xu, Z. (2016). Block-sparse compressed sensing with partially known
signal support via non-convex minimisation. IET Signal Processing 10(7) 717–723.
[21] Jacques, L. (2010). A short note on compressed sensing with partially known signal support. Signal
Processing 90(12) 3308–3312.
[22] Khajehnejad, M.A., Xu, W., Avestimehr, A.S. and Hassibi, B. (2009). Weighted `1 minimization for
sparse recovery with prior information. In Information Theory, 2009. ISIT 2009. IEEE International
Symposium on (pp. 483-487).
[23] Lv, X., Bi, G. and Wan, C. (2011). The group lasso for stable recovery of block-sparse signal repre-
sentations. IEEE Transactions on Signal Processing 59(4) 1371–1382.
[24] Majumdar, A. and Ward, R.K. (2010). Compressed sensing of color images. Signal Processing 90(12)
3122–3127.
[25] Mansour, H. and Saab, R. (2017). Recovery analysis for weighted `1-minimization using the null space
property. Applied and Computational Harmonic Analysis 43(1) 23–38.
[26] Mishali, M. and Eldar, Y.C. (2008). Reduce and boost: Recovering arbitrary sets of jointly sparse
vectors. IEEE Transactions on Signal Processing 56(10) 4692–4702.
[27] Mishali, M. and Eldar, Y.C. (2009). Blind multiband signal reconstruction: Compressed sensing for
analog signals. IEEE Transactions on Signal Processing 57(3) 993–1009.
[28] Needell, D., Saab, R. and Woolf, T. (2017). Weighted `1-minimization for sparse recovery under
arbitrary prior information. Information and Inference, iaw023.
[29] Ince, T., Nacaroglu, A.,and Watsuji, N. (2013). Nonconvex compressed sensing with partially known
signal support. Signal Processing 93(1) 338–344.
[30] Parvaresh, F., Vikalo, H., Misra, S. and Hassibi, B. (2008). Recovering sparse signals using sparse
measurement matrices in compressed DNA microarrays. IEEE Journal of Selected Topics in Signal
Processing 2(3) 275–285.
[31] Rauhut, H. and Ward, R. (2016). Interpolation via weighted `1 minimization. Applied and Computa-
tional Harmonic Analysis 40(2) 321–351.
[32] Saleh, A.A., Alajaji, F., and Chan, W.Y. (2015). Compressed sensing with non-Gaussian noise and
partial support information. IEEE Signal Processing Letters 22(10) 1703–1707.
[33] Vaswani, N. and Lu, W. (2010). Modified-CS: Modifying compressive sensing for problems with par-
tially known support. IEEE Transactions on Signal Processing 58(9) 4595-4607.
[34] Wang, Y., Wang, J. and Xu, Z. (2013). On recovery of block-sparse signals via mixed `2/`q(0 < q ? 1)
norm minimization. EURASIP Journal on Advances in Signal Processing 2013(1):76.
17
[35] Wang, Y., Wang, J. and Xu, Z. (2014). Restricted p-isometry properties of nonconvex block-sparse
compressed sensing. Signal Processing 104 188–196.
[36] Wen, F., Liu, P., Liu, Y., Qiu, R.C. and Yu, W. (2017). Robust Sparse Recovery in Impulsive Noise
via `p-`1 Optimization. IEEE Transactions on Signal Processing 65(1) 105–118.
[37] Xu, Z., Chang, X., Xu, F. and Zhang, H. (2012). `1/2 regularization: A thresholding representation
theory and a fast solver. IEEE Transactions on Neural Networks and Learning Systems 23(7) 1013–1027.
[38] Yu, X. and Baek, S.J. (2013). Sufficient conditions on stable recovery of sparse signals with partial
support information. IEEE Signal Processing Letters 20(5) 539–542.
[39] Yuan, M. and Lin, Y. (2006). Model selection and estimation in regression with grouped variables.
Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68(1) 49–67.
[40] Zeinalkhani, Z. and Banihashemi, A.H. (2015). Iterative reweighted `2/`1 recovery algorithms for
compressed sensing of block sparse signals. IEEE Transactions on Signal Processing 63(17) 4516–4531.
18
