ar
X
iv
:1
70
9.
01
11
2v
1 
 [
cs
.I
T
] 
 4
 S
ep
 2
01
7
1
Optimal deep neural networks for sparse recovery
via Laplace techniques
Steffen Limmer and S?awomir Stan?czak
Abstract—This paper introduces Laplace techniques to design
an optimal neural network for estimation of simplex-valued
sparse vectors from compressed measurements. To this end, we
recast the problem of MMSE estimation (w.r.t. a prescribed
uniform input distribution) as centroid computation of some
polytope that is an intersection of the simplex and an affine
subspace determined by the measurements. Due to a specific
structure, the centroid can be computed analytically by extending
a recent result by Lasserre that facilitates the volume computa-
tion of polytopes via Laplace transformations. Interestingly, the
computation of volume and centroid can be performed by a
classical deep neural network comprising threshold functions,
rectified linear (ReLU) and rectified polynomial (ReP) activation
functions. The proposed construction of a deep neural network
for sparse recovery is completely analytical, which allows for
bypassing time-consuming training procedures. Furthermore, we
show that the number of layers in our construction is equal to
the number of measurements which might enable novel low-
latency sparse recovery algorithms for a larger class of signals
than that assumed in this paper. To assess the applicability
of the proposed uniform input distribution, we showcase the
recovery performance on samples that are soft-classification
vectors generated by two standard datasets. As both volume and
centroid computation are known to be computationally hard, the
network width grows exponentially in the worst-case. However,
the width may be reduced by inducing sparse connectivity in
the neural network via a well-suited basis of the affine subspace.
Finally, we point out that our analytical construction may serve
as a viable initialization to be further optimized and trained using
particular input datasets at hand.
Index Terms—sparse recovery, compressed sensing, deep neu-
ral networks, convex geometry
I. INTRODUCTION
Deep neural networks have achieved significant improve-
ments in a series of tasks ranging from image classification to
speech recognition [1].While training the underlying networks
is extremely time-consuming, real-time inference has been
shown to be feasible, which has rendered successful com-
mercial applications as self-driving cars and realtime machine
translation possible [1]. These findings took many researchers
by surprise as the underlying problems were believed to be
both computationally and theoretically hard. Yet, theoretical
foundations of successful applications remain thin and analyt-
ical insights have only recently appeared in the literature. In
this context, results on the translation and deformation stability
of convolutional neural networks [2], [3] deserve a particular
attention. However, the interplay between sparsity of input
signals and particular network architectures has been so far
only addressed in numerical studies [4], [5], [6] or based on
shallow architectures [7]. This paper is a contribution towards
S. Limmer and S. Stan?czak are with the Department of Network-
Information Theory, TU Berlin.
closing this important gap in theory. To this end, we consider
a simple sparse recovery problem, which is shown to admit an
optimal solution by an analytically-designed neural network.
More precisely, we address the problem of estimating a
compressible vector from a set of dimension reduced noiseless
measurements via an M -layer deep neural network. The real-
valued input vector x ? RN is assumed to be distributed
uniformly over the standard simplex and is to be estimated
from M < N linear measurements y ? RM given by
y = Ax. (1)
We refer the reader to Fig. 9 in Sec. VI for an illustra-
tion of a realization x of this stochastic process as well
as markedly similar soft-classification vectors generated by
standard datasets. Here and hereafter, the measurement matrix
A ? RM×N is assumed to be an arbitrary fixed full-rank
matrix. The problem of designing efficient algorithms for
recovering x given y has been at the core of many research
works with well-understood algorithmic methods, including
?1-minimization [8] and iterative soft/hard-thresholding algo-
rithms (ISTA / IHT) [8], [9]. Training structurally similar
neural networks has been investigated in [6], [5], [4]; in
particular, these works focused on the problem of fine-tuning
parameters using stochastic gradient descent as a full search
over all possible architectures (number of layers, activation
function, size of the weight matrices) is infeasible. In contrast
to these approaches, this paper introduces multidimensional
Laplace transform techniques to obtain both the network
architecture as well as the parameters in a fully analytical
manner without the need for data-driven optimization. Inter-
estingly, this approach reveals an analytical explanation for
the effectiveness of threshold functions, rectified linear (ReLU)
and rectified polynomial (ReP) activation functions. Moreover,
we think that the resulting Laplace neural network can solve
a larger class of sparse recovery problems than that assumed
in this paper which is supported by a numerical study. To this
end, the proposed Laplace neural network can be used as a
viable initial model to be further optimized using conventional
SGD-type techniques.
A. Notation and Some General Assumptions
Throughout the paper, the sets of reals, nonnegative reals
and reals excluding the origin are designated by R, R+ and
R 6=0, respectively. S
N?1 ? RN and ? ? RN denote the N -
dimensional unit sphere and the standard simplex defined as
? := {x ? RN+ :
?N
n=1 xn ? 1}. We use lowercase, bold
lowercase and bold uppercase serif letters x, x, X to denote
scalars, vectors and matrices, respectively. Sans-serif letters are
2
(a) Intersection polytope Pt
(b) Centroid x?
Fig. 1. Intersection polytope and centroid for t = 0.5 and A =
V Ts = [0, 0, 1].
used to refer to scalar, vector and matrix random variables x ,
x , X . Throughout the paper, all random vectors are functions
defined over a suitable probability measure space (?,A, p)
where ? is a sample space, A is a ?-algebra, and p : A ?
[0, 1] a probability measure on A.1. We use p(X ) = p(x ? X )
where X ? A and U(X ) to denote the uniform distribution
over the set X ? A; finally, functions of random vectors are
assumed to be measurable functions, and we further assume
that random vectors x are absolute continuous with probability
density function (pdf) px (x) and expectation E [x ] < ?. We
use 0, 1, en and I to denote the vectors of all zeros, all ones,
n-th Euclidean basis vector, and the identity matrix, where
the size will be clear from the context. The i-th column, resp.
j-th row, resp. submatrix of i1-th to i2-th column and j1-
th to j2-th row, of a matrix is designated by A:,i, Aj,: and
Ai1:i2,j1:j2 . tr{·}, ?, ? and 1X : x ? {0, 1} denote the
trace of a matrix, entrywise product, entrywise division and
the indicator function defined as 1X (x) = 1 if x ? X and
0 otherwise. 1+(x) is the Heaviside function and (x)+ :=
max(0, x) is the rectified linear function.
II. OPTIMAL RECONSTRUCTION BY CENTROID
COMPUTATION OF INTERSECTION POLYTOPES
We assume that the sought vector x ? RN+ is a realization of
a non-negative random vector x drawn from a joint distribution
with a pdf denoted by px(x). Throughout the paper, we have
the following assumption:
1In particular, if the random vectors are in RN+ , then ? = R
N
+ and ? ? A
with A being the power set of RN+ .
Assumption 1. The standard simplex ? is the support of
px(x) so that we have px (?) = px(x ? ?) = 1.
This assumption imposes some compressibility on x, which
is often referred to as soft sparsity. In practice, input signals
x ? ? appear freqeuently in applications that involve discrete
probability vectors including softmax classification in machine
learning [1] as illustrated in Fig. 7 and 8, multiple hypothesis
testing in statistics [1] or maximum likelihood decoding in
communications [10]. The problem of compressing and recov-
ering such vectors applies in particular to distributed decision
making problems, where the decisions are distributed among
different decision makers and need to be fused to improve
an overall decision metric. We note that the set of discrete
probability vectors of length N + 1 generates the dihedral
face of a simplex in dimension N + 1 that can be embedded
naturally into the (solid) simplex of dimension N by removing
one arbitrary component. We refer the reader to Sec. VI for
a more detailed description as well as numerical results of
the proposed recovery method on soft-classification vectors
generated by two standard datasets.
As a result of Assumption 1, given a vector y = Ax, the
set of feasible solutions is restricted to a polytope
Py := ? ? {x : Ax = y} = ? ? (x0 + ker(A)) ,
2 (2)
where x0 is an arbitrary solution to y = Ax (e.g. the Moore-
Penrose pseudo inverse x0 = A
†y) and ker(A) denotes the
N ? M -dimensional kernel of A. In other words, upon ob-
serving y ? RM , the support of the conditional pdf px|y (x|y)
is equal to Py. To simplify the subsequent derivations, let V0
and Vs constitute a basis of ker(A) and ker
?(A) as obtained,
for instance, by the singular-value decomposition (SVD)
A = U?V T =
[
Us
] [
?s 0
]
[
V Ts
V T0
]
. (3)
Here, Us ? R
M×M , ?s ? R
M×M , Vs ? R
N×M and V0 ?
RN×N?M . Then, we apply (3) to (2) and multiply Ax = y
from left by ??1s U
T
s (note that U
T
s Us = IM ) to obtain an
equivalent description for the same polytope given by
Pt := ? ? {x : V
T
s x = t}. (4)
This description uses the equivalent measurement vector
t := ??1s U
T
s y = V
T
s x (5)
and defines the intersection polytope in terms of ker?(A) via
the orthogonal basis Vs. We point out that as A is assumed
to be real, so are also U and V . Fig. 1(a) depicts an example
of the polytope Pt for M = 1 and N = 3.
Lemma 1 (Optimality of centroid estimator under uniform
distribution). Assume x ? U(?) and suppose that a com-
pressed realization t = V Ts x (5) has been observed. Let ?P
be the Lebesgue measure on the (N ?M)-dimensional affine
subspace that contains Pt (see [11, Sec. 2.4]) and assume
2For two sets X and Y we denote their Minkowski Sum by X + Y :=
{x+ y : x ? X ,y ? Y}. Here, one set is the singleton x0.
3
Fig. 2. Neural network to solve problem (P1) and (P2).
that vol(Pt) :=
?
Pt
1 d?P > 0. Then, the conditional mean
estimator x? of x given t has the MMSE property
E
[
?x ? x??22
]
= inf
f
E
[
?x ? f(t)?22
]
(6)
and is obtained by
x?n = vol(Pt)
?1
?
Pt
xn d?P , n ? {1, . . . , N}, (7)
i.e., the centroid of the intersection polytope Pt, cf. (4).
Proof. The proof is a standard result in estimation theory [12].
The remaining part of the paper is devoted to the following
problem given a fixed measurement matrix A:
Problem 1. Design a neural network composed of only
elementary arithmetic operations and activation functions such
that if the input to the network is t, then its output is
(P1) the volume vol(Pt), and
(P2) the moments µ =
?
Pt
x d?P .
The sought neural network is depicted in Fig. 2
III. A REVIEW OF LASSERRE’S LAPLACE TECHNIQUES
To make this paper as self-contained as possible and high-
light the theoretical contribution of the original works [13],
[14], we start this section with a review of Laplace techniques
that will be used in Sec. IV and V for computing the volume
and moments of full- and lower-dimensional polytopes. To this
end, we introduce a set of definitions restated from [15], [16].
Definition 1 (Def. 1.4.3 [16]). The (one-sided) Laplace trans-
form (LT) of a function f : R+ ? R is the function F : C ? R
defined by
F (?) = Lt(f(t)) :=
? ?
0
f(t) exp(??t) dt (8)
provided that the integral exists.
Here and hereafter, we refer to t as transform variable and ?
as Laplace variable and conveniently designate LT transform
pairs by f(t) ? sF (?).
Remark 1. We assume input functions can be written in terms
of the Heaviside function as 1+(t)f(t) where 1+ will usually
be omitted. Thereby, expression of f(t) apply for t ? 0 and
f(t) = 0 for t < 0.
Lemma 2 (Def. 1.4.4, Th. 1.4.8 [16]). Let f be locally
integrable and assume there exists a number ? ? R such that
? ?
0
|f(t)| exp(??t) dt < ?. (9)
Then, the Laplace integral (8) is absolutely and uniformly
convergent on {? ? C : Re(?) ? ?} and
?ac := inf
{
? ? R :
? ?
0
|f(t)| exp(??t) dt < ?
}
(10)
is called the abscissa of absolute convergence.
Lemma 3 (Th. 1.4.12). [16] Let f be as in Lemma 2 and
(possibly piecewise) continuous. Then in points t of continuity
the inverse Laplace transform (ILT) is given by
f(t) = L?1? (F (?)) :=
1
2?i
? c+i?
c?i?
F (?) exp(?t) d?, c > ?ac
(11)
and in points of discontinuity we obtain the arithmetic mean
f(t+) + f(t?)
2
= L?1? (F (?)). (12)
As we assume that f(t) = 0 for t < 0 given expressions
for f(t) ? sF (?) are to be understood as 1+(t)f(t).
Definition 2. The (one-sided) M -dimensional LT (MD-LT) of
a function f : RM+ ? R is the function F : C
M ? C defined
by the concatenation of LTs
F (?) =
(
?Mm=1Ltm
)
(f(t)) (13)
provided that all integrals exist. The order of integration in
(13) is arbitrary provided that the individual LTs converge
absolutely and uniformly [15].
Definition 3. The M -dimensional ILT (MD-ILT) of a function
F (?) : CM ? C is defined by the concatenation of ILTs
f(t) =
(
?Mm=1L
?1
?m
)
(F (?)) (14)
provided that all integrals exist. The order of integration is
again arbitrary provided the individual ILTs converge and
given expressions for f(t) ? sF (?) are to be understood as
(
?M
m=1 1+(tm))f(t).
Remark 2. To make this article accessible for a broader
audience we omit a more detailed exposition of operational
properties as well as existence conditions of the (multidimen-
sional) Laplace operators and refer the interested reader to
[15], [16]. We note that the transforms appearing in this
article are obtained by combining standard transform pairs
summarized in Tab. I.
Now we are in a position to introduce the general idea of
Lasserre for polyhedral volume computation [13], [14] that
consists in exploiting the identity
f(t) =
(
?mL
?1
?m
)
(?mLtm) (f(t))
=
(
L?1?M ? . . . ? L
?1
?1
? ?? ?
M?times
?LtM ? . . . ? Lt1
? ?? ?
M?times
)
(f(t)) (15)
4
Fig. 3. Volume computation network with rectified polynomial layers for
input t ? R.
for functions f(t) admitting (multidimensional) Laplace trans-
forms according to Lemma 2, 3 and Def. 2, 3.
Interestingly, this approach allows for evaluating compli-
cated functions as vol(Pt) without resorting to costly multi-
dimensional numerical integration. For the particular case of
(P1) and a single measurement (see illustration in Fig. 1) with
f(t) = vol(? ? {x : aTx = t}) (16)
the corresponding result of Lasserre [14] is given below.
Lemma 4 (Volume of a simplex slice [14]). Let S =
{s1, . . .sN+1} = {e1, . . . , eN ,0} denote the vertices of the
standard simplex and a ? SN?1 be such that ?a, sn? 6=
?a, sn?? for any pair of distinct vertices sn, sn? . Then, the
volume of the simplex slice at any point t ? R is given by
vol (Pt) =
1
(N ? 1)!
N+1?
n=1
(t? ?a, sn?)
N?1
+
?
n?6=n(?a, sn?? ? ?a, sn?)
. (17)
Proof. The lemma is a restatement of Thm. 2.2 in [14].
On closer inspection of (17), we find that vol (Pt) can be
efficiently implemented given some fixed a and an input t ? R
by a 1-layer (shallow) neural network via a set of rectified
polynomial activation functions with shift {?a1, . . . ,?aN , 0}
and weights wn =
?
n?6=n(?a, sn??? ?a, sn?)
?1 as illustrated
in Fig. 3. Even though the rectified polynomial activation
function (t?an)
N?1
+ is currently not implemented in common
Deep Learning libraries, it can be efficiently computed via a
concatenation of a conventional ReLU (t? an)+ followed by
a polynomial activation function (t?an)
N?2 (both supported
in e.g. the Caffe or Tensorflow framework [17], [18]).
Moreover, it was shown in [14] that (17) also holds when
some an < 0 with f(t) > 0 for some t < 0 obstructing
the direct application of Laplace transform techniques (see
Rem. 1). Informally, the necessary extension was achieved by
introducing the translation operator St?(·) defined by
St?(f(t)) := f(t? t
?) (18)
and a translation identity in conjunction with (15) given by
f(t) = (St?S?t?) (f(t)) =
(
St?L
?1
? LtS?t?
)
(f(t)). (19)
Here, the shift t? ? R has to be chosen such that the identity
S?t?(f(t)) =
(
L?1? LtS?t?
)
(f(t)) (20)
# f(t) F (?)
(LT1) c1f1(t) + c2f2(t) c1F1(?) + c2F2(?)
(LT2)
tn?1
(n? 1)!
1
?n
(LT3) exp(at)
1
?? a
(LT4) t exp(at)
1
(?? a)2
(LT5)
exp(at)? exp(bt)
a? b
(a 6= b)
1
(?? a)(? ? b)
(LT6) 1+(t? a)f(t? a) (a ? 0) exp(?a?)F (?)
(LT7) f ?(t) ?F (?) ? f(0+)
TABLE I
TABLE OF LAPLACE TRANSFORMS
is admissible and the LT acts on a function f vanishing for
t < 0.
Remark 3. To avoid the obfuscation connected with a mul-
tidimensional extension of the shifted LT identity (20) and
involved analysis of admissibility conditions we consider in
the following only a special case where A admits a non-
negative orthogonal basis Vs ? R
N×M
+ for ker
?(A). We show
by numerical simulations that similar to Lemma 4 the neural
networks to be introduced in the following indeed apply for
every orthogonal basis Vs.
IV. VOLUME COMPUTATION NETWORK
A. Theoretical Foundation
The goal of this section is to solve (P1), i.e., to obtain a
neural network computing vol(Pt) given some t ? R
M . To
this end, we will first consider the special case Vs ? R
N×M
+ ,
V Ts Vs = I (see Remark 3). Using this assumption, we may
easily verify that vol(Pt) = 0 whenever some tm < 0 and
Laplace techniques such as the identity (15) are admissible.
To compute vol(Pt), we start with a Lemma on exponential
integrals over the simplex that will be used later on.
Lemma 5. Let l be a linear form such that ?l, sn? 6= ?l, sn??
for any pair of distinct vertices sn, sn? of the simplex ?. Then
we have
?
?
exp(??l,x?) dx =
N+1?
n=1
exp(??l, sn?)
?
n? 6=n?l, sn? ? sn?
. (21)
Proof. The lemma follows from [19, Cor. 12] by changing the
sign of the linear form and noting that the (full-dimensional)
simplex has volume (N !)?1.
We use this result to compute the inner MD-LT in (15).
To this end, let Tt := ? ? {x : V
T
s x ? t} denote the
intersection of the simplex and M halfspaces and consider
F (?) = F (?1, . . . , ?M ) =
(
LtM ? . . . ? Lt1
? ?? ?
M?times
)
(vol(Tt)) . (22)
5
By the definition of the MD-LT (13) and the fact that
R ? R+ : xk ? exp(xk) is non-negative the Fubini-Tonelli
theorem [20, Th. 9.11] implies that for Re(?) > 0 we have
F (?) =
?
R
M
+
exp(???, t?)
(
?
??{x:V Ts x?t}
1 dx
)
dt
=
?
?
(
?
[vT1 x,?)×...×[v
T
M
x,?)
exp(???, t?) dt
)
dx
=
1
?M
m=1 ?m
?
?
exp(??Vs?,x?) dx. (23)
Using the Laplace identity (15) we see that
vol(Tt) =
(
?mL
?1
?m
)
(
1
?
m ?m
?
?
exp(??Vs?,x?) dx
)
,
(24)
whenever the MD-ILT (14) on the RHS exists. In this case,
the RHS is a function of the transform variable t and the inner
simplex integral may be evaluated via Lemma 5 whenever the
corresponding condition holds. To obtain vol(Pt), we use (23)
to establish the following proposition.
Proposition 1. Let Vs be a non-negative orthogonal basis
(Vs ? R
N×M
+ , V
T
s Vs = IM ) and Pt := ? ? {x : Vsx = t}.
Then, we have
vol(Pt) =
(
?Mm=1L
?1
?m
)
(?
?
exp(??Vs?,x?) dx
)
(25)
provided that the integrals on the RHS exist.
Proof. The proof is deferred to Appendix A.
Let us now turn to the numerical evaluation of vol(Pt) via
the MD-ILT (14). To this end, we first evaluate the inner
integral over the simplex using Lemma 5 to obtain
vol(Pt) =
(
?Mm=1L
?1
?m
)
(
N?
n=1
exp(??Vs?, sn?)
?
n? 6=n?Vs?, sn? ? sn?
)
.
(26)
Given that the argument of the MD-ILT in (26) is a sum
of exponential-over-polynomial (exp-over-poly) functions we
continue with a Lemma on corresponding one-dimensional
transform pairs.
Lemma 6 (ILT of an exp-over-poly function).
1) Let M = 1, a ? 0, b ? RN6=0. Then we have the transform
pair (F (?) s ?f(t))
exp(?a?)
?N
n=1(bn?)
a?0
s ?
(t? a)N?1+
(N ? 1)!
?N
n=1 bn
. (27)
2) Let M ? 2, a ? RM and B:,1 ? R
N
6=0, B:,2:M ? R
N×M?1
with pairwise linearly independent rows. Then we have the
transform pair F (?1, . . . , ?M ) s ?f(t1, ?2, . . . , ?M )
exp(??a,??)
?N
n=1[B?]n
a1?0
s ?
1+(t1 ? a1)
?
n Bn,1
N?
n=1
exp(??a(n),?2:M ?)
?N?1
n?=1[B
(n)?2:M ]n?
.
(28)
Fig. 4. Volume network with rectified polynomial and threshold layers for
input t ? RM .
Setting C := B ? (B:,11
T ) we obtain a(n) ? RM?1 and
B(n) ? RN?1×M?1 (n ? {1, . . . , N}) by
a(n) = a2:M + (t1 ? a1)Cn,2:M , (29)
B(n) = C1:N\n,2:M ? 1Cn,2:M . (30)
Remark 4. We highlight that in case Bk,1 = 0 for some k
we can treat the corresponding factor Bk,:? = Bk,2:M?2:M
as a constant w.r.t. the ILT L?1?1 (·). Accordingly, we can apply
(28) using the truncated matrix B? = B1:N\k,:, where B?:,1 ?
R
N?1
6=0 , and obtain the truncated output B?
(n) ? RN?1×M?1.
For the subsequent ILT we have to include the corresponding
factor Bk,2:M? again by applying the matrix concatenation
B(n) =
[
B?(n)
Bk,2:M
]
. (31)
B. Structure of the network
Now we are in a position to present a computation network
for the MD-ILT (25) by an algorithm that can be described
by a computational tree (see [13]). Due to the particular form
of RHS in (21) the root of this tree is the MD-ILT of N + 1
exp-over-poly-functions
(a(i),B(i)) ?
exp(??a(i),??)
?
n? [B
(i)?]n?
. (32)
1) In layer m = 1, each node computes the ILT
F (?1, . . . , ?M ) s ?f(t1, ?2, . . . , ?M ) of an exp-over-
poly-function with parameters (a(i),B(i)) inherited from
the root node. If the branch is active, i.e. 1+(t1 ?
a
(i)
1 )(
?
n B
(i)
n,1)
?1 6= 0, it applies transform (28) and
passes the corresponding N exp-over-poly functions
(a(i,j),B(i,j)), (i, j) ? {1, . . . , N + 1} × {1, . . . , N} to
its children nodes.
2) In layer m = 2, each node computes the ILT
f(t1, ?2, . . . , ?M ) s ?f(t1, t2, ?3, . . . , ?M ) of an exp-
over-poly-function inherited from its parent node. If the
branch is active, i.e. 1+(t2 ? a
(i,j)
1 )(
?
n B
(i,j)
n,1 )
?1 6= 0,
6
Fig. 5. Intersection polytope (line) and centroid for t = [0.5; 0.933] and
A = V Ts = [0, 0, 1; 0.5, 0.866, 0].
it applies transform (28) and passes the correspond-
ing N ? 2 exp-over-poly functions (a(i,j,k),B(i,j,k)),
(i, j, k) ? {1, . . . , N+1}×{1, . . . , N}×{1, . . . , N?1}
to its children nodes.
3) ...
4) In the last layer m = M , each node computes
the ILT f(t1, . . . , tM?1, ?M ) s ?f(t1, . . . , tM ) of an
exp-over-poly-function inherited from its parent node.
If the branch is active, i.e. 1+(tM ? a
(i,j,...)
1 )((N ?
1)!(
?
n B
(i,j,...)
n,1 ))
?1 6= 0, it applies transform (27) and
obtains a numerical value.
5) The final result is obtained by summing and weighting
all values of the last layer.
The described computational tree is equivalent to a deep neural
network with M layers, weights
?
n B
(i,j,...)
n,1 and threshold as
well as rectified polynomial activation functions (see Fig. 4).
C. Volume computation example
In the following we will present a numerical example for the
computation of vol(Pt) via the described computation network
and repeated application of Lemma 6.
Example 1 (Volume computation). Assume M = 2, N = 3
and let Pt be defined by the measurements (see corresponding
illustration in Fig. 5)
t = V Ts x
[
0.5
0.0933
]
=
[
0 0 1
0.5 0.866 0
]
?
?
0.1
0.05
0.5
?
? . (33)
The exp-over-poly functions at the root node are obtained by
(21) and defined by the parameters
a(1) =
[
0
0.5
]
, a(2) =
[
0
0.866
]
, a(3) =
[
1
0
]
a(4) =
[
0
0
]
B(1) =
?
?
0 0.366
1 ?0.5
0 ?0.5
?
? , B(2) =
?
?
0 ?0.366
1 ?0.866
0 ?0.866
?
? ,
B(3) =
?
?
?1 0.5
?1 0.866
?1 0
?
? , B(4) =
?
?
0 0.5
0 0.866
1 0
?
?
The next step is to compute the ILT with respect to ?1, i.e.,
L?1?1
(
4?
i=1
exp(??a(i),??)
?N
n=1[B
(i)?]n
)
. (34)
Applying (28) and using truncation and concatenation when
required (see Rem. 4) yields
a(1,1) = 0.25, a(2,1) = 0.433, a(3,1) = 0.25,
a(3,2) = 0.433, a(3,3) = 0, a(4,1) = 0,
B(1,1) =
[
0.366
?0.5
]
, B(2,1) =
[
?0.366
?0.866
]
, B(3,1) =
[
?0.366
0.5
]
,
B(3,2) =
[
0.366
0.866
]
, B(3,3) =
[
?0.5
?0.866
]
, B(4,1) =
[
0.5
0.866
]
.
Accordingly, the outputs of the last layer in the computational
tree are given by
f (i,j) =
(t2 ? a
(i,j)
1 )+
?
n B
(i,j)
n,1
·
1+(t1 ? a
(i)
1 )
?
n? B
(i)
n?,1
f (1,1) = 0, f (2,1) = 0, f (3,1) = 0
f (3,2) = 0, f (3,3) = 0, f (4,1) = 0.2155,
where index n? ? supp(B
(i)
:,1 ) and the final result is given by
vol(Pt) = 0.2155.
V. MOMENT COMPUTATION NETWORK
The final step towards the optimal estimator of Lem. 1 is to
compute the moment vector µ =
?
Pt
x d?P . To this end, we
introduce an extension of Lemma 5 as well as a conjecture
that was verified numerically but currently lacks a rigorous
proof.
Lemma 7. Let l and ? be as in Lemma 5. Then we have
?
?
xk exp(??l,x?) dx
=
exp(??l, sk?)
?
n6=k?l, sn ? sk?
+
N+1?
n=1
n6=k
exp(??l, sn?)
?l, sk ? sn?
?
n? 6=n?l, sn? ? sn?
?
N+1?
n=1
n6=k
exp(??l, sk?)
?l, sn ? sk?
?
n? 6=k?l, sn? ? sk?
(35)
Proof. The proof is deferred to Appendix C.
Conjecture 1. Let Pt and Vs be as in Prop. 1. Then, the
moment µk :=
?
Pt
xk d?P = vol(Pt)x?k (see (7)) is given by
µk =
(
?mL
?1
?m
)
(?
?
xk exp(??Vs?,x?) dx
)
, (36)
provided that the integrals on the RHS exist.
Remark 5. Conj. 1 originates from Prop. 1 and its numerical
correctness was verified by extensive comparison with inte-
gration by simplicial decomposition [21]. However, a current
gap in a rigorous proof consists in relating the moment of a
polyhedron to the moment of an (N ? M)-dimensional face
7
Fig. 6. Moment network for µk with rectified linear, rectified polynomial
and threshold layers for t ? RM .
in the spirit of (44) [22, Prop. 3.3]. We will assume the
conjecture to be valid in what follows in the hope that our
network construction will prove useful to other researchers.
To evaluate the MD-ILT in (36) we inspect the terms in
the sum (35) and note that the first term contains only simple
poles permitting its Laplace transformation via Lemma 6.
In fact, the corresponding term already appears in (21) and
accordingly the MD-ILT is already computed as part of the
volume computation network. However, this does not apply
to the remaining 2N terms as they contain quadratic terms
?l, sk ? sn? and ?l, sn ? sk?. Accordingly, for M ? 2 two
rows of the denominator matrix B(i), 2 ? i ? 2N +1 will be
linearly dependent violating the assumptions of Lemma 6. A
modified result applicable for ILTs of exp-over-poly-functions
with one double pole (resp. two linearly dependent rows of
B) is provided in the following Lemma.
Lemma 8 (ILT of exp-over-poly function with one dou-
ble pole). Let M ? 2, N ? 3, B:,1 ? R
N
6=0 and
assume w.l.o.g. that the first and second row of B are
equal, i.e., B1,: = B2,:. Then, we have the transform pair
F (?1, . . . , ?M ) s ?f(t1, ?2, . . . , ?M )
exp(??a,??)
?N
n=1[B?]n
a1?0
s ?
(t1 ? a1)+
?N
n=1 Bn,1
exp(??a(1),?2:M ?)
?N?1
n?=1[B
(1)?2:M ]n?
+
1+(t1 ? a1)
?N
n=1 Bn,1
N?1?
n=2
(
exp(??a(n),?2:M ?)
?N?1
n?=1[B
(n)?2:M ]n?
?
exp(??a(1),?2:M ?)
?N?1
n?=1[B
(n)?2:M ]n?
)
. (37)
Setting C := B ? (B:,11
T ) we obtain a(n) ? RM?1 and
B(n) ? RN?1×M?1 (n ? {1, . . . , N ? 1}) by
a(n) =
{
a2:M + (t1 ? a1)C1,2:M , n = 1
a2:M + (t1 ? a1)Cn+1,2:M , n ? 2
(38)
B(n) =
?
??
??
C3:N,2:M ? 1C1,2:M , n = 1[
Cn+1,2:M ?C1,2:M
C3:N,2:M ? 1Cn+1,2:M
]
, n ? 2.
(39)
Proof. The proof is deferred to Appendix D.
Using Lemma 8 we can readily build a computational tree
(resp. neural network) to compute the moment µk (1 ? k ?
N ) comprising O(NM ) nodes as depicted in Fig. 6. It can be
shown that the individual networks for volume and moments
perform a set of identical computations, i.e., the networks
share particular subnetworks, which results in a subadditive
number of nodes for the combined network computing vol(Pt)
and µ. However, the resulting number of nodes in the network
still grows as O(NM ) in the worst case.
Remark 6. While the worst case growth of O(NM ) applies to
a fully connected network, a numerical investigation revealed
that large subnetworks were never activated by the corre-
sponding activation functions independently of the particular
network input t ? {V Ts x : x ? ?}. In addition, the size
of deactivated subnetworks changes with the chosen set of
orthogonal basis vectors Vs of the subspace ker
?(A) and
applying orthogonal transformations resulted in much smaller
or much larger active subnetworks. The choice of a favorable
(or even optimal) basis of the affine subspace is beyond the
scope of this paper but poses an interesting question to be
addressed in future works.
VI. NUMERICAL EXAMPLE: COMPRESSING
SOFT-CLASSIFICATION VECTORS
To assess the performance of the proposed network we
consider the problem of soft-decision compression for dis-
tributed decision making. In the presumed setting, nodes
reduce data traffic by transmitting only compressed versions of
their local soft-classification vectors and recovery at a fusion
center allows for enhanced algorithms to improve an overall
decision metric. One possible application in this regard is
multi-view image classification. To this end, we investigate the
compressibility of softmax-outputs of deep learning classifiers
on MNIST handwritten digits and CIFAR-10 images obtained
using MatConvNet (www.vlfeat.org/matconvnet/) and assess
the fitness of the uniform simplex distribution for practical
datasets. The outputs of the trained classifiers are vectors
x? ? R10+ , where each entry measures the estimated class-
membership probability corresponding either to occurrence of
digits {0, . . . , 9} for MNIST, or occurrence of classes {plane,
car, bird, cat, deer, dog, frog, horse, ship, truck} for CIFAR-10.
These vectors are preprocessed by removing one uninformative
component (e.g. x?10 = 1?
?9
n=1 x?n) which ensures that
x := x?1:9 ? ?. (40)
Examples of a realization x of x ? U(?) as well as input
images and outputs of standard classifiers are given in Fig. 7,
8 and 9. We adjust the confidence levels to match the measured
accuracy via the temperature-parameter of the softmax-output
such that the top-entry is on average 0.9748 for MNIST and
0.7987 for CIFAR-10 (for the default parameter almost all
decisions are made with unduly high confidence levels). We
measure the empirical mean-square error
eMSE :=
1
Ni
Ni?
i=1
?x(i) ? x?(i)?22 (41)
8
0 1 2 3 4 5 6 7 8 9
0
0.5
1
0 1 2 3 4 5 6 7 8 9
0
0.5
1
0 1 2 3 4 5 6 7 8 9
0
0.5
1
0 1 2 3 4 5 6 7 8 9
0
0.5
1
Fig. 7. MNIST images and softmax-classifications for low-confidence exam-
ples. True labels are {2, 5, 6, 9} and estimated labels are {2, 5, 5, 3}.
over a testing set of cardinality Ni = 500 and compare the
proposed estimator x? (7) with exact centroid computation by
simplicial decomposition using Qhull [21] as well as solutions
to the well-established non-negative ?1-minimization
x??1 = argmin
x?RN+ : y=Ax
?x?1 (42)
and (simplex constrained) ?2-minimization
x??2 = argmin
x??: y=Ax
?x?22. (43)
For the compression matrix A we use an i.i.d. random
Gaussian matrix drawn once and set fixed for all simula-
tions. The compressed soft-classification vector is given by
y = Ax ? RM where we vary the number of compressed
measurements M . As a reference, we also showcase the results
for input signals following the presumed uniform simplex
distribution. in Fig. 10. It is interesting to see that for the
prescribed uniform distribution the proposed centroid estima-
tor outperforms the conventional ?1- and ?2-based methods by
a factor of about 3 and 2 (see Fig. 10). For the MNIST and
CIFAR-10 dataset the proposed method is on par with the well-
established ?1-minimization method (see Fig. 11 and 12). All
simulations were run on a laptop with i7-2.9 GHz processor.3
Of course, additional performance gains of the proposed
network can be expected when fine-tuning the analytically
obtained parameters based on given datasets is employed.
3In the spirit of reproducible research, the simulation code for the com-
putation of volumes and centroids using Laplace techniques and simplicial
decomposition is made available at https://github.com/stli/CentNet.
p
la
n
e
ca
r
b
ir
d
ca
t
d
ee
r
d
og
fr
og
h
or
se
sh
ip
tr
u
ck
0
0.5
p
la
n
e
ca
r
b
ir
d
ca
t
d
ee
r
d
og
fr
og
h
or
se
sh
ip
tr
u
ck
0
0.5
p
la
n
e
ca
r
b
ir
d
ca
t
d
ee
r
d
og
fr
og
h
or
se
sh
ip
tr
u
ck
0
0.5
p
la
n
e
ca
r
b
ir
d
ca
t
d
ee
r
d
og
fr
og
h
or
se
sh
ip
tr
u
ck
0
0.5
Fig. 8. CIFAR-10 images and softmax-classifications for low-confidence
examples. True labels are {bird, dog, bird, frog} and estimated labels are
{cat, cat, bird, frog}.
1 2 3 4 5 6 7 8 9
n
0
0.1
0.2
0.3
0.4
0.5
x
n
Fig. 9. Realization of x ? U(?) for N = 9.
As a side note, the volumes of the intersection polytopes
can be surprisingly small and in some cases were below
numerical precision causing precision problems and numerical
underflows. On the other hand, numerical instability is a well-
known problem in the design of deep neural networks (see e.g.
[1]) and appropriate numerical stabilization techniques, e.g.,
via logarithmic calculus, are often required. For the datasets
at hand, numerical underflows occurred for a small number
of MNIST examples, where the input x was close to a vertex
si of the simplex ? and the intersection volume becomes
extremely small. We note, that these examples were removed
for the evaluation of the neural network but kept for all
other estimators. As this case is rather easy to solve using
conventional ?1-minimization and resulting estimation errors
9
1 2 3 4 5 6 7
M
0
0.05
0.1
0.15
em
p
ir
ic
a
l
M
S
E
analytical dnn
simplex decomposition
l1 minimization
l2 minimization
Fig. 10. Empirical MSE for N = 9, x ? U(?), i.i.d. Gaussian matrix A
and varying number of measurements M .
1 2 3 4 5 6 7
M
0
0.2
0.4
0.6
0.8
em
p
ir
ic
a
l
M
S
E
analytical dnn
simplex decomposition
l1 minimization
l2 minimization
Fig. 11. Empirical MSE for MNIST dataset, i.i.d. Gaussian matrix A and
varying number of measurements M .
are typical smaller than average the depicted results do not
favour the proposed approach. A detailed numerical analysis
and numerically redesigned network is beyond the scope of
this paper.
VII. CONCLUSION
In this paper we proposed a novel theoretically well-founded
neural network for sparse recovery. By using multidimensional
1 2 3 4 5 6 7
M
0
0.2
0.4
0.6
em
p
ir
ic
a
l
M
S
E
analytical dnn
simplex decomposition
l1 minimization
l2 minimization
Fig. 12. Empirical MSE for CIFAR-10 dataset, i.i.d. Gaussian matrix A and
varying number of measurements M .
Laplace techniques and a prescribed input distribution, we
obtain a neural network in a fully analytical fashion. Inter-
estingly, the obtained neural network is composed of weights
as well as commonly employed threshold functions, rectified
linear (ReLU) and rectified polynomial (ReP) activation func-
tions. The obtained network is a first step to understanding
the practical effectiveness of classical deep neural network
architectures. To scale to higher dimensions, a main problem
is to decrease the network width which may be achieved by
deactivating maximally large subnetworks via a well-chosen
basis of the affine subspace which poses an interesting problem
for future works. In addition, it may be beneficial to investi-
gate approximations of the constructed network by a smaller
subnetwork which may yield a reasonable approximation of
the centroid of interest.
APPENDIX A
PROOF OF PROPOSITION 1
First note that Pt is an (N ? M)-dimensional face of Tt.
Hence, repeated application of [22, Prop. 3.3] shows that
vol(Pt) = ?v1?2 · · · ?vM?2
?M
?t1 · · · ?tM
vol(Tt). (44)
Next, applying f(t) = vol(Pt) to (15) yields
vol(Pt) =
(
?mL
?1
?m
)
(?mL?m) vol(Pt)
=
(
?mL
?1
?m
)
(?mL?m)
?M
?t1 · · · ?tM
vol(Tt). (45)
By continuity of vol(Tt) we have ? m ? {1, . . . ,M} that
limtm?0 vol(Tt) = 0 (46)
and repeated application of the transform pair (LT7) yields
the desired result.
APPENDIX B
PROOF OF LEMMA 6
For Lemma 6.1 with M = 1 the stated transform pair
is obtained by using the transform pairs (LT6), (LT2) and
linearity (LT1).
For Lemma 6.2 with M ? 2 we first prove the transform
pair (?
(0)
n 6= ?
(0)
n? for n 6= n
?)
c exp(?a?)
?N
n=1(?? ?
(0)
n )
a?0
s ?1+(t? a)
N?
n=1
c exp(?
(0)
n (t? a))
?
n? 6=n(?
(0)
n ? ?
(0)
n? )
.
(47)
To this end, we use the partial fraction expansion (see [23,
(10) p.77])
1
?N
n=1(? ? ?
(0)
n )
=
N?
n=1
1
?? ?
(0)
n
1
d
d?
?N
n?=1(?? ?
(0)
n? )|?=?(0)n
=
N?
n=1
1
?? ?
(0)
n
1
?
n? 6=n(?
(0)
n ? ?
(0)
n? )
(48)
10
in conjunction with the transform pair (LT3) to obtain the
transform pair
1
?N
n=1(?? ?
(0)
n )
s ?
N?
n=1
exp(?
(0)
n t)
?
n? 6=n(?
(0)
n ? ?
(0)
n? )
. (49)
Then, (47) follows from (49) by linearity and the transform
pair (LT6). Finally, by assumption
?(0)n := ?
1
Bn,1
Bn,2:M?2:M (50)
are pairwise distinct (we can assume ?2:M is arbitrary but
fixed) and the result (28) follows from (47) by setting ? := ?1,
a := a1 and c := (
?N
n=1 Bn,1)
?1 exp(??a2:M ,?2:M ?).
APPENDIX C
PROOF OF LEMMA 7
To obtain the desired integral assume l1:N\k is arbitrary but
fixed and consider the function
f(x, lk) := exp(??l,x?). (51)
As f is jointly continuous in the variables x, lk and
?
?lk
f(x, lk) is continuous it holds that (see [24, Th. 8.11.2])
?
?
xk exp(??l,x?) dx =
?
?
?
?
?lk
f(x, lk) dx
= ?
?
?lk
?
?
f(x, lk) dx = ?
?
?lk
N+1?
n=1
exp(??l, sn?)
?
n? 6=n?l, sn? ? sn?
.
Carrying out the differentiation yields the desired result.
APPENDIX D
PROOF OF LEMMA 8
First we obtain the transform pair F (?) s ?f(t) (?
(0)
n 6=
?
(0)
n? for n 6= n
? ? {1, . . . , N ? 1})
exp(a?)
(?? ?
(0)
1 )
?N?1
n=1 (? ? ?
(0)
n )
a?0
s ?
(t? a)+ exp(?
(0)
1 (t? a))
?N?1
n=2 (?
(0)
1 ? ?
(0)
n )
+1+(t? a)
N?1?
n=2
exp(?
(0)
n (t? a))? exp(?
(0)
1 (t? a))
(?
(0)
n ? ?
(0)
1 )
?
n? 6=n(?
(0)
n ? ?
(0)
n? )
(52)
by using the partial fraction expansion (53)
1
(?? ?
(0)
1 )
?N?1
n=1 (?? ?
(0)
n )
= (53)
=
1
(?? ?
(0)
1 )
N?1?
n=1
1
?? ?
(0)
n
1
?
n? 6=n(?
(0)
n ? ?
(0)
n? )
(54)
in conjunction with the transform pairs (LT4), (LT5) and
(LT6). By assumption, B1,: = B2,: and ?2:M is arbitrary but
fixed so that
?(0)n :=
{
? 1
B1,1
B1,2:M?2:M , n = 1
? 1
Bn,1
Bn+1,2:M?2:M , n ? {2, . . . , N ? 1}
(55)
are pairwise distinct and (37) follows from (52) by setting
? := ?1, a := a1 and multiplying both sides by c :=
(
?N
n=1 Bn,1)
?1 exp(??a2:M ,?2:M ?).
REFERENCES
[1] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning, MIT press,
2016.
[2] S. Mallat, “Understanding deep convolutional networks,” Phil. Trans.
R. Soc. A, vol. 374, no. 2065, pp. 20150203, 2016.
[3] T. Wiatowski and H. Bo?lcskei, “A mathematical theory of deep
convolutional neural networks for feature extraction,” arXiv preprint
arXiv:1512.06293, 2015.
[4] B. Xin, Y. Wang, W. Gao, D. Wipf, and B. Wang, “Maximal sparsity
with deep networks?,” in Advances in Neural Information Processing
Systems, 2016, pp. 4340–4348.
[5] U. Kamilov and H. Mansour, “Learning optimal nonlinearities for
iterative thresholding algorithms,” IEEE Signal Processing Letters, vol.
23, no. 5, pp. 747–751, 2016.
[6] Z. Wang, Q. Ling, and T. Huang, “Learning deep l0 encoders,” in AAAI
Conference on Artificial Intelligence, 2016, pp. 2194–2200.
[7] S. Limmer and S. Stanczak, “Towards optimal nonlinearities for sparse
recovery using higher-order statistics,” in Machine Learning for Signal
Processing (MLSP), 2016 IEEE 26th International Workshop on. IEEE,
2016, pp. 1–6.
[8] M. Fornasier and H. Rauhut, “Compressive sensing,” in Handbook of
mathematical methods in imaging, pp. 187–228. Springer, 2011.
[9] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding
algorithm for linear inverse problems,” SIAM journal on imaging
sciences, vol. 2, no. 1, pp. 183–202, 2009.
[10] D. Tse and P. Viswanath, Fundamentals of wireless communication,
Cambridge university press, 2005.
[11] B. Makarov and A. Podkorytov, Real analysis: Measures, integrals and
applications, Springer Science & Business Media, 2013.
[12] S. Kay, “Fundamentals of statistical signal processing, volume i:
estimation theory,” 1993.
[13] J. B. Lasserre and E. S. Zeron, “A laplace transform algorithm for the
volume of a convex polytope,” Journal of the ACM, vol. 48, no. 6, pp.
1126–1140, 2001.
[14] J. B. Lasserre, “Volume of slices and sections of the simplex in closed
form,” Optimization Letters, vol. 9, no. 7, pp. 1263–1269, 2015.
[15] Y. Brychkov, V. K. Tuan, H. J. Glaeske, and A. Prudnikov, “Multidi-
mensional integral transformations,” 1992.
[16] H. J. Glaeske, A. Prudnikov, and K. Sko?rnik, “Operational calculus and
related topics,” 2006.
[17] Y. Jia et al., “Caffe: Convolutional architecture for fast feature embed-
ding,” arXiv preprint arXiv:1408.5093, 2014.
[18] M. Abadi et al., “TensorFlow: Large-scale machine learning on hetero-
geneous systems,” 2015.
[19] V. Baldoni, N. Berline, J. De Loera, M. Ko?ppe, and M. Vergne, “How
to integrate a polynomial over a simplex,” Mathematics of Computation,
vol. 80, no. 273, pp. 297–325, 2011.
[20] G. Teschl, “Topics in real and functional analysis,” 2014.
[21] B. Bu?eler, A. Enge, and K. Fukuda, “Exact volume computation
for polytopes: a practical study,” in Polytopes, combinatorics and
computation. Springer, 2000, pp. 131–154.
[22] J. B. Lasserre, “An analytical expression and an algorithm for the volume
of a convex polyhedron in Rn,” Journal of optimization theory and
applications, vol. 39, no. 3, pp. 363–377, 1983.
[23] G. Doetsch, Introduction to the Theory and Application of the Laplace
Transformation, Springer Science & Business Media, 2012.
[24] J. Dieudonne, Foundations of Modern Analysis, vol. 1, Academic Press,
1969.
