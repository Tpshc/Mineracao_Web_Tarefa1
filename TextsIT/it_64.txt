ar
X
iv
:1
70
9.
00
13
4v
1 
 [
cs
.I
T
] 
 1
 S
ep
 2
01
7
1
Universality of Logarithmic Loss in Lossy
Compression
Albert No, Member, IEEE, and Tsachy Weissman, Fellow, IEEE
Abstract
We establish two strong senses of universality of logarithmic loss as a distortion criterion in lossy compression: For
any fixed length lossy compression problem under an arbitrary distortion criterion, we show that there is an equivalent
lossy compression problem under logarithmic loss. In the successive refinement problem, if the first decoder operates
under logarithmic loss, we show that any discrete memoryless source is successively refinable under an arbitrary
distortion criterion for the second decoder.
Index Terms
Fixed-length lossy compression, logarithmic loss, rate-distortion, successive refinability.
I. INTRODUCTION
In the lossy compression problem, logarithmic loss distortion is a criterion allowing a “soft” reconstruction of
the source, a departure from the classical setting of a deterministic reconstruction. Although logarithmic loss plays
a crucial role in the theory of learning and prediction, relatively little work has been done in the context of lossy
compression, notwithstanding the two-encoder multiterminal source coding problem under logarithmic loss [1],
[2], or recent work on the single-shot approach to lossy courec coding under logarithmic loss [3]. Note that lossy
compression under logarithmic loss is closely related to the information bottleneck method [4]–[6]. In this paper,
we focus on universal properties of logarithmic loss in two lossy compression problems.
First, we consider the fixed-length lossy compression problem. We show that there is a one-to-one correspondence
between any fixed-length lossy compression problem under an arbitrary distortion measure and that under logarithmic
loss. The correspondence is in the following strong sense:
• Optimum schemes for the two problems are the same.
• A good scheme for one problem is also a good scheme for the other.
We will be more precise about “optimum” and “goodness” of the scheme in later sections. This finding essentially
implies that it is enough to consider the lossy compression problem under logarithmic loss. We point out that our
The material in this paper has been presented in part at the 2015 International Symposium on Information Theory. This work was supported
by the Hongik University new faculty research support fund.
A. No is with the Department of Electronic and Electrical Engineering, Hongik University, Seoul, Korea (email: albertno@hongik.ac.kr)
T. Weissman is with the Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA (email: tsachy@stanford.edu)
2
result is different from that of [7] which justifies logarithmic loss by showing it is the only loss function that
satisfies a natural data processing requirement.
We also consider the successive refinement problem under logarithmic loss. Successive refinement is a network
lossy compression problem where one encoder wishes to describe the source to two decoders [8], [9]. Instead
of having two separate coding schemes, the successive refinement encoder designs a code for the decoder with
a weaker link, and sends extra information to the second decoder on top of the message of the first decoder.
In general, successive refinement coding cannot do as well as two separate encoding schemes optimized for the
respective decoders. However, if we can achieve the point-to-point optimum rates using successive refinement
coding, we say the source is successively refinable. We show that any discrete memoryless source is successively
refinable as long as the weaker link employs logarithmic loss, regardless of the distortion criterion used for the
stronger link. This idea can be useful to construct practical point-to-point lossy compression since it allows a smaller
codebook and lower encoding complexity [10]–[12].
The remainder of the paper is organized as follows. In Section II, we revisit some of the known results pertaining
to logarithmic loss and fixed-length lossy compression. Section III is dedicated to the equivalence between lossy
compression under arbitrary distortion measures and that under logarithmic loss. Section IV is dedicated to successive
refinement under logarithmic loss in the weaker link.
Notation: Xn denotes an n-dimensional random vector (X1, X2, . . . , Xn) while x
n denotes a specific possible
realization of the random vector Xn. Similarly, Q denotes a random probability mass function while q denotes a
specific probability mass function. We use natural logarithm and nats instead of bits.
II. PRELIMINARIES
A. Logarithmic Loss
Let X be a set of discrete source symbols (|X | < ?), and M(X ) be the set of probability measures on X .
Logarithmic loss ? : X ×M(X )?[0,?] is defined by
?(x, q) = log
1
q(x)
for x ? X and q ? M(X ). Logarithmic loss between n-tuples is defined by
?n(x
n, qn) =
1
n
n
?
i=1
log
1
qi(xi)
,
i.e., the symbol-by-symbol extension of the single letter loss.
Let Xn be the discrete memoryless source with distribution PX . Consider the lossy compression problem under
logarithmic loss where the reconstruction alphabet is M(X ). The rate-distortion function is given by
R(D) = inf
PQ|X :E[?(X,Q)]?D
I(X ;Q)
=H(X)?D.
The following lemma provides a property of the rate-distortion function achieving conditional distribution.
3
Lemma 1. The rate-distortion function achieving conditional distribution PQ?|X satisfies
PX|Q?(·|q) =q (1)
H(X |Q?) =D (2)
for PQ? almost every q ? M(X ). Conversely, if PQ|X satisfies (1) and (2), then it is a rate-distortion function
achieving conditional distribution, i.e.,
I(X ;Q) =R(D) = H(X)?D
E [?(X,Q)] =D.
The key idea is that we can replace Q by PX|Q(·|Q), and have lower rate and distortion, i.e.,
I(X ;Q) ?I(X ;PX|Q(·|Q))
E [?(X,Q)] ?E
[
?(X,PX|Q(·|Q)
]
,
which directly implies (1).
Interestingly, since the rate-distortion function in this case is a straight line, a simple time sharing scheme achieves
the optimal rate-distortion tradeoff. If the encoder losslessly compresses only the first
H(X)?D
H(X) fraction of the source
sequence components, while the decoder perfectly recovers those components and uses PX as its reconstruction
for the remaining components then the resulting scheme obviously achieves distortion D with rate H(X)?D.
Moreover, this simple scheme directly implies successive refinability of the source. For D1 > D2, suppose
the encoder losslessly compresses the first
H(X)?D2
H(X) fraction of the source. Then the first decoder can perfectly
reconstruct
H(X)?D1
H(X) fraction of the source with the message of rate H(X)?D1 and distortion D1 while the second
decoder can achieve distortion D2 with rate H(X)?D2. Since both decoders can achieve the best rate-distortion
pair, it follows that any discrete memoryless source under logarithmic loss is successively refinable.
B. Fixed-Length Lossy Compression
In this section, we briefly introduce the basic settings of the fixed-length lossy compression problem (see [13]
and references therein for more details). In the fixed-length lossy compression setting, we have a source X with
finite alphabet X = {1, . . . , r} and source distribution PX . An encoder f : X ? {1, . . . ,M} maps the source
symbol to one of M messages. On the other side, a decoder g : {1, . . . ,M} ? X? maps the message to the actual
reconstruction X? where the reconstruction alphabet is also finite X? = {1, . . . , s}. Let d : X × X? ? [0,?) be a
distortion measure between the source and the reconstruction.
First, we can think of the code that the expected distortion is lower than a given distortion level.
Definition 1 (Average distortion criterion). An (M,D) code is a pair of an encoder f with |f | ? M and a decoder
g such that
E [d(X, g(f(X)))] ? D.
4
The minimum number of codewords required to achieve average distortion not exceeding D is defined by
M?(D) = min{M : ?(M,D) code}.
Similarly, we can define the minimum achievable average distortion given the number of codewords M .
D?(M) = min{D : ?(M,D) code}.
One may consider a stronger criterion that restricts the probability of exceeding a given distortion level.
Definition 2 (Excess distortion criterion). An (M,D, ?) code is a pair of an encoder f with |f | ? M and a decoder
g such that
Pr [d(X, g(f(X))) > D] ? ?.
The minimum number of codewords required to achieve excess distortion probability ? and distortion D is defined
by
M?(D, ?) = min{M : ?(M,D, ?) code}.
Similarly, we can define the minimum achievable excess distortion probability given the target distortion D and
the number of codewords M .
??(M, ?) = min{? : ?(M,D, ?) code}.
Given the target distortion D and PX , the informational rate-distortion function is defined by
R(D) = inf
P
X?|X :E[d(X,X?)]?D
I(X ; X?)
We make the following benign assumptions:
• There exists a unique rate-distortion function achieving conditional distribution P ?
X?|X
.
• We assume that P
X??
(x?) > 0 for all x? ? X? since we can always discard the reconstruction symbol with zero
probability.
• If d(x, x?1) = d(x, x?2) for all x ? X , then x?1 = x?2.
Define the information density of the joint distribution PX,X? by
?
X;X?(x; x?) = log
PX,X?(x, x?)
PX(x)PX? (x?)
.
Then, we are ready to define the D-tilted information which plays a key role in fixed-length lossy compression.
Definition 3. [13, Definition 6] The D-tilted information in x ? X is defined as
?X(x,D) = log
1
E
[
exp
(
??D ? ??d(x, X??)
)]
where the expectation is with respect to the marginal distribution of X?? and ?? = ?R?(D).
5
Theorem 2. [14, Lemma 1.4] For all x? ? X? ,
?X(x,D) = ?X;X??(x; x?) + ?
?d(x, x?)? ??D, (3)
and therefore we have
R(D) = E [?(X,D)].
Let P ?
X|X?
be the induced conditional probability from P ?
X?|X
. Then (3) can equivalently be expressed as
log
1
P ?
X|X?
(x|x?)
= log
1
PX(x)
? ?X(x,D) + ?
?d(x, x?)? ??D. (4)
The following lemma shows that P ?
X|X?
(·|x?) are all distinct.
Lemma 3. For all x?1 6= x?2, there exists x ? X such that P
?
X|X?
(x|x?1) 6= P
?
X|X?
(x|x?2).
Proof. Suppose by contradiction that P ?
X|X?
(x|x?1) = P
?
X|X?
(x|x?2) for all x ? X . Then, (4) implies
log
1
PX(x)
? ?X(x,D) + ?
?d(x, x?1)? ?
?D
= log
1
PX(x)
? ?X(x,D) + ?
?d(x, x?2)? ?
?D
for all x ? X , i.e., d(x, x?1) = d(x, x?2) for all x ? X , which violates our assumption on the distortion measure.
C. Successive Refinability
In this section, we review the successive refinement problem with two decoders. Let the source Xn be i.i.d.
random vector with distribution PX . The encoder wants to describe X
n to two decoders by sending a pair of
messages (m1,m2) where 1 ? mi ? Mi for i ? {1, 2}. The first decoder reconstructs X?
n
1 (m1) ? X?
n
1 based only
on the first message m1. The second decoder reconstructs X?
n
2 (m1,m2) ? X?
n
2 based on both m1 and m2. The
setting is described in Figure 1.
Xn Enc Dec 1
Dec 2
X?n1
X?n2
m1
m2
Fig. 1: Successive Refinement
Let di(·, ·) : X × X?i ? [0,?) be a distortion measure for i-th decoder. The rates of code (R1, R2) are simply
defined as
R1 =
1
n
logM1
6
R2 =
1
n
logM1M2.
An (n,R1, R2, D1, D2, ?)-successive refinement code is a coding scheme with block length n and excess distortion
probability ? where rates are (R1, R2) and target distortions are (D1, D2). Since we have two decoders, the excess
distortion probability is defined by Pr
[
di(X
n, X?ni ) > Di for some i
]
.
Definition 4. A rate-distortion tuple (R1, R2, D1, D2) is said to be achievable, if there is a family of (n,R
(n)
1 , R
(n)
2 ,
D1, D2, ?
(n))-successive refinement code where
lim
n??
R
(n)
i = Ri for all i,
lim
n??
?(n) = 0.
For some special cases, both decoders can achieve the point-to-point optimum rates simultaneously.
Definition 5. Let Ri(Di) denote the rate-distortion function of the i-th decoder for i ? {1, 2}. If the rate-distortion
tuple (R1(D1), R2(D2), D1, D2) is achievable, then we say the source is successively refinable at (D1, D2). If the
source is successively refinable at (D1, D2) for all D1, D2, then we say the source is successively refinable.
The following theorem provides a necessary and sufficient condition of successive refinable sources.
Theorem 4 ([8], [9]). A source is successively refinable at (D1, D2) if and only if there exists a conditional
distribution P
X?1,X?2|X
such that X ? X?2 ? X?1 forms a Markov chain and
Ri(Di) = I(X ; X?i)
E
[
di(X, X?i)
]
? Di
for i ? {1, 2}.
The condition in the theorem holds for all discrete memoryless sources under logarithmic loss. Note that the
above results of successive refinability can easily be generalized to the case of k decoders.
III. ONE TO ONE CORRESPONDENCE BETWEEN GENERAL DISTORTION AND LOGARITHMIC LOSS
A. Main Results
Consider fixed-length lossy compression under an arbitrary distortion d(·, ·) as described in Section II-B. We have
a source X with the finite alphabet X = {1, . . . , r}, source distribution PX , and the finite reconstruction alphabet
X? = {1, . . . , s}. For the fixed number of messages M , let f? and g? be the encoder and decoder that achieve the
optimum average distortion D?(M). I.e.,
E [d(X, g?(f?(X)))] = D?(M).
Let P ?
X?|X
denote the rate-distortion function achieving conditional distribution at distortion D = D?(M). I.e.,
PX × P
?
X?|X
achieves the infimum in
R(D?(M)) = inf
P
X?|X :E[d(X,X?)]?D?(M)
I(X ; X?). (5)
7
Note that R(D?(M)) may be strictly smaller than logM in general since R(·) is an informational rate-distortion
function which does not characterize the best achievable performance for the “one-shot” setting in which D?(M)
is defined.
Now, we define the corresponding fixed-length lossy compresson problem under logarithmic loss. In the corre-
sponding problem, the source alphabet X = {1, . . . , r}, source distribution PX , and the number of messages M
remain the same. However, we have different reconstruction alphabet Y = {P ?
X|X?
(·|x?) : x? ? X?} ? M(X ) where
P ? pertains to the achiever of the infimum in (5) associated with the original loss function. Let the distortion now
be logarithmic loss.
We can also connect the encoding and decoding schemes between the two problems. Suppose f : X ?
{1, . . . ,M} and g : {1, . . . ,M} ? X? are an encoder and decoder pair in the original problem. Let f? = f
and g? : {1, . . . ,M} ? Y where
g?(m) = P
?
X|X?
(·|g(m)).
Then, f? and g? are an valid encoder and decoder pair for the corresponding fixed-length lossy compression problem
under logarithmic loss. Conversely, given f? and g?, we can find corresponding f and g because Lemma 3 guarantees
that P
X|X?(·|x?) are distinct.
The following result shows the relation between the corresponding schemes.
Theorem 5. For any encoder-decoder pair (f?, g?) for the corresponding fixed-length lossy compression problem
under logarithmic loss, we have
E [?(X, g?(f?(X)))]
= H(X |X??) + ?? (E [d(X, g(f(X)))]?D?(M))
? H(X |X??)
where (f, g) is the corresponding encoder-decoder pair for the original lossy compression problem. Note that
H(X |X??) and the expectations are with respect to the distribution PX × P
?
X?|X
. Moreover, equality holds if and
only if f? = f
? and g?(m) = P
?
X|X?
(·|g?(m)).
Proof. We have
E [?(X, g?(f?(X)))]
= E
[
?
(
X,P ?
X|X?
(·|g(f(X)))
)]
= E
[
log
1
P ?
X|X?
(x|g(f(x)))
]
.
Then, (4) implies that
E [?(X, g?(f?(X)))]
= E
[
log
1
PX(X)
? ?X(X,D
?(M))
]
8
+ E [??d(X, g(f(X)))? ??D?(M)]
= H(X |X??) + ?? (E [d(X, g(f(X)))]?D?(M)) (6)
? H(X |X??) (7)
where (6) is because E [?X(X,D
?(M))] = R(D?(M)) = I(X ; X??) with respect to the distribution PX × P
?
X?|X
.
The last inequality (7) is because D?(M) is the minimum achievable average distortion with M codewords. Equality
holds if and only if E [d(X, g(f(X)))] = D?(M) which can be achieved by the optimum scheme for the original
lossy compression problem. In other words, equality holds if
f?? =f
?
g?? (m) =P
?
X|X?
(·|g?(m)).
Remark 1. In the corresponding fixed-length lossy compression problem under logarithmic loss, the minimum
achievable average distortion given the number of codewords M is
D?? (M) = H(X |X?
?)
where the conditional entropy is with respect to the distribution PX × P
?
X?|X
.
B. Discussion
1) One-to-One Correspondence: Theorem 5 implies that for any fixed-length lossy compression problem, we
can find an equivalent problem under the logarithmic loss where optimum encoding schemes are the same. Thus,
without loss of generality, we can restrict our attention to the problem under logarithmic loss with reconstruction
alphabet Y = {q(1), . . . , q(s)} ? M(X ).
2) Suboptimality of the Scheme: Suppose f and g are sub-optimal encoder and decoder for the original fixed-
length lossy compression problem, then the theorem implies
E [?(X, g?(X))]?H(X |X?
?)
= ?? (E [d(X, g(f(X)))]?D?(M)) . (8)
The left hand side of (8) is the cost of sub-optimality for the corresponding lossy compression problem. On the other
hand, the right hand side is proportional to the cost of sub-optimality for the original problem. In Section III-B1,
we discussed that the optimum schemes of the two problems coincide. Equation (8) shows stronger equivalence
that the cost of sub-optimalities are linearly related. This simply implies a good code for one problem is also good
for another.
3) Operations on X? : In general, the reconstruction alphabet X? does not have any algebraic structure. However,
in the corresponding rate-distortion problem, the reconstruction alphabet is the set of probability measures where
we have very natural operations such as convex combinations of elements, projection to a convex hull, etc.
9
C. Exact Performance of Optimum Scheme
In the previous section, we showed that there is a corresponding lossy compression problem under logarithmic
loss which shares the same optimum coding scheme. In this section, we investigate the exact performance of the
optimum scheme for the fixed-length lossy compression problem under logarithmic loss, where the reconstruction
alphabet is the set of all measures on X , i.e., M(X ). Although the optimum scheme associated with M(X ) may
differ from the optimum scheme with the restricted reconstruction alphabets Y , it may provide some insights. Note
that we are not allowing randomization, but restrict attention to deterministic schemes.
1) Average Distortion Criterion: In this section, we characterize the minimum average distortion D?(M) when
we have fixed number of messages M . Let an encoder and a decoder be f : X ? {1, . . . ,M} and g : {1, . . . ,M} ?
M(X ) where g(m) = q(m) ? M(X ). Then, we have
E [?(X, g(f(X)))]
=
?
x?X
PX(x) log
1
q(f(x))(x)
= H(X) +
M
?
m=1
?
x?f?1(m)
PX(x) log
PX(x)
q(m)(x)
= H(X) +
M
?
m=1
um log um
+
M
?
m=1
um
?
x?f?1(m)
PX(x)
um
log
PX(x)/um
q(m)(x)
where um =
?
x?f?1(m) PX(x). Since PX|f(X)(x|m) =
PX(x)
um
for all x ? f?1(m), we have
E [?(X, g(f(X)))]
= H(X)?H(f(X))
+
M
?
m=1
umD
(
PX|f(X)(·|m)
?
?
?
q(m)
)
? H(X)?H(f(X)).
Equality can be achieved by choosing q(m) = PX|f(X)(·|m) which can be done no matter what f is. Thus, we
have
D?(M) = H(X)? max
f :|f |?M
H(f(X)).
Note that one trivial lower bound is
D?(M) ? H(X)? logM.
2) Excess Distortion Criterion: In this section, we characterize the minimum number of codewords M?(D, ?)
that can achieve the distortion D and the excess distortion probability ?. Let an encoder and a decoder be f : X ?
10
{1, . . . ,M} and g : {1, . . . ,M} ? M(X ) where g(m) = q(m) ? M(X ). Since ?(x, q) ? D is equivalent to
q(x) ? e?D, we have
1? Pe =
?
x?X
PX(x)1
(
q(f(x))(x) ? e?D
)
=
M
?
m=1
?
x?f?1(m)
PX(x)1
(
q(m)(x) ? e?D
)
.
However, at most ?eD? of the q(m)(x) can be larger than e?D where ?x? is the largest integer that is smaller
than or equal to x. Thus, we can cover at most M · ?eD? of the source symbols with M codewords. Suppose
PX(1) ? PX(2) ? · · · ? PX(r), then the optimum scheme is
f(x) =
?
x
?eD?
?
q(m)(x) =
?
?
?
?
?
?
?
1/?eD? if f(x) = m
0 otherwise
where q(m) = g(m). The idea is that each reconstruction symbol q(m) covers ?eD? number of source symbols by
assigning probability mass 1/?eD? to each of them.
The above optimum scheme satisfies
1? Pe =
M·?eD?
?
x=1
PX(x)
=FX
(
M · ?eD?
)
where FX(·) is the cumulative distribution function of X . This implies that the minimum error probability is
??(M,D) = 1? FX
(
M · ?eD?
)
.
On the other hand, if we fix the target error probability ?, the minimum number of codewords is
M?(D, ?) =
?
F?1X (1? ?)
?eD?
?
where F?1X (y) = argmin
1?x?r
{x : FX(x) ? y} and ?x? is the smallest integer that is larger than or equal to x. Note that
if we allow variable length coding without prefix condition, the optimum coding scheme is similar to the optimum
nonasymptotic lossless coding introduced in [15].
IV. SUCCESSIVE REFINABILITY
We considered the fixed-length lossy compression problem so far. In this section, we provide another universal
property of logarithmic loss where the source is discrete memoryless.
11
A. Main Results
Consider the successive refinement problem with a discrete memoryless source as described in Section II-C.
Specifically, we are instrested in the case where the first decoder is under logarithmic loss and the second decoder
is under some arbitrary distortion measure d. Using the result from previous section, there is an equivalent rate-
distortion problem under logarithmic loss for the second decoder. Since any discrete memoryless source under
logarithmic loss is successively refinable, one may argue that the source is successively refinable under this setting.
However, this can be misleading since we cannot directly apply our result to discrete memoryless source. This is
mainly because the decoder only considers product measures when the source is discrete memoryless. Moreover, the
equivalent rate-distortion problem under logarithmic loss has restricted reconstruction set with only finitely many
measures, while successive refinability of logarithmic loss is guaranteed when the reconstruction alphabet is the set
of all measures on X .
Despite of these misconceptions, we show that our initial guess was correct, i.e., it is successively refinable. This
provides an additional universal property of logarithmic loss in the context of the successive refinement problem.
Theorem 6. Let the source be arbitrary discrete memoryless. Suppose the distortion criterion of the first decoder
is logarithmic loss while that of the second decoder is an arbitrary distortion criterion d : X × X??[0,?]. Then
the source is successively refinable.
Proof. The source is successively refinable at (D1, D2) if and only if there exists a X ? X? ?Q such that
I(X ;Q) =R1(D1), E [?(X,Q)] ? D1
I(X ; X?) =R2(D2), E
[
d(X, X?)
]
? D2.
Let P
X??|X be the informational rate-distortion function achieving conditional distribution for the second decoder.
I.e.,
I(X ; X??) = R2(D2), E
[
d(X, X??)
]
= D2.
Consider a random variable Z ? Z such that the joint distribution of X, X??, Z is given by
P
X,X?,Z
(x, x?, z) = P
X,X?
(x, x?)P
Z|X?(z|x?)
and H(X |Z) = D1. We assume that all the PX|Z(·|z) are distinct for all z ? Z . If we let Q = PX|Z(·|Z) and
q(z) = PX|Z(·|z) for all z ? Z , then X ? X?
? ?Q forms a Markov chain and
PX|Q(x|q
(z)) = q(z)(x).
Since Q = PX|Z(·|Z) is a one-to-one mapping, we have
I(X ;Q) =I(X ;Z) = H(X)?D1 = R1(D1).
Also, we have
E [?(X,Q)] =E
[
log 1
PX|Z(X|Z)
]
= H(X |Z) = D1.
12
We have no constraints on the set Z and the only requirements for the random variable Z is H(X |Z) = D1.
Therefore, we can always find such random variable Z , and we can say that the source and respective distortion
measures are successively refinable.
The key idea of the theorem is that (1) is the only loose required condition for the rate-distortion function
achieving conditional distribution. Thus, for any distortion criterion in the second stage, we are able to choose an
appropriate distribution PX,X?,Q that satisfies both (1) and the condition for successive refinability.
Remark 2. We would like to point out that the way of constructing the joint distribution PX,X?,Q in the proof using
random variable Z is the only possible way. More precisely, consider a Markov chain X ? X? ? Q that satisfies
the condition for successive refinability, then there exists a random variable Z such that Q = PX|Z(·|Z) where
X?X??Z forms a Markov chain. This is because we can have Z = Q, in which case PX|Z(·|Z) = PX|Q(·|Q) = Q.
Theorem 6 can be generalized to successive refinement problem with K intermediate decoders. Consider random
variables Zk ? Zk for 1 ? k ? K such that X?X??ZK?· · ·?Z1 forms a Markov chain and the joint distribution
of X, X??, Z1, . . . , ZK is given by
P
X,X?,Z1,...,ZK
(x, x?, z1, . . . , zK)
= P
X,X?
(x, x?)P
Z1|X?
(z1|x?)
K?1
?
k=1
PZk+1|Zk(zk+1|zk)
where H(X |Zk) = Dk and all the PX|Zk(·|zk) are distinct for all zk ? Zk. Similar to the proof of Theorem 6,
we can show that Qk = PX|Zk(·|Zk) for all 1 ? k ? K satisfy the condition for successive refinability. Thus, we
can conclude that any discrete memoryless source with K intermediate decoders is successively refinable as long
as all the intermediate decoders are under logarithmic loss.
V. CONCLUSION
To conclude our discussion, we summarize our main contributions. We showed that for any fixed length lossy
compression problem under arbitrary distortion measure, there exists a corresponding lossy compression problem
under logarithmic loss where the optimum schemes coincide. Moreover, we proved that a good scheme for one
lossy compression problem is also good for the corresponding problem. This provides an algebraic structure on any
reconstruction alphabet. On the other hand, in the context of successive refinement problem, we showed another
universal property of logarithmic loss that any discrete memoryless source is successively refinable as long as the
intermediate decoders operate under logarithmic loss.
REFERENCES
[1] T. A. Courtade and R. D. Wesel, “Multiterminal source coding with an entropy-based distortion measure,” in Proc. IEEE Int. Symp. Inf.
Theory. IEEE, 2011, pp. 2040–2044.
[2] T. Courtade and T. Weissman, “Multiterminal source coding under logarithmic loss,” IEEE Trans. Inf. Theory, vol. 60, no. 1, pp. 740–761,
Jan 2014.
[3] Y. Shkel and S. Verd, “A single-shot approach to lossy source coding under logarithmic loss,” in Proc. IEEE Int. Symp. Inf. Theory, July
2016, pp. 2953–2957.
13
[4] F. Tishby, N. Pereira and W. Bialek, “The information bottleneck method,” in Proc. 37th Ann. Allerton Conf. Comm. Control Comput.,
1999, pp. 368–377.
[5] P. Harremoe?s and N. Tishby, “The information bottleneck revisited or how to choose a good distortion measure,” in Proc. IEEE Int. Symp.
Inf. Theory. IEEE, 2007, pp. 566–570.
[6] R. Gilad-Bachrach, A. Navot, and N. Tishby, “An information theoretic tradeoff between complexity and accuracy,” in Learning Theory
and Kernel Machines. Springer, 2003, pp. 595–609.
[7] J. Jiao, T. A. Courtade, K. Venkat, and T. Weissman, “Justification of logarithmic loss via the benefit of side information,” IEEE Trans.
Inf. Theory, vol. 61, no. 10, pp. 5357–5365, 2015.
[8] W. H. Equitz and T. M. Cover, “Successive refinement of information,” IEEE Trans. Inf. Theory, vol. 37, no. 2, pp. 269–275, 1991.
[9] V. Koshelev, “Hierarchical coding of discrete sources,” Problemy peredachi informatsii, vol. 16, no. 3, pp. 31–49, 1980.
[10] A. No, A. Ingber, and T. Weissman, “Strong successive refinability and rate-distortion-complexity tradeoff,” IEEE Trans. Inf. Theory,
vol. 62, no. 6, pp. 3618–3635, 2016.
[11] R. Venkataramanan, T. Sarkar, and S. Tatikonda, “Lossy compression via sparse linear regression: Computationally efficient encoding and
decoding,” IEEE Trans. Inf. Theory, vol. 60, no. 6, pp. 3265–3278, June 2014.
[12] A. No and T. Weissman, “Rateless lossy compression via the extremes,” IEEE Trans. Inf. Theory, vol. 62, no. 10, pp. 5484–5495, 2016.
[13] V. Kostina and S. Verdu?, “Fixed-length lossy compression in the finite blocklength regime,” IEEE Trans. Inf. Theory, vol. 58, no. 6, pp.
3309–3338, 2012.
[14] I. Csisza?r, “On an extremum problem of information theory,” Studia Scientiarum Mathematicarum Hungarica, vol. 9, no. 1, pp. 57–71,
1974.
[15] I. Kontoyiannis and S. Verdu, “Optimal lossless data compression: Non-asymptotics and asymptotics,” IEEE Trans. Inf. Theory, vol. 60,
no. 2, pp. 777–795, Feb 2014.
